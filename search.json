[{"date":"2024-04-22T13:14:49.575Z","url":"/2024/04/22/%E9%9A%8F%E6%89%8B%E8%AE%B0/","categories":[["undefined",""]],"content":"113.54.151.70lm lm^985211root root^985211 笔记发送。 个人博客 ReplicaChecker ensures region has the best replicas.Including the following:Replica number management.Unhealthy replica management, mainly used for disaster recovery of TiKV.Location management, mainly used for cross data center deployment.ReplicaChecker 确保区域拥有最好的副本。包括以下内容：副本编号管理。不健康副本管理，主要用于 TiKV 的容灾。位置管理，主要用于跨数据中心部署。"},{"date":"2024-04-22T13:14:49.540Z","url":"/2024/04/22/XinKa/","categories":[["undefined",""]],"content":"日程安排[[今日计划]][[近期计划]] 研究[[研究]] 工具使用[[工具教程]] 环境配置[[配置：Windows]] 语言学习[[go语言学习记录]] [[英语学习]] 数据收集[[数据集]] AI 学习[[AI 学习]] 日常[[工作日志]]"},{"date":"2024-04-22T13:14:49.496Z","url":"/2024/04/22/TEMP%EF%BC%9A%E7%BB%84%E4%BC%9A%E9%93%BA%E5%9E%AB/","categories":[["undefined",""]]},{"title":"Hello Worldaaaaaaa","date":"2024-04-22T13:14:49.444Z","url":"/2024/04/22/TEMP%EF%BC%9A%E4%B8%AD%E9%93%81/","categories":[["undefined",""]],"content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. 密码学的诞生和发展密码学的诞生和发展是一个跨越数千年、涉及多个学科领域的复杂过程。从最初的古典密码学到现代的公钥密码系统，密码学经历了显著的演变和进步。 在古代，人们为了安全通信而采用了一些原始的方法，这些方法包括但不限于代换密码和置换密码7。例如，古罗马人使用凯撒密码（一种简单的替换加密技术），而阿拉伯人在9世纪就已经有了关于密码学的书籍，其中al-Kindī是已知最古老的密码学著作的作者13。这表明密码学的概念至少可以追溯到公元9世纪，而且阿拉伯人在密码学的发展上做出了重要贡献。 进入中世纪，随着通信技术的进步，特别是印刷术的发明，密码学开始逐渐发展成为一个更为系统化的学科。这一时期，密码学主要是一门艺术而非科学，其核心手段主要是代换和置换9。 20世纪初至中期，随着计算机科学和信息论的发展，密码学开始逐渐转变为一门科学。C.E.Shannon的《保密系统的通信理论》标志着现代密码学理论的起点20。Shannon的工作强调了信息理论在设计安全通信系统中的重要性，为后来的密码学研究奠定了理论基础。 随后，W·Diffie和M.E.Hellman的工作进一步推动了密码学的发展，他们提出的公钥密码体制彻底改变了密码学的研究方向20。公钥密码体制的最大特点是使用两个不同但相关的密钥——一个公钥用于加密信息，一个私钥用于解密信息——从而实现了加密和解密能力的分离2。 进入21世纪，随着网络技术的飞速发展和量子计算的兴起，密码学面临着新的挑战和机遇。量子计算机的出现使得传统的加密方法面临被破解的风险，因此研究抗量子攻击的密码体制变得尤为重要2。此外，随着云计算、大数据等新技术的应用，现代密码学也面临着新的需求和挑战，如多方通信模式、异地计算模式等3。 总之，密码学从最初的简单方法发展到今天的复杂系统，经历了从古典密码学到现代密码学的演变过程。这一过程不仅涉及到数学、计算机科学等多个学科领域的知识和技术的进步，还包括对新兴技术应用的需求响应。未来，随着技术的不断进步和社会需求的变化，密码学将继续演化和发展，以应对新的安全挑战。 密码学的应用，重点 office 的加密过程原理密码学的应用在Office文件加密过程中扮演着至关重要的角色。随着信息技术的快速发展，信息安全成为了全球性的关注点，尤其是在办公软件中文件的安全性问题10。Microsoft Office作为广泛使用的办公软件，其文件的安全性主要通过加密技术来实现14。自Office97发布以来，Microsoft公司对Office办公软件的文件加密功能进行了不断的改进，但文件加密的安全性仍然难以保证1。 近年来，为了提高Office文档加密的安全性，研究者们提出了使用新的加密算法的方法。例如，2019年的研究讨论了将新的加密算法O’zDSt1105:2009和第一个哈希算法O’zDSt1106:2009应用于Microsoft Office文档加密的方法2。这种方法不仅适用于Microsoft Word 2016，还可以应用于Microsoft Office产品的其他文档，通过设计和实现下一代加密提供者来应用这些新算法，从而提高了加密Microsoft Office文档的相对加密稳定性2。 此外，密码学的基本原理和技术也在不断发展中。现代密码体制主要分为公钥体制和私钥体制两大类12。公钥体制允许公开加密密钥而仅需保密解密密钥，具有数字签名、鉴别等功能，被广泛应用于金融、商业等社会生活各领域12。而私钥体制又称为单钥体制，其加密、解密密钥相同12。这些密码体制的发展和应用，为Office文档加密提供了技术支持。 然而，尽管采用了先进的加密技术，Office文档的加密安全仍然面临挑战。例如，随着计算机速度的快速提升和新解码方法的提出，文档安全性受到了质疑5。此外，网络办公系统存在的安全漏洞也为黑客攻击办公系统提供了机会9。因此，除了采用强大的加密技术外，还需要从计算机技术和网络安全管理两方面入手，解决网络存在的漏洞，制定网络安全策略，以确保在安全可靠的网络环境下进行信息化办公9。 总之，密码学在Office文件加密过程中的应用是多方面的，包括采用新的加密算法提高安全性2、利用公钥和私钥体制进行数据加密和解密12，以及面对安全挑战时采取综合措施保护信息安全9。随着技术的不断进步和安全需求的不断提高，密码学在保障Office文档安全方面的作用将会越来越重要。 当前密码破解的研究引出本系统"},{"date":"2024-04-22T13:14:49.407Z","url":"/2024/04/22/TEMP%EF%BC%9A%E8%AE%BA%E6%96%87%E6%80%9D%E8%B7%AF/","categories":[["undefined",""]],"content":"如何 把 词性标注：这个 概念 连接到 密码的只是 学习 这个领域。 如果要用 专家模型，如何对数据进行分类？分几类？"},{"title":"Englislearning","date":"2024-04-22T13:14:49.345Z","url":"/2024/04/22/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/%E8%8B%B1%E8%AF%AD%E5%8D%95%E8%AF%8D/","categories":[["undefined",""]],"content":" 单词 词性 翻译 关联 concatenate v. | adj. 连接 n. concatenation leet n 民事法庭 laborious adj. 费力的；艰难的 novel n.adj. 小说；新奇的 a novel approach promising adj. 有希望的；有前途的 thereof adj. 它的； variants thereof: 它的变体 heuristics adj. 启发式的 hypothesize v. 假设假定； 来自 hypothesis：假说 density n. 密度 mimic n.v.adj. 模仿 discriminative adj. 有判别能力的 criminative：使负罪的 overwhlming adj 势不可挡的 the overwhelming majority of：绝大多数 tradeoff n. 公平交易，折衷 权衡 nuisance v. 干扰 inception n. 开端，创始 instantiate n. 实例化 residual adj.n. 残余的，残留的；残余，残渣 approximate v. 估计；逼近 intractable adj. 棘手的 integral n. 积分 truncation n. 截断 consecutive adj. 连续的 premise n. 前提 elapse v. 消逝，流逝 invocation n. 调用，启用 pending adj. 悬而未决的，即将发生的 "},{"date":"2024-04-22T13:14:49.345Z","url":"/2024/04/22/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/%E8%8B%B1%E8%AF%AD%E7%9F%AD%E8%AF%AD/","categories":[["undefined",""]],"content":" 短语 含义 用法 At its core 其核心是 At its core, our idea is to do sth. in (stark) contrast with sth 和..形成鲜明对比 in a way 在某种程度上 "},{"date":"2024-04-22T13:14:49.345Z","url":"/2024/04/22/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/","categories":[["undefined",""]],"content":"[[英语单词]] [[英语短语]]"},{"date":"2024-04-22T13:14:49.295Z","url":"/2024/04/22/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/%E7%A0%94%E7%A9%B6/","categories":[["undefined",""]],"content":"[[方向调研]] [[检索工具]] [[会议调研]] @article: 用于引用发表在学术期刊中的文章，包括期刊论文、期刊文章等。 @book: 用于引用书籍、专著等完整的出版物。 @inproceedings: 用于引用会议论文或会议文章。 @thesis: 用于引用学位论文、硕士论文、博士论文等。 @techreport: 用于引用技术报告、研究报告等非正式出版物。 @website: 用于引用网页、网站或在线资源。 "},{"date":"2024-04-22T13:14:49.295Z","url":"/2024/04/22/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/%E6%A3%80%E7%B4%A2%E5%B7%A5%E5%85%B7/","categories":[["undefined",""]],"content":"检索数据库 名称 描述 链接 Engineering Village 平台上的10多个数据库涵盖了工程、应用科学相关的最为广泛的领域，内容来源包括学术文献、商业出版物、发明专利、会议论文和技术报告等等;其中的Compendex 就是美国工程索引Engineering Index数据库 （用 Edge 打开） "},{"date":"2024-04-22T13:14:49.293Z","url":"/2024/04/22/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/%E6%96%B9%E5%90%91%EF%BC%9A%E5%AF%86%E7%A0%81%E7%94%9F%E6%88%90/","categories":[["undefined",""]],"content":"背景互联网和安全不断发展，互联网的开放性也带来了许多内在的安全隐患。 为了保护用户信息，当前已经产生了多种认证方式：身份认证的分类： 用户知道的：密码，个人识别码 用户拥有的：U盾，加密卡 用户本身的： 生理特征：指纹，虹膜 行为特征：手写 对于上面这些认证方式，其中密码（即口令）破解是最常见的认证方式，原因主要有： 互联网应用不断发展，人们不得不设置许多满足不同密码策略的密码来保护相应的软件应用程序。 人的记忆能力有限。研究表明，人类只能记住五到七个密码，因此在设置密码时普遍会使用一些低信息熵的密码，比如： 在多个系统中使用相同密码 在密码中使用个人相关信息 大量知名网站的密码文件被泄露：比如著名的 rockyou数据集就是2018年受黑客攻击而泄漏的。 且人工智能以及到现在 大模型的产生和发展更进一步增强了破解口令认证系统的能力，因此对于这方面的研究也显得更加迫切和重要。（对口令认证系统攻击最严重的是口令猜测攻击） 口令猜测分类（密码猜测的角度|方法： 启发式搜索 概率模型 深度学习） 根据是否在线（也就是是否与服务器进行交互）： 离线密码猜测 在线密码猜测 （前一种攻击要求身份验证服务器存储用户帐户密码文件，然后攻击者在本地主机上猜测密码。在这种情况下，可以尝试的猜测次数仅受攻击者的计算资源的限制。后者不需要密码文件，攻击者只需要连接到网络即可。然而，可以尝试的猜测次数往往受到服务器安全策略的限制，例如美国国家身份标准NIST-800-63-3，其中规定政府网站系统一个月允许的最大登录失败次数为100次，如果超过100次，帐户将被锁定。） 根据是否利用用户的个人信息： 拖网猜测。 定向猜测，利用用户个人信息。（拖网猜测主要利用用户选择流行密码的倾向，而定向猜测不仅利用普通用户使用流行密码的漏洞，还利用用户重复使用密码和使用个人信息构建密码的漏洞。在个人身份信息和历史密码等信息的帮助下，定向密码猜测的成功率明显高于相同猜测次数的拖网猜测） 是否使用人工智能： 传统攻击 神经攻击（前者将密码猜测作为一项文本生成任务，依靠人工智能相关技术在大规模密码训练语料库上生成文本密码。后者封装了其他方法，不包括深度神经网络猜测方法。） （几乎所有现有的工作都是离线猜测，而在线猜测太难在100个猜测内猜出一个密码。所以当前的分类方式主要为：传统的。。基于神经网络的。。） （说一下如何比较生成密码的质量： 1. 生成的唯一密码个数和比例； 2. 生成密码的速度； 3. 生成的密码和训练数据集之外其他数据集的匹配个数 ） 拖网猜测传统拖网猜测 启发式算法 这些算法没有严格的理论体系，严重依赖于零散的奇思妙想，例如，基于开源软件使用精心设计的猜测序列构建独特的猜测词典。这些启发式方法很难重现，也很难相互公平地比较。因此，在这里，我们只介绍一些常用的启发式密码猜测工具。 Jtr是一个密码猜测工具，专注于破解UNIX&#x2F;Linux系统的弱密码。在JTR中有四种模式–简单破解模式、单词表模式、增量模式和外部模式。 Hashcat是世界上速度最快、最先进的密码恢复实用程序，支持针对300多种高度优化的哈希算法的五种独特攻击模式。Hashcat目前支持Linux、Windows和MacOS上的CPU、GPU和其他硬件加速器，并具有帮助启用分布式密码破解的设施。 彩虹攻击：一种基于预先计算和空间-时间折中的攻击方法。通过事先计算大量的密码和散列值之间的对应关系（彩虹表），以便在攻击时快速查找并破解散列值。彩虹表是一个预先计算的表格，其中包含了大量的密码和对应的散列值。通过在彩虹表中进行查找，攻击者可以找到与目标散列值匹配的原始密码。 概率上下文无关语法 该算法的核心假设是密码的字母段L、数字段D和特殊字符段S相互独立。该算法包括训练和猜测两个阶段。 在训练阶段，最关键的是根据泄露的密码数据集计算密码模式(结构)和字符成分(语义)的频率。 在猜测阶段，根据训练阶段得到的模式频率表和语义频率表生成具有频率猜测的集合，以模拟真实密码的概率分布。 粒度太细，无法获得字符之间的语义关系，改进： 基于词的内聚力和自由度提取密码中的语义片段，并对基于语义片段的PCFG算法进行改进 密码视为由几个块组成，其中块是频繁出现在一起以模拟密码的相关字符的序列。 扩展字节对编码(BPE)算法，它使用块词汇表平均长度的可配置参数来替换合并操作的数量。 为了解决长密码猜测的困难，提出了一种改进的基于PCFG的LONG 马尔可夫序列决策 该算法的核心假设是用户从前到后依次构造口令。它不像PCFG那样分割密码，而是训练整个密码，并通过从左到右链接字符来计算密码的概率。 传统的马尔可夫模型因其结构简单、推理速度快而被广泛应用于口令猜测工作中。但是，它也有一定的缺陷，如过度拟合、重复率高、基于随机抽样生成的密码覆盖率低等。 过拟合：将拉普拉斯平滑和结束符号正则化技术应用于马尔可夫模型。平滑策略消除了数据集中的过拟合问题，正则化技术使攻击算法产生的猜测概率总是和为1。 重复率：设计了一种基于随机抽样的动态分配机制。该机制允许动态调整密码的概率分布，并在猜测过程中严格收敛到均匀分布。基于上述动态分布机制，提出了一种动态马尔可夫模型。 对口令中的语义段进行建模：提出了一个名为Word马尔可夫的模型，通过语义分段从密码中提取单词的凝聚力和自由度。 神经拖网猜测算法密码猜测任务视为文本生成问题，核心就是把密码数据集给神经网络进行训练，并利用训练好的模型生成候选密码。 递归神经网络(RNN)：递归神经网络以序列数据为输入，在序列进化方向上递归，所有节点以链的形式连接。 RNN 有许多变体：如： LSTM，GRU，BiLSTM 等。 步骤：基于RNN的密码猜测方法通常有以下两个步骤： ·将训练集中的密码序列输入到RNN模型中，以进行顺序文本生成的训练。 训练好的 RNN 猜测模型旨在根据已有的密码字符生成下一个密码字符，直到输出终止字符位置。 在整个密码生成过程中，RNN计算任何字符作为下一个密码令牌的概率。对于给定的阈值，概率高于阈值的密码将被放入密码猜测集中作为有效密码。 一些变体： 对密码单词进行切分，在词段的基础上使用BiLSTM生成密码。 层次语义模型HSM，该模型将LSTM和语义分析相结合，用于挖掘用于密码猜测的词之间潜在的概率关系。 基于汉字音节的密码猜测方法。该方法将汉语音节作为整体元素对密码进行解析和处理。然后，在LSTM神经网络中对处理后的密码进行训练以生成密码。 特别是具有注意力机制的RNN来组合和内插同一组用户的信息，以定义比他们的密码分布更稳健和准确的优先级。推理时使用辅助数据来适应目标密码的分布。通过这种方式，开发了一种利用辅助信息并实例化上下文感知口令计量和猜测攻击的全自动方法，而不需要来自目标口令分布的任何明文样本。 生成式对抗网络（GAN）： 基于GAN的密码猜测模型由密码生成器和鉴别器组成。这两个网络相互对抗，不断地调整参数。最终目的是使鉴别器网络无法判断生成网络的输出是否真实，从而达到口令生成的效果。 问题：离散密码数据的不可微性可能导致梯度反向传播失败，基于GAN的密码猜测模型训练难以收敛，由GAN模型生成的密码重复率较高。 离散数据不可微问题解决： 使用Gumbel-Softmax松弛技术来训练基于GAN的密码猜测模型。 使用通过附加的自动编码器获得的真实口令的平滑表示。 种基于SeqGAN的密码猜测方法RLPassGAN，该方法使用策略梯度来确保模型参数的持续优化。 收敛困难的问题解决：基于双向生成对抗网络的猜测算法，提高了算法的收敛速度。与传统的GaN相比，它可以在更短的时间内产生相同数量的样本 重复率高问题解决： 使用蒙特卡罗搜索来评估中间层输出处的不完整密码序列，减少冗余的合成密码。（此外，通过蒙特卡罗搜索来评估输出的不完整密码序列。蒙特卡罗是一种使用大量随机样本来了解特定系统的计算方法。） 使用鉴别器和控制器构建额外的控制器网络，以分别学习生成的密码分布与真实密码分布和均匀分布之间的度量。然后利用这两个度量对生成器进行训练，从而降低了口令生成的重复率。 RLPass：还创新性地将表征学习用于密码猜测。具体地，将密码投影到隐藏空间，并且使用隐藏空间中的密码表示之间的距离来定义密码的相似性。基于口令的强局部性和弱局部性现象，提出了一种动态口令猜测和条件口令算法。 GAN的模型生成的长密码质量较低的问题解决：设计了一种基于DenseNet的GAN密码猜测结构DenseGAN，并提出了两种新的密码猜测DenseGAN模型，这两种模型都能生成高质量的密码猜测。 自动编码器（AE）：它基于反向传播算法和优化方法，使用输入数据本身作为监督来指导神经网络学习映射关系，以获得重建的输出。AE 包括编码器和解码器两部分。根据学习范式，AE 可分为欠完备自动编码器、正则化自动编码器和变分自动编码器，其中前两种是判别性模型，后者是生成性模型。 在拖网密码猜测的研究中，通常使用变分自动编码器来生成密码猜测。将密码样本输入到VAE的编码器以获得表示，然后使用解码器基于该表示重建样本。基于输入和输出之间的重构损失来训练密码生成器。除了使用最经典的VAE框架来猜测密码，也提出来了许多优化。 优化 GAN：将VAE技术与GaN技术相结合，用VAE代替GaN生成器，旨在解决离散口令数据的反向传播问题。 轻量级问题：用门控卷积神经网络(GCNN)代替了复杂的RNN生成单元，降低了模型的复杂性。 Transformer. ![[Pasted image 20240305201324.png]] Transformer 模型采用编解码器体系结构，并使用注意力机制替换 Seq2Seq 模型中的递归结构，以实现序列建模的并行化。这种并行化结构给自然语言处理 (NLP) 领域带来了巨大的冲击。随着研究的深入，相关技术逐渐从自然语言处理向计算机视觉 (CV)、语音、生物、化学等领域发展。同样，在拖网密码猜测的研究中也出现了一些基于Transformer的方法： 基于改进 Transformer 的口令猜测模型：将消息（个人信息和密码的相关性）权重引入到数据预处理中，并在模型中使用改进的波束搜索算法来快速搜索排名靠前的密码猜测。 PassBERT：基于双向 Transformer 的猜测框架，首次将预训练&#x2F;微调的范式应用于密码破解。具体地说， 首先，作者设计了包含一般密码分布知识的通用密码预训练模型。 然后，提出了三种特定于攻击的微调方法来定制预先训练的口令模型以适应以下真实攻击场景：条件口令猜测、目标口令猜测和基于自适应规则的口令猜测。最后，他们进一步提出了一种混合密码强度计来降低这三种攻击的风险。 Reinforcement Learning (RL) 强化学习 RL是机器学习的范例和方法之一，用于描述和解决智能代理在与环境交互过程中最大化回报或实现特定目标的学习策略问题。密码生成器是代理，每个生成的密码序列代表一个完整的轨迹，生成过程中的每个字符都被视为一个操作。例如，由时间戳t生成的字符是action at。根据由所生成的不完整序列确定的关于当前状态ST的随机策略来生成每个动作At。生成器根据一组随机策略从任意字符生成密码，直到满足预定长度。 RLPassGAN：遵循SeqGAN，将密码猜测视为一个连续决策，并使用策略梯度 来确保参数可以连续优化。 Flow： （一个强大的密度估计统计工具）流动模型的一个非常独特的特征是，它的转变通常是可逆的。流模型不仅找到了从分布Z转移到分布X的网络路径，而且该路径还允许X改变为Z。简而言之，流模型找到了分布Z和X之间的双向路径（Z和X的数据维度必须相同）。 PassFlow：基于产生流模型的口令猜测方法。基于流的口令猜测模型使用精确的对数似然计算和优化，使潜在变量的推断更加准确。此外，还给出了潜在空间的一种有意义的表示，这使得探索潜在空间的特定子空间和内插等操作成为可能。（密码猜测方面优于现有的基于GAN的方法） 另一种分类根据泄露的数据集和目标密码是否来自同一数据源，拖网场景可以进一步分为站内场景和跨站场景。 虽然跨站密码猜测场景较为真实，但攻击者一般以掌握目标数据的部分分布信息为前提来猜测密码。 定向密码猜测定向整体来说其实是对传统的优化，比如不改变核心思想的前提下修改嵌入，数据预处理等方面。 神经定向密码猜测随着自然语言处理技术的发展，一些复杂的神经网络被应用到目标密码猜测领域。 PG-PASS：由指针生成网络组成的有针对性的口令猜测模型。该工作创新性地将目标口令猜测作为一项摘要任务，并将智能摘要领域中常用的指针网络技术应用于该任务。需要注意的是，除了用户的人口统计相关信息(姓名、生日等)外，攻击者还可以利用用户在其他网站上泄露的密码进行有针对性的攻击。可以预期，这种利用用户密码重复使用这一易受攻击行为的定向攻击可能比基于人口统计相关信息的攻击更具危害性。 重用模型：重用或微调旧密码来生成新密码的事实，引入深度学习技术来表征用户的密码重复使用行为。更具体地说，他们训练了序列到序列(Seq2seq)模型，以预测将现有密码转换为其姊妹密码所需的修改，并在大规模数据集(即4iQ数据集)上进行了验证。 PASS2EDIT：有针对性的密码猜测算法，用于模拟日益严重的凭据篡改攻击，在该攻击中，攻击者利用受害者泄露的密码来提高在其他站点猜测受害者密码的成功率。特别是，他们提出了一种多步决策训练机制，并建立了一个分类神经网络来学习一步编辑操作对现有密码的反应。由于每项工作使用的数据集不同，因此很难进行横断面比较。 传统定向密码猜测攻击者使用与目标人员相关的个人信息来增强猜测。 基于拖网马尔可夫攻击模型的定向攻击猜测方法： 基本思想是，关于使用某些个人信息的人口的百分比，攻击目标也将具有相同的使用该个人信息的可能性百分比。 首先将PII划分为几种类型，如用户名-A、电子邮件前缀-E和名称-N，并根据所需的粒度进一步细分每种广泛的类型。然后，将训练集的每个密码中的所有PII替换为对应的PII类型。 训练阶段的其余步骤与行走马尔可夫模型的步骤相同。 猜测集生成阶段分为两个步骤。 在第一步中，运行马尔可夫模型以生成中间猜测集合，该中间猜测集合包含直接可用的猜测， 第二步用相应的PII信息替换中间猜测中的基本PII类型字符。 基于PCFG的定向攻击猜测方法： –Personal-PCFG。。其基本思想与PCFG攻击模型相同：根据字符类型和长度对密码进行切片。为了实现这一思想，将六种PI字符类型 (即用户名-A、电子邮件前缀-E、姓名-N、生日-B、电话号码-P和ID-G)等同于拖网PCFG模型中的L、D和S，从而在个人PCFG中有九种类型的字符。然后，在训练过程中，与拖网PCFG攻击模型中一样，训练集中的每个密码根据相应的字符类型及其长度进行分段。 TarGuess Wang等人。提出了一个框架，它用四个数学概率模型系统地刻画了典型的定向猜测场景。 第一个场景TarGuess-I旨在利用用户的PII创建在线目标密码猜测。为了在密码中表示PII令牌，除了PCFG模型中的L、D和S标签外，作者还定义了基于个人信息类型的28个PII标签(例如，N1−N7和B1−B10)。对于每个PII标签，其下标编号表示该类型的PII使用的细分，而不是指示相应长度的下标编号，例如L、D和S标签。例如，N表示名称信息，而N1表示全名，而n2表示全名的缩写 第二个是Targuess-II，目的是根据用户在其他网站(例如Dodonew)中泄露的密码来猜测目标网站(例如CSDN)中的用户密码。具体地，作者提出了6种结构级和2种字符级助记转换来描述密码重用，并基于上述重用规则使用马尔可夫模型来刻画上下文无关的转换文法。 第三个是TarGuessIII，目的是使用姐妹密码和一些PII信息来猜测用户的密码。TarGuess-III将PII信息引入到TarGuess-II模型中，允许在结构级密码重用中嵌入PII信息。 与TarGuess-III相比，TarGuess-IV场景中的攻击者知道难以量化的额外PII(例如，性别、教育)。为了解决某些PII难以直接体现在口令中的问题，在这种难以量化的PII的基础上，巧妙地引入贝叶斯理论来计算口令的重用概率。 RFGuess-PII.在RFGuess的基础上，Wang et al.提出了一种新的目标口令猜测模型RFGuess-PII。密码训练和生成过程类似于拖网猜测场景。不同的是，通过新的PII匹配，密码中的PII字符串被替换为相应的数字标签。这种新的PII匹配旨在最小化信息熵，并试图准确地提取整个用户组的PII使用行为。PII匹配算法的第一步是细分PII的各种可能的变换，并使用数字标签来表示它们。第二步是为训练集中的每个密码列出具有PII标签的所有可能的表示。然后，按频率从高到低对表征进行排序。 RFGuess-重复使用。除了基于PII的针对性密码破解研究外，作者还专注于对用户的密码重复使用行为进行建模。他们还考虑了结构级和段级转换，如TarGuess-II。具体地，它们通过计算训练集中每个密码对的编辑矩阵来计算结构级转换，并训练基于随机森林的段级转换(即，相同类型的字符串内的转换，例如字母段中的密码→密码)模型。 TG-SPSR图谱。马尔可夫模型和PCFG模型转化为目标攻击，提出了一种基于结构划分和字符串重组的系统目标攻击模型，称为TG-SPSR。在结构划分阶段，除了将密码划分为类似于PCFG的基本结构外，还在基本语法模式中定义了基于轨迹的键盘，并引入了索引位来准确描述特殊字符的位置。此外，基于定义的9条修改规则，构造了一个BiLSTM分类器来重用和修改密码的行为。 可能的方向总体来说还是做的工作挺多的，16-23年提出来了三十多种方法，2016年至2018年零星出版数量较少，2021年增至9种。。但是当前比较热门的还是基于深度学习的密码猜测方法。 修改模型结构提高模型训练时的速度：在低资源的情况下快速执行密码猜测也是具有挑战性的。 尝试大模型：预训练的语言模型近年来在自然语言处理领域应用很成熟。可以尝试关于密码猜测的预训练&#x2F;精调范式，将预先训练的语言模型强大的通用语言建模功能与现有的密码猜测工作结合起来 尝试更多模型：目前的神经密码猜测方法都有一个前提，即密码字符串是一个字符序列，使用序列编码对密码进行处理。比如根据密码的语义依赖将密码组织成图，并使用最新的图神经网络(GNN)相关技术进行采样和生成。 传统猜测方法与深度学习方法相结合。比如用深度学习生成规则等。 之后的想法 这两周涉及到了解一些背景所以读的也比较仔细，后面计划读快一些，重点关注方法和创新性。 选择较新，效果较好，的论文尝试复现，分析数据，寻找优化的点。 "},{"date":"2024-04-22T13:14:49.292Z","url":"/2024/04/22/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/%E6%96%B9%E5%90%91%E8%B0%83%E7%A0%94/","categories":[["undefined",""]],"content":"论文调研旨在找到一个喜欢的方向并且实现小论文撰写， ICDE 2023介绍ICDE-2023共接收论文228篇，录用率在25%左右，研究方向统计如下： 方向 子方向 论文数量 图与网络 图算法、图处理框架、图数据库、图神经网络、图分析、知识图谱 69 时空、轨迹数据分析 34 AI4DB 基数估计、查询优化、学习索引、NL2SQL、表格推理、工作负载预测、性能调优、数据库嵌入 13 数据库管理 物化视图管理、工作负载分析、事务、死锁检测、数据库安全 8 存储 4 查询、索引优化 18 人工智能 机器学习、深度学习、聚类、强化学习、在线学习 25 流处理 - 9 数据挖掘与分析 数据挖掘、数据压缩、推荐 25 边缘计算、区块链、物联网 - 11 视频处理 - 2 隐私保护、数据安全 - 4 众包 - 6 详情图与网络 论文 子方向 备注 COCLEP: Contrastive Learning-based Semi-Supervised Community Search 图算法 FASI: FPGA-friendly Subgraph Isomorphism on Massive Graphs 图算法 HGMatch: A Match-by-Hyperedge Framework for Efficient and Parallel Subhypergraph Matching 图算法 Layph: Making Change Propagation Constraint in Incremental Graph Processing by Layering Graph 图算法 LightTraffic: On Optimizing CPU-GPU Data Traffic for Efficient Large-scale Random Walks 图算法 Lumos: Heterogeneity-aware Federated Graph Learning over Decentralized Devices 图算法 PSPC: Efficient Parallel Shortest Path Counting on Large-Scale Graphs 图算法 TDB: Breaking All Hop-Constrained Cycles in Billion-Scale Directed Graphs 图算法 Finer-Grained Engagement in Hypergraphs 图算法 Distributed (α, β)-Core Decomposition over Bipartite Graphs 图算法 Fairness-aware Maximal Biclique Enumeration on Bipartite Graphs 图算法 Index-Based Biclique Percolation Communities Search on Bipartite graphs 图算法 Efficiently Answering Quality Constrained Shortest Distance Queries in Large Graphs 图算法 Verification-Free Approaches to Efficient Locally Densest Subgraph Discovery 图算法 Efficient Maximum Signed Biclique Identification 图算法 Explainable Hyperlink Prediction: A Hypergraph Edit Distance-Based Approach 图算法 Reinforcement Learning Enhanced Weighted Sampling for Accurate Subgraph Counting on Fully Dynamic Graph Streams 图算法 Towards Efficient Shortest Path Counting on Billion-Scale Graphs 图算法 Fair Group Summarization with Graph Patterns 图算法 Top-r keyword-based community search in attributed graphs 图算法 Accelerating k-Core Decomposition by a GPU 图算法 Minimizing the Influence of Misinformation via Vertex Blocking 图算法 Efficiently Sampling and Estimating Hypergraphs By Hybrid Random Walk 图算法 Efficient Multi-GPU Graph Processing with Remote Work Stealing 图算法 Neighborhood Skyline on Graphs: Concepts, Algorithms and Applications 图算法 Finding Top-k Important Edges on Bipartite Graphs: Ego-betweenness Centrality-based Approaches 图算法 VC-dimension and Rademacher Averages of Subgraphs, with Applications to Graph Mining 图算法 Most Probable Densest Subgraphs 图算法 Scalable Algorithms for Densest Subgraph Discovery 图算法 Distributed Near-Maximum Independent Set Maintenance over Large-scale Dynamic Graphs 图算法 Fast Unsupervised Graph Embedding via Graph Zoom Learning 图算法 Wind-Bell Index: Towards Ultra-Fast Relational Query for Graph Databases 图数据库 查询 Hop-Constrained s-t Simple Path Enumeration on Large Dynamic Graphs 图数据库 查询 A Reachability Index for Recursive Label-Concatenated Graph Queries 图数据库 索引 VEND: Vertex Encoding for Edge Nonexistence Determination 图数据库 Extracting Graphs Properties with Semantic Joins 图数据库 Integrating Connection Search in Graph Queries 图数据库 查询 AFaVS: Accurate Yet Fast Version Switching for Graph Processing Systems 图处理系统 FLASH: A Framework for Programming Distributed Graph Processing Algorithms 图处理系统 Partitioner Selection with EASE to Optimize Distributed Graph Processing 图处理系统 Optimizing Graph Partition by Optimal Vertex-Cut: A Holistic Approach 图处理系统 图划分 HyTGraph: GPU-Accelerated Graph Processing with Hybrid Transfer Management 图处理系统 GAMMA: A Graph Pattern Mining Framework for Large Graphs on GPU 图处理系统 图模式挖掘 HyGNN: Drug-Drug Interaction Prediction via Hypergraph Neural Network 图神经网络 Demystifying Bitcoin Address Behavior via Graph Neural Networks 图神经网络 Jointly Attacking Graph Neural Network and its Explanations 图神经网络 Relational Temporal Graph Convolutional Networks for Ranking-Based Stock Prediction 图神经网络 A Bayesian Graph Neural Network for EEG Classification — A Win-Win on Performance and Interpretability 图神经网络 Layer-refined Graph Convolutional Networks for Recommendation 图神经网络 AutoAC: Towards Automated Attribute Completion for Heterogeneous Graph Neural Network 图神经网络 SEIGN: A Simple and Efficient Graph Neural Network for Large Dynamic Graphs 图神经网络 Revisiting Citation Prediction with Cluster-Aware Text-Enhanced Heterogeneous Graph Neural Networks 图神经网络 Towards Higher-order Topological Consistency for Unsupervised Network Alignment 图神经网络 CLDG: Contrastive Learning on Dynamic Graphs 图分析 GALE: Active Adversarial Learning for Erroneous Node Detection in Graphs 图分析 IFCA: Index-Free Community-Aware Reachability Processing Over Large Dynamic Graphs 图分析 Unsupervised Graph Outlier Detection: Problem Revisit, New Insight, and Superior Method 图分析 Efficient and Compact Spreadsheet Formula Graphs 图分析 Inconsistency Detection with Temporal Graph Functional Dependencies 图分析 Keyword-based Socially Tenuous Group Queries 图分析 Community Search: A Meta-Learning Approach 图分析 Dynamic Activation of Clients and Parameters for Federated Learning over Heterogeneous Graphs 图分析 RETIA: Relation-Entity Twin-Interact Aggregation for Temporal Knowledge Graph Extrapolation 知识图谱 Disconnected Emerging Knowledge Graph Oriented Inductive Link Prediction 知识图谱 Relational Message Passing for Fully Inductive Knowledge Graph Completion 知识图谱 MMKGR: Multi-hop Multi-modal Knowledge Graph Reasoning 知识图谱 Robust Attributed Graph Alignment via Joint Structure Learning and Optimal Transpor 知识图谱 Multimodal Biological Knowledge Graph Completion via Triple Co-attention Mechanism 知识图谱 A Holistic Approach for Answering Logical Queries on Knowledge Graphs 知识图谱 时空、轨迹数据分析 论文 子方向 备注 TSC-AutoML: Meta-learning for automatic Time Series Classification Algorithm Selection 时空数据分析 TSEXPLAIN: Explaining Aggregated Time Series by Surfacing Evolving Contributors 时空数据分析 Towards Long-Term Time-Series Forecasting: Feature, Pattern, and Distribution 时空数据分析 PriSTI: A Conditional Diffusion Framework for Spatiotemporal Imputation 时空数据分析 REncoder: A Space-Time Efficient Range Filter with Local Encoder 时空数据分析 ROI-demand Traffic Prediction: A Pre-train, Query and Fine-tune Framework 时空数据分析 When Spatio-Temporal Meet Wavelets: Disentangled Traffic Forecasting via Efficient Spectral Graph Attention Networks 时空数据分析 Discovering Frequency Bursting Patterns in Temporal Graphs 时空数据分析 Matrix Factorization with Landmarks for Spatial Data 时空数据分析 Uncertainty Quantification for Traffic Forecasting: A Unified Approach 时空数据分析 Extreme-Aware Local-Global Attention for Spatio-Temporal Urban Mobility Learning 时空数据分析 Mining Seasonal Temporal Patterns in Time Series 时空数据分析 A Stitch in Time Saves Nine: Enabling Early Anomaly Detection with Correlation Analysis 时空数据分析 Self-Supervised Spatial-Temporal Bottleneck Attentive Network for Efﬁcient Long-term Trafﬁc Forecasting 时空数据分析 namic Hypergraph Structure Learning for Traffic Flow Forecasting 时空数据分析 Forecasting COVID-19 Dynamics: Clustering, Generalized Spatiotemporal Attention, and Impacts of Mobility and Geographic Proximity 时空数据分析 Double Hierarchical Labeling Shortest Distance Querying in Time-dependent Road Networks 时空数据分析 BERT-Trip: Effective and Scalable Trip Representation using Attentive Contrast Learning 轨迹数据分析 LHMM: A Learning Enhanced HMM Model for Cellular Trajectory Map-matching 轨迹数据分析 Contrastive Trajectory Similarity Learning with Dual-Feature Attention 轨迹数据分析 Online Anomalous Subtrajectory Detection on Road Networks with Deep Reinforcement Learning 轨迹数据分析 Towards Efficient MIT query in Trajectory Data 轨迹数据分析 A Lightweight Framework for Fast Trajectory Simplification 轨迹数据分析 Self-supervised Trajectory Representation Learning with Temporal Regularities and Travel Semantics 轨迹数据分析 RNTrajRec: Road Network Enhanced Trajectory Recovery with Spatial-Temporal Transformer 轨迹数据分析 Collision-Aware Route Planning in Warehouses Made Efficient: A Strip-based Framework 轨迹数据分析 Efficient Public Transport Planning on Roads 轨迹数据分析 Reinforcement Learning based Tree Decomposition for Distance Querying in Road Networks 路网分析 Global Routing Optimization In Road Networks 路网分析 Experimental Evaluation of Indexing Techniques for Shortest Distance Queries on Road Networks 路网分析 Finding Top-k Optimal Routes with Collective Spatial Keywords on Road Networks 路网分析 Data Imputation for Sparse Radio Maps in Indoor Positioning 地理信息分析 Semi-supervised Learning with Network Embedding on Ambient RF Signals for Geofencing Services 地理信息分析 A Contextual Master-Slave Framework on Urban Region Graph for Urban Village Detection 地理信息分析 Ai4DB 论文 子方向 备注 AutoCE: An Accurate and Efficient Model Advisor for Learned Cardinality Estimation 基数估计 Couper: Memory-Efficient Cardinality Estimation under Unbalanced Distribution 基数估计 Dynamic Materialized View Management using Graph Neural Network 查询优化 物化视图、图神经网络 Efficient Deep Ensemble Inference via Query Difficulty-dependent Task Scheduling 查询优化 GAR: A Generate-and-Rank Approach for Natural Language to SQL Translation NL2SQL Efficiently Learning Spatial Indices 学习索引 Towards Optimizing Storage Costs on the Cloud 存储 Data Ambiguity Profiling for the Generation of Training Examples NL2SQL Towards Explainable Table Interpretation Using Multi-view Explanations 表解释 Toward a Unified Framework for Unsupervised Complex Tabular Reasoning 表格推理 DBAugur: An Adversarial-based Trend Forecasting System for Diversified Workloads 工作负载预测 Active Sampling for Sparse Table by Bayesian Optimization with Adaptive Resolution 性能调优 Stable Tuple Embeddings for Dynamic Databases 数据库嵌入 数据库管理 论文 子方向 备注 S&#x2F;C: Speeding up Data Materialization with Bounded Memory 物化视图 Co-Utilizing SIMD and Scalar to Accelerate the Data Analytics Workloads 工作负载分析 Leopard: A Black-Box Approach for Efficiently Verifying Various Isolation Levels 事务 Knock Out 2PC with Practicality Intact: a High-performance and General Distributed Transaction Protocol 事务 Database Deadlock Diagnosis for Large-scale ORM-based Web Applications 死锁检测 LCL: A Lock Chain Length-based Distributed Algorithm for Deadlock Detection and Resolution 死锁检测 Sequence-Oriented DBMS Fuzzing 数据库安全 DBCatcher: A Cloud Database Online Anomaly Detection System based on Indicator Correlatio 数据库安全 存储、LSM Tree 论文 子方向 备注 ACEing the Bufferpool Management Paradigm for Modern Storage Devices 存储 Redesigning High-Performance LSM-based Key-Value Stores with Persistent CPU Caches 存储 Workload-Aware Log-Structured Merge Key-Value Store for NVM-SSD Hybrid Storage 存储 Real-Time LSM-Trees for HTAP Workloads LSM Tree 适用于 HTAP 工作负载的实时 LSM 树 查询、索引优化 论文 子方向 备注 ADAMANT: A Query Executor with Plug-In Interfaces for Easy Co-processor Integration 查询 Skyline Micro-Cluster Query: A Novel and Practical Spatial Query 查询 Ver: View Discovery in the Wild 查询 EulerFD: An Efficient Double-Cycle Approximation of Functional Dependencies 查询 Finding Best Tuple via Error-prone User Interaction 查询 KOIOS: Top-k Semantic Overlap Set Search 查询 Complete Join Reordering for Null-Intolerant Joins 查询 Efficiently Answering Top-k Window Aggregate Queries: Calculating Coverage Number Sequences over Hierarchical Structures 查询 On Explaining Confounding Bias 查询 MetricJoin: Leveraging Metric Properties for Robust Exact Set Similarity Joins 查询 Selecting Sub-tables for Data Exploration 查询 In-Situ Cross-Database Query Processing 查询 These Rows Are Made for Sorting and That’s Just What We’ll Do 查询 JanusAQP: Efficient Partition Tree Maintenance for Dynamic Approximate Query Processing 查询 dLSM: An LSM-Based Index for Memory Disaggregation 索引 Cutting Learned Index into Pieces: An In-depth Inquiry into Updatable Learned Indexes 索引 Indexing for Near-Sorted Data 索引 EEPH: An Efficient Extendible Perfect Hashing for Hybrid PMem-DRAM 索引 人工智能 论文 子方向 备注 Fed-SC: One-Shot Federated Subspace Clustering over High-Dimensional Data 聚类 Parallel Filtered Graphs for Hierarchical Clustering 聚类 Efficient Augmentation for Imbalanced Deep Learning 深度学习 Pricing Prediction Services for Profit Maximization with Incomplete Information 深度学习 Detection of Groups With Biased Representation in Ranking 深度学习 The Art of Losing to Win: Using Lossy Image Compression to Improve Data Loading in Deep Learning Pipelines 深度学习 Opponent-aware Order Pricing towards Hub-oriented Mobility Services 深度学习 Rotary: A Resource Arbitration Framework for Progressive Iterative Analytics 深度学习 Sudowoodo: Contrastive Self-supervised Learning for Multi-purpose Data Integration and Preparation 深度学习 ENLD: Efficient Noisy Label Detection for Incremental Datasets in Data Lake ENLD：数据湖中增量数据集的高效噪声标签检测 深度学习 Improving Progressive Retrieval for HPC Scientific Data using Deep Neural Network 深度学习 Lightweight-Yet-Efficient: Revitalizing Ball-Tree for Point-to-Hyperplane Nearest Neighbor Search 机器学习 Automatic Feasibility Study via Data Quality Analysis for ML: A Case-Study on Label Noise 机器学习 A Hierarchical Approach to Anomalous Subgroup Discovery 机器学习 Metam: Goal-Oriented Data Discovery 机器学习 Dynamic Shapley Value Computation 机器学习 SK-Gradient: Efficient Communication for Distributed Machine Learning with Data Sketch 机器学习 分布式机器学习 Distribution-Regularized Federated Learning on Non-IID Data 机器学习 分布式机器学习 Orchestrating Large-Scale SpGEMMs using Dynamic Block Distribution and Data Transfer Minimization on Heterogeneous Systems 机器学习 分布式机器学习 Enhancing Decentralized Federated Learning for Non-IID Data on Heterogenous Devices 机器学习 分布式机器学习 Toward Efficient Automated Feature Engineering 特征工程 PA-FEAT: Fast Feature Selection for Structured Data via Progress-Aware Multi-Task Deep Reinforcement Learning 强化学习 Discovering Editing Rules by Deep Reinforcement Learning 强化学习 Schema Matching using Pre-Trained Language Models NLP Online Pricing Strategy Achieving Vanishing Regret for Shipping Container Pricing with Limited Inventory 在线学习 流处理 论文 子方向 备注 CompressStreamDB: Fine-Grained Adaptive Stream Processing without Decompression 流处理 HistSketch: A Compact Data Structure for Accurate Per-Key Distribution Monitoring 流处理 HyperCalm Sketch: One-Pass Mining Periodic Batches in Data Streams 流处理 RADAR: Reactive Concept Drift Management with Robust Variational Inference for Evolving IoT Data Streams 流处理 SASPAR: Shared Adaptive Stream Partitioning 流处理 SketchConf: A Framework for Automatic Sketch Configuration 流处理 Parallelizing Stream Compression for IoT Applications on Asymmetric Multicores 流处理 Globally Aware Contextual Embeddings for Named Entity Recognition in Social Media Streams 流处理 Finding Simplex Items in Data Streams 流处理 数据分析与挖掘 论文 子方向 备注 IndiBits: Incremental Discovery of Relaxed Functional Dependencies using Bitwise Similarity 数据挖掘 UADB: Unsupervised Anomaly Detection Booster 数据挖掘 Learn to Explore: on Bootstrapping Interactive Data Exploration with Meta-learning 数据挖掘 Efficient Parallel Mining of High-utility Itemsets on Multicore Processors 数据挖掘 Benchmarking Filtering Techniques for Entity Resolution 数据挖掘 Voting-based Opinion Maximization 数据挖掘 Interactive Search with Mixed Attributes 数据挖掘 Planning Data Poisoning Attacks on Heterogeneous Recommender Systems in a Multiplayer Setting 数据挖掘 A Feature-Driven Fixed-Ratio Lossy Compression Framework for Real-World Scientific Datasets 数据压缩 Neural Node Matching for Multi-Target Cross Domain Recommendation 推荐 Incremental Learning for Multi-Interest Sequential Recommendation 推荐 Disentangled Graph Social Recommendation 推荐 Group Buying Recommendation Model Based on Multi-task Learning 推荐 多任务学习 Probabilistic Majority Rule-Based Group Recommendation 推荐 Sequential Recommendation with User Causal Behavior Discovery 推荐 Towards Capacity-Aware Broker Matching: From Recommendation to Assignment 推荐 Visualization Recommendation Through Visual Relation Learning and Visual Preference Learning 推荐 Personalized Diversification for Neural Re-ranking in Recommendation 推荐 A Competition-Aware Approach to Accurate TV Show Recommendation 推荐 Intent-aware Multi-source Contrastive Alignment for Tag-enhanced Recommendation 推荐 Instant Representation Learning for Recommendation over Large Dynamic Graphs 推荐 A Generic Reinforced Explainable Framework with Knowledge Graph for Session-based Recommendation 推荐 Contrastive Enhanced Slide Filter Mixer for Sequential Recommendation 推荐 Bayesian Negative Sampling for Recommendation 推荐 Influential Recommender System 推荐 边缘计算、区块链、物联网 论文 子方向 备注 EdgeNN: Efficient Neural Network Inference for CPU-GPU Integrated Edge Devices 边缘计算 FedKNOW: Federated Continual Learning with Signature Task Knowledge Integration at Edge 边缘计算 Reliable Transactions in Serverless-Edge Architecture 边缘计算 Ziziphus: Scalable Data Management Across Byzantine Edge Servers 边缘计算 Saguaro: An Edge Computing-Enabled Hierarchical Permissioned Blockchain 边缘计算 、区块链 SChain: Scalable Concurrency over Flexible Permissioned Blockchain 区块链 TxAllo: Dynamic Transaction Allocation in Sharded Blockchain Systems 区块链 On Sharding Across Heterogeneous Blockchains 区块链 Scaling Blockchain Consensus via a Robust Shared Mempool 区块链 Non-Blocking Raft for High Throughput IoT Data 物联网 Federated IoT Interaction Vulnerability Analysis 物联网 漏洞分析 视频处理 论文 子方向 备注 Marshalling Model Inference In Video Streams 视频处理 Track Merging for Effective Video Query Processing 视频处理 隐私保护 论文 子方向 备注 Dynamic Private Task Assignment under Differential Privacy 隐私保护 Differential Aggregation against General Colluding Attackers 隐私保护 KVSAgg: Secure Aggregation of Distributed Key-Value Sets 隐私保护 PrivNUD: Effective Range Query Processing under Local Differential Privacy 隐私保护 众包 论文 子方向 备注 Air-Ground Spatial Crowdsourcing with UAV Carriers by Geometric Graph Convolutional Multi-Agent Deep Reinforcement Learning 众包 Efficient Cross Dynamic Task Assignment in Spatial Crowdsourcing 众包 Batch-Based Cooperative Task Assignment in Spatial Crowdsourcing 众包 Exploring both Individuality and Cooperation for Air-Ground Spatial Crowdsourcing by Multi-Agent Deep Reinforcement Learning 众包 Hierarchical Crowdsourcing for Data Labeling with Heterogeneous Crowd 众包 Learning from Noisy Crowd Labels with Logics 从喧闹的人群标签中学习逻辑 众包 密码生成介绍关于口令生成的方向调研 个人总结[[方向：密码生成]] 安全一流会议S&amp;P、CCS、NDSS、USENIX Conference、IEEE tDSC 详情 论文 时间 级别 笔记 GENPass: A General Deep Learning Model for Password Guessing with PCFG Rules and Adversarial Generation 2018 PassGAN: A Deep Learning Approach to Password Guessing代码改进代码 2019 [[PassGAN]] Recurrent gans password cracker for iot password security enhancement 2019 Research on Password Cracking Technology Based on Improved Transformer 2020 [[Pass improved Transformer]] Generating optimized guessing candidates toward better password cracking from multi-dictionaries using relativistic GAN 2020 三区 [[REDPACK]] Densegan: A password guessing model based on densenet and passgan 2021 Dynamic Markov Model: Password Guessing Using Probability Adjustment 2021 C A large-scale analysis of the semantic password model and linguistic patterns in passwords综述 2021 B [[语义密码模型和密码语言模式分析]] LPG–PCFG: an improved probabilistic context-free grammar to hit low-probability passwords 2022 C On deep learning in password guessing, a survey 2022 C GNPassGAN: improved generative adversarial networks for trawling offline password guessing 2022 A [[GNPassGAN]] PassDiff: A New Approach for Password Guessing Using Diffusion Model阅读代码 2023 [[PassDiff]] OneRuleToFindThem: Efficient Automated Generation of Password Cracking自动生成密码规则 2023 A Systematic Review on Password Guessing Tasks密码生成的系统综述 2023 [[AReview]] Adaptive password guessing: learning language, nationality and dataset自适应密码猜测 2023 4区 SE# PCFG: Semantically Enhanced PCFG for Password Analysis and Cracking密码分析和破解语义的增强 2023 PassGPT: Password Modeling and (Guided) Generation with Large Language 2023 [[PassGPT]] A New Targeted Online Password Guessing Algorithm Based on Old Password 2023 B Improving real-world password guessing attacks via bi-directional transformers 2023 A [[PassBERT]] Password and passphrase guessing with recurrent neural networks 2023 (JCR:Q2) [[Pass_RNN]] Password guessing using random forest随机森林代码 2023 A [[PASS_RF]] PGTCN: A novel password-guessing model based on temporal convolution 2023 C Search-based Ordered Password Generation of Autoregressive Neural Networks 2024 C GuessFuse: Hybrid Password Guessing with Multi-View 2024 A [[GuessFuse]] A Two-Decade Retrospective Analysis of a University’s Vulnerability to Attacks Exploiting Reused Passwords凭据猜测攻击通常会利用在用户在线帐户中重复使用的密码。为了了解组织如何更好地保护用户，我们回顾性分析了我们大学二十年来遭受凭据猜测攻击的脆弱性。给定一份大学用户名列表，我们从数百个网站和十几个大型泄露事件汇编中搜索了两个数据泄露事件中的匹配项。在破解哈希密码并调整猜测后，我们成功猜测了 32.0% 与数据泄露中的大学电子邮件地址匹配的帐户的密码，以及 6.5% 与用户名（但不一定是域名）匹配的帐户的密码。这些帐户中的许多帐户在被泄露的数据泄露后多年仍然容易受到攻击，并且在泄露中逐字发现的密码被利用的可能性（即观察到可疑的帐户活动）几乎是经过调整的猜测的四倍。超过 70 种不同的数据泄露和各种用户名匹配策略引导了正确的猜测。在对我们猜测密码的 40 名用户进行的调查中，许多用户没有意识到他们的大学帐户存在风险，或者他们的凭据已被泄露。对我们大学密码重用的分析为组织保护帐户提供了务实的建议。 参考文章使用 RNN(LSTMs) 生成密码：代码不全 （1）（2）Generative Adversarial Networks can crack your password!（3）Password Cracker - Generating Passwords"},{"date":"2024-04-22T13:14:49.291Z","url":"/2024/04/22/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/%E4%BC%9A%E8%AE%AE%E8%B0%83%E7%A0%94/","categories":[["undefined",""]],"content":"[[中国计算机学会推荐国际学术会议和期刊目录-2019.pdf]] 会议级别ei 会议 &lt; ei 期刊 &lt; sci | ssci 会议信息 | 2023年12月截稿的密码学与信息安全会议整理 (qq.com) 主要依据 期刊CCF等级 未收录 JCR分区 Q2 JCR分区是通过将一个学科内收录的期刊总数分为四等份来进行的，其中影响因子排名前25%的期刊为一区（Q1），26-50%为二区（Q2），51-75%为三区（Q3），76-100%为四区（Q4） CCF分类则是中国计算机学会（CCF）对计算机科学领域的期刊和会议进行的评级，基于期刊和会议的国际影响力、学术水平等因素，通过将它们划分为不同的等级（A、B、C）， 中科院期刊分区根据13个大类学科，分别将各大类的SCI期刊由3年平均影响因子划分为 1 区（最高区）、2区、3区和4区四个等级。这种分区标准也被更多的机构采纳以作为科研评价的指标。"},{"date":"2024-04-22T13:14:49.273Z","url":"/2024/04/22/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/PASS_RF/","categories":[["undefined",""]],"content":"文档的重点是基于随机森林的密码猜测模型。它提出了一种新的技术路线，重新编码密码字符，使得可以应用经典的机器学习技术来进行密码猜测。具体来说，它使用随机森林、提升算法等经典的机器学习技术来处理多类分类问题，从而实现密码猜测。此外，它还提出了一种基于随机森林的框架RFGuess，该框架对三种最具代表性的密码猜测场景进行了特征化。实验证明，基于随机森林的猜测模型非常有效，在各种场景中都表现出色。同时，它还提出了一种新的PII匹配算法，能够更准确地捕捉用户的PII使用情况，并且可以改进主要猜测模型的成功率。总的来说，这篇文档提出了一种基于经典机器学习技术的密码猜测模型，并且在多个实验中证明了其有效性和实用性。 摘要对密码字符进行重新编码，并使一系列解决多类分类问题的经典机器学习技术（例如随机森林、增强算法及其变体）用于密码猜测成为可能。 此外，我们提出了 RFGuess，一种基于随机森林的框架，它描述了三种最具代表性的密码猜测场景（即拖网猜测、基于个人身份信息 (PII) 和用户密码重用行为的有针对性的猜测）。 这项工作除了具有理论意义外，还具有实用价值。 使用 13 个大型真实密码数据集进行的实验表明，我们的基于随机森林的猜测模型是有效的：（1）RFGuess 用于拖网猜测场景，其猜测成功率与最先进的同行相当； （2）基于PII的定向猜测RFGuess-PII，在100次猜测中猜测了20%∼28%的普通用户，比最先进的对手高出7%∼13%； (3) RFGuess-Reuse 基于用户密码重用&#x2F;修改行为进行定向猜测，在相关模型中表现最好或第二。 我们相信这项工作朝着将经典机器学习技术引入密码猜测迈出了实质性的一步。 如何工作 首先对密码字符进行重新编码，将每个密码字符表示为n阶字符串，其中n可以是4、5或6 然后将这些字符串表示为四个维度：字符类型、字符的排名、键盘行号和键盘列号，再加上两个额外的维度：字符在密码中的位置和字符在当前段中的位置。这种表示方法使得经典的机器学习技术（如随机森林和Boosting算法）首次成功应用于密码猜测。 随后，模型使用这些特征进行训练，并利用训练好的随机森林模型来生成密码猜测。 通过大量实验，研究表明RFGuess在不同的猜测场景中都取得了很好的效果，包括对PII的目标猜测。因此，RFGuess模型通过经典机器学习技术的应用，为密码猜测领域带来了重要的突破。 "},{"date":"2024-04-22T13:14:49.265Z","url":"/2024/04/22/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E5%8E%86/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E5%8E%86/","categories":[["undefined",""]],"content":"这里主要展示自己所参与过或者开发的项目，一些包含一些思考和总结。 分布式数据库[[TinyKV 项目要求]]"},{"date":"2024-04-22T13:14:49.258Z","url":"/2024/04/22/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E5%8E%86/TinyKV/TinyKV%20%E9%A1%B9%E7%9B%AE%E8%A6%81%E6%B1%82/","categories":[["undefined",""]],"content":"参考文档Smith-Cruise&#x2F;TinyKV-White-Paper: Tutorial for TinyKV project in Talent Plan. (github.com)源码解析 Project2 RaftKV背景知识Raft[[Raft 协议]] Raft GCRaft GC 是指在 Raft 一致性算法中用于清理日志条目的过程。在 Raft 中，每个节点都会维护一个日志，用于记录系统状态的变化。当日志变得庞大时，执行一致性操作的成本也会增加。为了限制日志的大小并提高性能，Raft 使用 Raft GC 机制来清理已经提交（committed）的日志条目。 Raft GC 的基本原则是，一旦某个日志条目已经被提交并应用到状态机，就可以安全地删除该条目之前的所有条目。删除旧的日志条目可以释放存储空间，并减少后续一致性操作的开销。Raft GC 通常通过一些策略（例如基于索引或时间）来判断哪些日志条目可以安全删除。 Snapshot（快照）Raft 中的快照是一种机制，用于在节点的状态机状态较新的情况下压缩和清理日志。当日志变得很大时，将整个日志传输给新加入的节点可能会很耗时和耗带宽。为了解决这个问题，Raft 使用快照机制来捕获节点的状态，并将其保存为一个快照文件。 快照包含了节点在某个特定时间点的状态机状态。当新节点加入集群时，它可以通过获取最新的快照来快速将自己带到当前状态，并只追加日志中从快照之后的新条目。这样可以大大减少传输和恢复的开销。 快照通常是通过在节点的状态机达到某个预定阈值时触发，或者通过节点之间的协商来创建。创建快照后，旧的日志条目可以被截断和删除，只保留最新的日志条目和快照文件。 任务目标 实现基本的 raft 算法。 在 Raft 上构建容错 KV 服务器。 添加 Raft GC 和 snapshot 的支持。 Part A代码 代码位于 raft&#x2F; 使用逻辑时钟实现，选举和超时时间 粗略阅读一下 eraftpb.proto ， 消息接收和发送的定义 注意，这里与标准 Raft 协议不同，将 HeartBeat 和 AppendEntries 分成不同的消息。 大致步骤： Leader election Log replication Raw node interface 实现 Raft 算法 Raft/Raft.go 提供了 Raft 算法的核心，包括消息处理、驱动逻辑时钟等。 Leader 选举 raft.Raft.tick() 可以将内部时钟前移一个 发送消息时仅将消息推入raft.Raft.msgs，因为测试代码会从这个里面的消息然后使用 raft.Raft.Step() 处理回应消息 make project2aa 测试。 日志复制 先在发送方和接收方实现处理 “MsgAppend” 和 “MsgAppendResponse” 。 raft/log.go 中的 raft.RaftLog 是关键结构。 需要通过raft/Storage.go中定义的Storage 接口与上层应用程序交互，以获取日志条目和快照等持久化数据。 make project2ab 进行测试。 实现 Rawnode 接口 raft/rawnode.go 中的 raft.RawNode 是与上层应用交互的接口。 包含 raft.Raft ，封装了时钟相关： RawNode.Tick()and RawNode.Step()，日志 ：RawNode.Propose() 包含一个新的 结构 Ready，处理消息或推进逻辑时钟时，raft.Raft 可能需要与上层应用程序交互，例如： 向其他对等方发送消息 将日志项保存到稳定的存储中 保存硬状态（如术语），提交索引，并投票到稳定存储 将提交的日志条目应用于状态机 等交互不会立即发生，相反，它们被封装在 “Ready” 中，并由 “RawNode” 返回。Ready（）到上层应用程序。这取决于上层应用程序何时调用 Ready（）并进行处理。在处理完返回的 Ready 之后，上层应用程序还需要调用一些函数，如 RawNode.Advance（） 更新 raft.Raft 的内部状态，如应用索引、稳定日志索引等。 “makeproject2ac” 来测试实现 也可以运行 “make project2a” 来测试整个A部分。 提示。 需要时添加任何状态到 raft.Raft, raft.RaftLog, raft.RawNode 和消息，在eraftpb.proto 中 测试中假设初始的 term 0 测试中假设新选举的 Leader 应该在任期内添加一个 noop 条目。 测试中假设一旦领导者推进其提交索引，它将通过 “MessageType_MsgAppend” 消息广播提交索引。 测试中不会为本地 Message，MessageType_MsgHup, MessageType_MsgBeat 和 MessageType_MsgPropose 添加 term 号, 领导者和非领导者的日志条目附加有很大不同，有不同的来源，检查和处理，请小心。 不要忘记，peers 之间的选举超时时间应该不同。 rawnode.go 中的一些包装函数可以通过 raft.Step(local message) 来实现 当启动一个新的 raft 时，从Storage中获取最后的稳定状态来初始化raft.Raft和raft.Raft Log Part B使用 part A 中 实现的 Raft 模块构建一个容错键值存储服务。 键&#x2F;值服务将是一个复制状态机，由多个使用 Raft 进行复制的键&#x2F;值服务器组成。 只要大多数服务器处于活动状态并且可以通信，尽管存在其他故障或网络分区，您的键&#x2F;值服务就应该继续处理客户端请求。 三个术语：“Store”、“Peer” 和 “Region”（定义在 “proto&#x2F;proto&#x2F;metapb.proto” 中） Store 代表tinykv-server 的一个实例 Peer 代表在 Store 上运行的 Raft 节点 Region是 Peers 的集合，也称为 Raft group (现在不需要考虑Region的范围。 项目3中将进一步引入多个区域。) 代码查看 kv/storage/raft_storage/raft_server.go 中的 RaftStorage ：实现了 Storage 接口（与 Project1 中的单机存储不同），首先将每个写入和读取请求发送到 Raft，然后在 Raft 提交请求后才对底层引擎进行实际的写入和读取。RaftStorage 创建一个 Raftstore 来驱动 Raft。 当调用 Reader 或 Write 函数时，它实际上通过通道（ 通道是“raftWorker”的“raftCh”），并在 Raft 提交并应用命令后返回响应。 Reader 和 Write 函数的 kvrpc.Context 参数现在很有用，它从客户端的角度携带 Region 信息，并作为 RaftCmdRequest 的 header 传递。 这些信息可能不正确或过时，因此 raftstore 需要检查它们并决定是否提出请求。接下来，就到了TinyKV的核心——raftstore。 结构有点复杂，阅读 TiKV 参考资料可以更好地理解设计： （中文版） （英文版） raftstore 的入口是 Raftstore，参见kv/raftstore/raftstore.go。 它启动一些工作线程异步处理特定任务，其中大多数现在不使用，因此您可以忽略它们。 你需要关注的是raftWorker。(kv&#x2F;raftstore&#x2F;raft_worker.go) 整个过程分为两部分：raftworker 轮询 “raftCh” 以获取消息，包括驱动 Raft 模块的基本 tick 和建议作为 Raft 条目的 Raft 命令； 它从 Raft 模块获取并处理就绪，包括发送 raft 消息、持久化状态、将提交的条目应用到状态机。 申请后，将响应返回给客户。 实现对等存储 peer storage对等存储是您通过 A 部分中的 “Storage” 接口进行交互的内容，但除了 raft 日志之外，对等存储还管理其他持久化元数据，这对于重启后恢复一致状态机非常重要 proto/proto/raft_serverpb.proto 中定义了三个重要的状态： RaftLocalState：用于存储当前 Raft 的 HardState 和最后一个 Log Index。 RaftApplyState：用于存储 Raft 最后应用的 Log 索引以及一些被截断的 Log 信息。 RegionLocalState：用于存储 Region 信息以及该 Store 上对应的 Peer 状态。 Normal 表示该 Peer 正常，Tombstone 表示该 Peer 已从 Region 中移除，无法加入 Raft Group。这些状态存储在两个 badger 实例中：raftdb 和 kvdb： raftdb 存储 raft 日志和 RaftLocalState kvdb 将键值数据存储在不同的列族 “RegionLocalState” 和 “RaftApplyState” 中。 可以把kvdb看成Raft论文中提到的状态机 why?实际上，可以只使用一个 badger 来存储 raft 日志和状态机数据。 分成两个实例只是为了与 TiKV 设计保持一致。 Key KeyFormat Value DB raft_log_key 0x01 0x02 region_id 0x01 log_idx Entry raft raft_state_key 0x01 0x02 region_id 0x02 RaftLocalState raft apply_state_key 0x01 0x02 region_id 0x03 RaftApplyState kv region_state_key 0x01 0x03 region_id 0x01 RegionLocalState kv 这些元数据应该在 “PeerStorage” 中创建和更新。 创建 PeerStorage 时，请参阅 “kv&#x2F;raftstore&#x2F;peer_storage.go”。 它会初始化该 Peer 的RaftLocalState、RaftApplyState，或者重启时从底层引擎获取之前的值。 注意，RAFT_INIT_LOG_TERM 和 RAFT_INIT_LOG_INDEX 的值都是 5（只要大于 1），而不是 0。之所以不设置为 0，是为了与 conf 更改后被动创建 Peer 的情况区别。 你现在可能还不太明白，所以请记住这一点，当你实现 conf 更改时，详细信息将在project3b 中描述。 这部分需要实现的代码只有一个函数：PeerStorage.SaveReadyState，该函数的作用是将 raft.Ready 中的数据保存到 badger 中，包括追加日志条目和保存 Raft 硬状态。 硬状态：更新 RaftLocalState.HardState 并保存。 要追加日志条目，只需将 “raft.Ready.Entries” 中的所有日志条目保存到 raftdb 并删除任何以前追加的永远不会提交的日志条目。 另外，更新对等存储的 “RaftLocalState”并将其保存到 raftdb。 提示： 使用 WriteBatch 立即保存这些状态。 有关如何读取和写入这些状态的信息，请参阅 peer_storage.go 中的其他函数。 设置环境变量 LOG_LEVEL&#x3D;debug 这可以帮助您进行调试，另请参阅所有可用的日志级别。 实施 Raft Ready 流程在 Part A 部分，构建了一个基于 tick 的 Raft 模块。 现在您需要编写外部进程来驱动它。 大部分代码已经在 kv/raftstore/peer_msg_handler.go 和 kv/raftstore/peer.go 下实现。 所以你需要学习代码并完成 proposeRaftCommand 和 HandleRaftReady 的逻辑。 以下是对该框架的一些解释。 Raft “RawNode” 已使用 “PeerStorage” 创建并存储在 “peer” 中。 在 Raft Worker 中，您可以看到它采用 “peer” 并通过 “peerMsgHandler” 包装它。 peerMsgHandler 主要有两个功能：一是 HandleMsg ，另一个是 HandleRaftReady。 HandleMsg 处理从 raftCh 接收到的所有消息，包括 MsgTypeTick 调用 RawNode.Tick() 来驱动 Raft，MsgTypeRaftCmd 包装来自客户端的请求，以及 MsgTypeRaftMessage 是 Raft 对等点之间传输的消息 。 所有消息类型都在 kv/raftstore/message/msg.go 中定义。 具体大家可以查看一下，其中一些会在后面的部分用到。 消息处理完毕后，Raft 节点应该有一些状态更新。 因此 HandleRaftReady 应该从 Raft 模块中做好准备并执行相应的操作，例如持久化日志条目、应用提交的条目并通过网络向其他对等点发送 raft 消息。 在伪代码中，raftstore 使用 Raft，如下所示： 之后读取或写入的整个过程将是这样的： 客户端调用RPC RawGet&#x2F;RawPut&#x2F;RawDelete&#x2F;RawScan RPC处理程序调用RaftStorage相关方法 RaftStorage 向 raftstore 发送 Raft 命令请求，并等待响应 RaftStore 将 Raft 命令请求作为 Raft 日志提出 Raft模块追加日志，并通过PeerStorage持久化 Raft模块提交日志 Raft Worker在Raft准备就绪时执行Raft命令，并通过回调返回响应 RaftStorage 接收回调的响应并返回到 RPC 处理程序 RPC 处理程序执行一些操作并将 RPC 响应返回给客户端。 您应该运行“make project2b”来通过所有测试。 整个测试运行一个模拟集群，包括多个带有模拟网络的 TinyKV 实例。 它执行一些读写操作并检查返回值是否符合预期。 需要注意的是，错误处理是通过测试的重要组成部分。 您可能已经注意到“proto&#x2F;proto&#x2F;errorpb.proto”中定义了一些错误，并且错误是 gRPC 响应的一个字段。 另外，实现了 error 接口的相应错误定义在 kv&#x2F;raftstore&#x2F;util&#x2F;error.go 中，因此您可以将它们用作函数的返回值。 这些错误主要与Region有关。 所以它也是RaftCmdResponse的RaftResponseHeader的成员。 当提出请求或应用命令时，可能会出现一些错误。 如果是这样，您应该返回带有错误的 raft 命令响应，然后错误将进一步传递给 gRPC 响应。 当返回带有错误的响应时，您可以使用 kv&#x2F;raftstore&#x2F;cmd_resp.go 中提供的 BindRespError 将这些错误转换为 errorpb.proto 中定义的错误。 在这个阶段，你可能会考虑这些错误，其他的将在 project3 中处理： ErrNotLeader：在跟随者上提议 raft 命令。 所以用它来让客户端尝试其他对等点。 ErrStaleCommand：可能由于领导者更改，某些日志未提交并被新领导者的日志覆盖。 但客户并不知道这一点，仍在等待回复。 因此，您应该返回此信息以让客户端知道并再次重试该命令。 TODO whyRAFT_INIT_LOG_TERM 和 RAFT_INIT_LOG_INDEX 的值都是 5（只要大于 1） Config.Applied uint64 干嘛用的 "},{"date":"2024-04-22T13:14:49.243Z","url":"/2024/04/22/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E5%8E%86/TinyKV/Raft%20%E5%8D%8F%E8%AE%AE/","categories":[["undefined",""]],"content":"Raft 协议参考：Raft-4：Raft 协议的基础工作流程_哔哩哔哩_bilibili知识点：Paxos核心原理：在多个副本之间进行投票和决策，以确保数据的一致性。两个阶段： prepare: 提案者提出一个提议，发送给其他所有参与者，每个参与者都参与投票。如果大多数参与者同意这个协议，则进入 accept 阶段。 accept：提案者再次向所有参与者发送一个包含提议编号和提议内容的消息，如果大多数参与者再次同意这个提议，则该提议被视为有效并被接受。关键特性： 去中心化设计：没有单一的领导者节点负责做出最终决策，相反，任何节点都可以成为提案者或者接收者。Raft：基本流程： 多个 server 共同选举产生一个 Leader，负责响应客户端的请求。 Leader 通过一致性协议，将客户端的指令发送到所有节点上。 每个节点将客户端的指令以 Entry的形式保存到自己的 Log 中，此时 Entry 状态为 uncommited 。 当有多数节点共同保存了 Entry 后，就可以执行 Entry 中的客户端指令，提交到 State 状态机中，此时 Entry 更新为 commited 状态。![[Pasted image 20240320211915.png]]理解 Raft 中的 Term： Raft 将时间线分割为多个 term （任期）； 一个任期有一个 id ，分两个阶段： 选举 election：选举出一个 leader。如果选举失败，此 term（任期）快速结束。 操作：进行执行任务。![[Pasted image 20240320211822.png]] 理解 Raft 中的 状态机制Raft 协议会为每一个服务器记录一个状态，状态有三种： 1. Follower：负责同步 Leader 的操作日志。 2. Candidate 3. Leader： 所有节点都是从 Follower 开始，Follower 负责同步 Leader 的操作日志。 Leader 会给 Follower 发送心跳。 Follower 收到心跳超时（随机值），转入 Candidate 状态，发起选举，如果成功，则变成 Leader； 新的 Leader 会向所有其他节点发送心跳。 此时其他节点无论是什么状态，都会变成 Follower![[Pasted image 20240320211748.png]] Raft 协议会为每一个服务器记录两个超时： 选举超时。 Raft 和 Paxos 类似，但是更容易理解，也更容易实现。Raft 主要是用来竞选主节点。单个 Candidate 的竞选有三种节点：Follower、Candidate 和 Leader。Leader 会周期性的发送心跳包给 Follower。每个 Follower 都设置了一个随机的竞选超时时间，一般为 150ms~300ms，如果在这个时间内没有收到 Leader 的心跳包，就会变成 Candidate，进入竞选阶段。 下图表示一个分布式系统的最初阶段，此时只有 Follower，没有 Leader。Follower A 等待一个随机的竞选超时时间之后，没收到 Leader 发来的心跳包，因此进入竞选阶段。 此时 A 发送投票请求给其它所有节点。 其它节点会对请求进行回复，如果超过一半的节点回复了，那么该 Candidate 就会变成 Leader。 之后 Leader 会周期性地发送心跳包给 Follower，Follower 接收到心跳包，会重新开始计时。 多个 Candidate 竞选 如果有多个 Follower 成为 Candidate，并且所获得票数相同，那么就需要重新开始投票，例如下图中 Candidate B 和 Candidate D 都获得两票，因此需要重新开始投票。 当重新开始投票时，由于每个节点设置的随机竞选超时时间不同，因此能下一次再次出现多个 Candidate 并获得同样票数的概率很低。 日志复制 来自客户端的修改都会被传入 Leader。注意该修改还未被提交，只是写入日志中。 Leader 会把修改复制到所有 Follower。 Leader 会等待大多数的 Follower 也进行了修改，然后才将修改提交。 此时 Leader 会通知的所有 Follower 让它们也提交修改，此时所有节点的值达成一致 "},{"date":"2024-04-22T13:14:49.243Z","url":"/2024/04/22/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E5%8E%86/TinyKV/TinyKV%20%E9%A1%B9%E7%9B%AE%E6%97%A5%E5%BF%97/","categories":[["undefined",""]],"content":"Project 1Project 2Project 2 整体架构Project 2AProject 2A 整体架构本部分实现基础的 Raft 算法，且不需要考虑快照操作。我们共需要实现三个模块，分别为 RawNode、Raft 和 RaftLog，分别对应文件 rawnode.go、raft.go 和 log.go ，这三个模块，共同构成一层，我将其称为 raft 层。结构图如下![[image2.png]] RawNode：该模块用来接收上层传来的信息，将信息下传给 Raft 模块。比如，上层传递一个 Msg 给 RawNode，这个 Msg 可能是 心跳请求、日志提交请求、日志追加请求等等。然后 RawNode 收到这个 Msg 之后，将其交给 Raft 模块去处理。比如，上层交给 RawNode 一个日志提交请求，然后 RawNode 将其交给 Raft ，如果该节点是领导人，那么其就会追加这条日志，然后发送给其他节点进行同步。另外，RawNode 不仅用来接受请求然后传递给 Raft，还要用来收到 Raft 的同步结果然后传递给上层。RawNode 要负责检查 Raft 模块是否已有同步好的日志需要应用、是否有 Msg 需要发送、是否有日志需要持久化等等，然后把这些信息（Ready）交给上层，上层会据此进行处理。总的来说，RawNode 是 raft 层暴露给上层的一个模块，用来进行信息的传递。 Raft：整个 raft 层最关键的就是它，它就是实现 Raft 算法的核心模块。其中，领导人选举、日志追加、角色转换等等均是在这里实现。Raft 模块会接受 RawNode 发来的信息，然后在进群中进行相关的同步操作，同步结果不需要主动返回给 RawNode，因为 RawNode 自己会进行检查。 RaftLog：该模块用来暂存节点的日志条目，同时还要维护一些日志指针，如 committed、applied 等等。 总结一下，RawNode 是 raft 层中暴露在外面的模块，用于该层与上层的信息交互，Raft 模块是 raft 层中的核心模块，算法的核心逻辑均在该模块实现，RaftLog 用来暂存日志信息，并维护相关指针。 Project 2 AA｜Project 2 AB消息类型以及基本逻辑 case pb.MessageType_MsgHup: 选举的本地消息，由 Tick() 触发。 如果发生选举超时，节点应将“MessageType_MsgHup”传递给其 Step 方法并开始新的选举，向其他所有节点发送 MsgRequestVote； 一旦收到了这个消息，说明就是要重新开始一次选举。 case pb.MessageType_MsgBeat: 本地心跳信息。表示当前节点的时钟前进，判断心跳超时和选举超时并进行相应的处理。 注意只有 Leader 需要处理这个消息。 case pb.MessageType_MsgPropose: 注意只有 Leader 会处理这个消息。 Leader 收到此消息需要将日志广播给其他节点。 发送时，如果要发送的 Index 已经被压缩了，转为发送快照。 否则发送 MsgAppend。 如果 MsgAppend 被接收者拒绝，Leader 会调整 next，重新进行前置判断，如果无需发快照，则按照新的 next 重新发送 MsgAppend。 发送方：由 Raft 层的上层构造此 msg 表示一次追加日志。此外，becomeLeader 时也会发送一个空 Entry 的 propose（也会被 广播）。 case pb.MessageType_MsgAppend: 发送方：如上，一般 propose 之后触发，由 Leader 发送给其他节点来同步日志条目。 接收方： 判断 Msg 的 Term 是否大于等于自己的 Term，是则更新，否则拒绝； 拒绝，如果 prevLogIndex &gt; r.RaftLog.LastIndex() 说明中间有日志缺失。否则往下； 拒绝，如果接收者日志中没有包含这样一个条目：即该条目的 Term 在 prevLogIndex 上不能和 prevLogTerm 匹配上。否则往下； 追加新条目，同时删除冲突条目，冲突条目的判别方式和论文中的一致； 当前节点更新 committedIndex 时，要比较 Leader 已知已经提交的最高的日志条目的索引 m.Commit 或者是上一个新条目的索引，然后取两者的最小值。为 接受； case pb.MessageType_MsgAppendResponse: 只有 Leader 会处理该 Msg，其余角色直接忽略； 如果被 reject 了，那么重置 next。重置规则为将旧的 next –，然后比较 m.Index + 1，最后取小的那个赋值给 next，然后重新进行日志 &#x2F; 快照追加； 如果没有 reject，则更新 match 和 next。next 赋值为 m.Index + 1，match 赋值为 next - 1 ； 按照论文的思路更新 commit。假设存在 N 满足N &gt; commitIndex，使得大多数的 matchIndex[i] ≥ N以及log[N].term &#x3D;&#x3D; currentTerm 成立，则令 commitIndex &#x3D; N。为了快速更新，这里先将节点按照 match 进行了递增排序，这样就可以快速确定 N 的位置。 case pb.MessageType_MsgRequestVote: 判断 Msg 的 Term 是否大于等于自己的 Term，是则更新，否则拒绝； 如果 votedFor 不为空或者不等于 candidateID，则说明该节点以及投过票了，直接拒绝。否则往下； Candidate 的日志至少和自己一样新，那么就给其投票，否者拒绝。新旧判断逻辑如下： 如果两份日志最后的条目的任期号不同，那么任期号大的日志更加新 如果两份日志最后的条目任期号相同，那么日志比较长的那个就更加新； Candidate 会通过 r.votes 记录下都有哪些节点同意哪些节点拒绝，当同意的票数过半时，即可成为 Leader，当拒绝的票数过半时，则转变为 Follower。 case pb.MessageType_MsgRequestVoteResponse: 只有 Candidate 会处理该 Msg，其余节点收到后直接忽略； 根据 m.Reject 更新 r.votes[m.From]，即记录投票结果； 算出同意的票数 agrNum 和拒绝的票数 denNum； 如果同意的票数过半，那么直接成为 Leader； 如果拒绝的票数过半，那么直接成为 Follower case pb.MessageType_MsgSnapshot: Project2C 中才会实现，所以该 Msg 在 Project2C 处 case pb.MessageType_MsgHeartbeat: 发送： 每当 Leader 的 heartbeatTimeout 达到时，就会给其余所有节点发送 MsgHeartbeat； 接收与处理： 判断 Msg 的 Term 是否大于等于自己的 Term，是则更新，否则拒绝； 重置选举计时 r.electionElapsed 发送 MsgHeartbeatResponse Commit: min(r.RaftLog.committed, r.Prs[to].Match), 确保了心跳消息中的 Commit 字段不会超过已经在目标节点上提交的日志索引，使得心跳消息不需要推进 Commit。 case pb.MessageType_MsgHeartbeatResponse: 发送： 当节点收到 MsgHeartbeat 时，会相应的回复 MsgHeartbeatResponse； 接收与处理： 只有 Leader 会处理 MsgHeartbeatResponse，其余角色直接忽略； 通过 m.Commit 判断节点是否落后了，如果是，则进行日志追加； case pb.MessageType_MsgTransferLeader: 用于上层请求转移 Leader，Project3 使用。 case pb.MessageType_MsgTimeoutNow: 节点收到后清空 r.electionElapsed，并即刻发起选举 消息使用的字段 MessageType\\msg_nr MsgType To From Term LogTerm Index Entries Commit Snapshot Reject Hup √ Beat √ Propose √ √ √ RequestVote √ √ √ √ √ √ requestVoteResponse √ √ √ √ 。 。 √ Heartbeat √ √ √ √ √ HeartbeatResponse √ √ √ √ Append √ √ √ √ √ √ √ √ AppendResponse √ √ √ √ √ √ Snapshot √ √ √ √ √ √ TransferLeader √ √ √ TimeoutNow √ √ √ 注意点： Hup 和 Beat 一定是本地消息，而 Propose 不一定是本地消息，可能是外部构造的不再 Raft 内部调用 Step 处理，所以：From 不使用但是 To 需要使用。 TimeoutNow 也是 本地消息，即刻发起选举。 对于 MessageType_MsgAppend： LogTerm 为要发送的条目的前一个条目的 Term，即论文中的 prevLogTerm Index 为要发送的条目的前一个条目的 Index，即论文中的 prevLogIndex 当前节点 (Leader) 的 committedIndex 对于 MessageType_MsgAppendResponse： Index：r.RaftLog.LastIndex()；该字段用于 Leader 更快地去更新 next Reject 两个功能，一个 投票一个 在 append 不同的是，propose 的 entry 无论是不是一个空的，都会被当成一个新的 entry 进行添加，而 Appen 的 entry，则可能被拒绝或者截取部分。 推进器 Step()Step() 作为驱动器，用来接收上层发来的 Msg，然后根据不同的角色和不同的 MsgType 进行不同的处理。 计时器 tick()该函数起到计时器的作用，即逻辑时钟。每调用一次，就要增加节点的心跳计时（ r.electionElapsed），如果是 Leader，就要增加自己的选举计时（ r.heartbeatElapsed），然后，应按照角色进行对应的操作。 log.go关键点｜易错点 raft 的 id 是从 0 开始的，但是 0 是无效 id ：const RaftInvalidIndex uint64 &#x3D; 0 entry 的 index 指的是 在 storage 中的 索引，而不是在数组中的下标。 下标计算： index - entries[0].Index 注意每一个 entry 都有一个自己的 term Leader 不会自己发起一个新的选举。 注意变成 Candidate 后发现，只有一个节点，再直接变成 Leader。 注意某一个消息处理时如果遍历 Prs 且跳过了当前节点的 id ，需要额外考虑一下集群只有一个节点的情况。 becomeCandidate 和 发送 请求 投票要分开进行 allEntries() 描述中需要排除 排除虚拟条目，但是实际上， &#x2F;&#x2F; if ent.Data !&#x3D; nil { &#x2F;&#x2F; 如果 这里的 ent 的 Data 是空的,说明 entry 无效 &#x2F;&#x2F; } 不需要排除，这个是在 测试中确定的。 handleRequestVoteResponse 除了统计同意的个数，还需要统计 拒绝的个数。 TODO随机选举时间问题 如果 et 太小，会过早开始选举，导致 term 比测试预期大。如果太大，会很晚发生选举，导致 term 比测试预期小。而且如果按照etcd那样一直递增，最后时间会非常长，直接卡住。我最后把它限制在 10~20 之间，通过测试。 leader 更新 committed 之后要告知 follower eader 收到 appendResp 后，会相应的更新自己的 committed，但是更新之后一定要把更新结果告知全部 follower，即另外发送一个 appendResp 。如果不这样的话，最后一轮完成后，集群的 committed 会不同步。并且为了防止死循环，leader 只能在 committed 发生变化的时候去通知 follower。 测试要求 msg 是nil，而不是空切片 TestRawNodeRestartFromSnapshot2C 中，want 里的 msg 为 nil，即测试点预期 newRaft 处的 msg 应该为 nil，而不是 make 一个空切片。 Project 2 AB空空如也 Project 2 ACRawNode 部分整体工作流程 代码解释： RawNode 是 Raft 的包装器，上层 ( peer 层) 会不停的调用 RawNode 的 tick() 函数，进一步触发 Raft 的 tick() 函数。 上层会定时从 RawNode 获取 Ready，首先上层通过 HasReady() 进行判断，如果有新的 Ready，上层会调用RawNode 的Ready()方法进行获取，RawNode 从 Raft 中 获取信息生成相应的 Ready 返回给上层应用，Raft 的信息则是存储在 RaftLog 之中。上层应用处理完 Ready 后，会调用 RawNode 的Advance() 方法进行推进，告诉 RawNode 之前的 Ready 已经被处理完成，然后你可以执行一些操作，比如修改 applied，stabled 等信息。 上层应用可以直接调用 RawNode 提供的 Propose(data []byte) ，Step(m pb.Message) 等方法，RawNode 会将这些请求统一包装成 Message，通过 Raft 提供的 Step(m pb.Message) 输入信息。 HasReady()RawNode 通过 HasReady() 来判断 Raft 模块是否已经有同步完成并且需要上层处理的信息，包括： 是否有需要持久化的状态； 是否有需要持久化的条目； 是否有需要应用的快照； 是否有需要应用的条目； 是否有需要发送的 Msg 其中，最后一点的 Msg，就是 Raft 模块中节点间相互发送的 Msg。也就是说，节点间发送的 Msg 是通过 RawNode 先发送给上层，然后上层在将其发送给对应节点的。 如果 HasReady() 返回 true，那么上层就会调用 Ready() 来获取具体要做的事情，和上述 HasReady() 的判断一一对应。该方法直接调用 rn.newReady() 生成一个 Ready() 结构体然后返回即可。 Advance()当上层处理完 Ready 后，调用 Advance() 来推进整个状态机。Advance() 的实现就按照 Ready 结构体一点点更改 RawNode 的状态即可，包括： prevHardSt 变更； stabled 指针变更； applied 指针变更； 清空 rn.Raft.msgs； 丢弃被压缩的暂存日志； 清空 pendingSnapshot； 思考&amp;总结 RawNode 是 Raft 的封装，保存 Raft 的一部分状态 Project 2A 总结&amp;思考一些思考 对于某个 Raft 变为 Candidate 提出选举，是当场给自己投一票 还是 同样构造一个 msg 发给自己处理？从 term 的角度思考。 1. 假设发消息。可能导致当前 Raft 先收到其他 Raft 的投票请求，从而自己没有给自己投票。 2. 因此当场给自己投一票。这样也减少了自己和自己的发消息多余操作。 如果当前的 Raft 就是自己本身，开始一个新的选举之后 term 要增加吗？ 增加。 如果超时，是先增加 term 还是先发送 MsgHup 消息？ 先增加 term。如果先发送消息，那么在消息队列中，这个消息前面的消息会认为这个 Raft 还是没有超时（实际上已经超时了） 先发心跳包等到响应再增加 Elapsed 还是 先增加？ 先增加。万一响应收不到了，Elapsed 还是要增加的。 Message 中 Commit 字段的作用？ 表示提交索引（commit index）。在 Raft 一致性算法中，每个节点都会维护一个提交索引。提交索引表示在该索引之前的所有日志条目都已经被安全地复制到了大多数节点，并且可以被应用到状态机中。换句话说，提交索引是已经达成共识并可以执行的日志条目的最高索引。 RaftLog 中 committed | applied | stabled 三个字段？ committed：已知在多数节点上的稳定存储中的最高日志位置。 applied：已经应用到状态机的最大 log 位置 stabled： 已经被持久化存储的最大 log 位置。 持久化存储中到底要存储什么？ 1. “Save log entries to stable storage”（将日志条目保存到稳定存储）：在 Raft 中，每个节点会维护一个日志（log），其中包含按顺序记录的操作或状态变化。当节点接收到来自客户端的命令或其他节点的日志复制请求时，它将这些日志条目追加到自己的日志中。为了确保数据的持久性和可靠性，节点需要将这些日志条目保存到稳定存储介质（如磁盘）上，以便在节点重启或发生故障时能够恢复日志的状态。 2. “Save hard state like the term, commit index, and vote to stable storage”（将任期、提交索引和投票信息等硬状态保存到稳定存储）：除了日志条目之外，Raft 还维护了一些重要的状态信息，称为硬状态。这些硬状态包括当前任期（term）、已提交的索引（commit index）和投票信息（如上一次投票的候选人ID等）。这些硬状态的变化需要被持久化保存，以确保在节点重启或发生故障时能够恢复到正确的状态。 MemoryStorage 中 ents[i] has raft log position i+snapshot.Metadata.Index 的含义？ ents[i] 表示 Raft 日志中的第 i 个条目。该条目的 Raft 日志位置是通过 i + snapshot.Metadata.Index 来确定的。 在 Raft 算法中，为了支持快照（snapshot）功能，可以将当前节点的状态和日志压缩成一个快照。快照包含了快照的元数据信息以及存储在快照中的状态和日志条目。 在代码中的 ents 切片中，存储了一系列的日志条目。为了与快照中的日志条目对应起来，ents[i] 的 Raft 日志位置是通过 i + snapshot.Metadata.Index 计算得出的。 具体来说，snapshot.Metadata.Index 表示快照元数据中的索引值，它指示了快照中第一个条目的索引。然后，对于 ents 切片中的第 i 个条目，其在 Raft 日志中的位置就是 i + snapshot.Metadata.Index。 这样的计算方式可以确保在应用快照之后，ents 切片中的日志条目仍然与 Raft 日志中的对应位置保持一致。 需要注意的是，snapshot.Metadata.Index 是快照元数据中的一个字段，用于表示快照中第一个条目的索引。在给出的代码中，snapshot 是一个 Metadata 类型的字段，可能是一个结构体或变量，用于存储快照的元数据信息。 也就是说 0 号位置的 Index 就是 snapshot.Metadata.Index RaftLog 中的 entries 和 Storage 中的 ents 有什么区别？ 1. entries 是 从下标 1 开始作为有效记录；ents 是从下表 0 开始作为有效记录。 2. .ents[0].Index 始终是有效的，用来作为基础索引 RaftLog.storage.Snapshot() 和 RaftLog.pendingSnapshot 中的区别 一个永久快照，一个临时快照。 心跳 heatbeat 时间和选举 election 时间的关系 1. heatbeat 只有 Leader 发送；election 时间是非 leader 需要维护的信息，以便即使开始一个选举。 收到陌生 Raft 的信息，该怎么处理？ 1. 可能是一个新的节点，需要添加到已有结构中。 2. 什么情况下添加？ 1. 首先只有这个新的 Raft 发送消息时才能被旧 Raft 知道，但是陌生 Raft 的工作消息直接处理可能会出问题，所以应该使用 heatbeat 作为新 Raft 的识别消息（谁收到了这个消息谁添加节点）。那么： 2. 选举过程： 1. Candidate 收到 heatbeat：多一个投票人，不会导致选举失败 。 2. 其他角色 收到 heatbeat：更不会影响选举。 3. 工作过程： 1. Leader 收到 heatbeat：多一个工作者，不会导致工作错误 。 2. 其他角色 收到 heatbeat：更不会影响工作（因为不会给 这个 Raft 发送工作消息）。 3. 总结：任何情况下，收到 新 Raft 的 heatbeat 时，可将其添加到当前 Raft 的记录中（最好同时向这个新 Raft 同步一下当前系统的情况）。 sendAppendResponse 不需要 回复 —-嘛嘛嘛？？？？？？？ 总结一下 commmit|applied|stable 的修改时机&#x2F;&#x2F; snapshot&#x2F;first…..applied….committed….stabled…..last&#x2F;&#x2F; ——–|————————————————|&#x2F;&#x2F; log entries RaftLog.committed 修改时机总结 在 newRaft() 中调用 newLog() 时被初始化为 storage 中 hardState 的 Commit Leader 在 发送 MessageType_MsgAppend 时发送当前 commited Raft 在收到 MessageType_MsgAppend 并更新 RaftLog.entries 之后，将 commited 置为以下两者的最小值 Leader 的 committed： m.Commit Leader 以为的 此 Raft 的 Committed：m.Index+uint64(len(m.Entries)) Leader 在收到 AppendEntriesResponse 之后 将自己的 commited 置为 当前 Term 的Entry 被大多数 Peers 中 都 Match 的最小 Index。 Leader 在 发送 MessageType_MsgHeartbeat 时发送 min(r.RaftLog.committed, r.Prs[to].Match) （确保心跳消息中的 Commit 字段不会超过已经和目标j节点同步的日志索引，从而使得心跳消息不会推进 committed ） Raft 收到 Lead 的 MessageType_MsgHeartbeat 之后，如果发现 m.Commit（Leader 认为的提交） &gt;&#x3D; r.RaftLog.committed （Raft 自认为的提交），说明当前 Raft 的 RaftLog.entries 中有一部分 Leader 认为还未提交（需要被覆盖）。修改 committed 值为 min(m.Commit, r.RaftLog.LastIndex()) RaftLog.applied 修改时机总结 在 newRaft() 中调用 newLog() 时被初始化为 storage 中 的 ent[0].Index 或者 初始化为 0，表示初始没有被 applied 的 entry。 在 Advance() 中第一次修改。 RaftLog.stabled 修改时机总结 在 newRaft() 中调用 newLog() 时被初始化为 storage 中 ent 最后一个 entry 的 Index。 r.RaftLog.stabled &#x3D; m.Index rn.Raft.RaftLog.stabled &#x3D; rd.Entries[len(rd.Entries)-1].Index r.Prs 的 Match &amp; Next 收到 Propose 时 if lastIndex &#x3D;&#x3D; 0 Propose Project 2BProject 2B 整体流程Project2B 实现了 rawNode 之上的上层应用，即真正开始多线程集群操作，引入了 peer 和 region 的概念。同时，除了实现上层的调用，Project2B 还需要通过调用 RaftStorage 中的接口真正实现写落盘。 store、peer、region 三者的关系如下：![[Pasted image 20240420190155.png]] Store：每一个节点叫做一个 store，也就是一个节点上面只有一个 Store。代码里面叫 RaftStore，后面统一使用 RaftStore 代称。 Peer：一个 RaftStore 里面会包含多个 peer，一个 RaftStore 里面的所有 peer 公用同一个底层存储，也就是多个 peer 公用同一个 badger 实例。 Region：一个 Region 叫做一个 Raft group，即同属一个 raft 集群，一个 region 包含多个 peer，这些 peer 散落在不同的 RaftStore 上。 这里将 Rawnode-Raft-RaftLog 统称为 raft 层，把要实现的部分称为 peer 层。peer 层首先接收来自 client 的 RaftCmdRequest，其中包含着不同的命令请求，接着它会把这些请求逐一以 entry 的形式传递给 raft 层，当然，这个 peer 应该是 Leader，不然 client 会找下一个 peer 继续试。raft 层收到条目后，会在集群内部进行同步，这就是 project2a 的内容。同步的过程中，peer 层会不时询问 raft 层有哪些已经同步好的 entry 可以拿来应用（执行）？哪些 entry 需要持久化？有没有快照需要应用？等等。三层的交互如下图所示：![[Pasted image 20240421125735.png]]此模块要完善两个文件，分别为 peer_msg_handler.go 和 peer_storage.go  TODOtype peer struct 中为什么&#x2F;&#x2F; Instance of the Raft module 是 RaftGroup *raft.RawNode一个 region 包含多个 peer 怎么理解 Project 2CProject 2C 整体流程 RawNode 中 hardState 和Project 3 技巧日志输出 总结TODO&#x2F;&#x2F; TODO 这里的 stabled 在 newlog 的时候赋值过一次，应该不会是 0 ，为什么会判断一次stabled if l.stabled &#x3D;&#x3D; 0 { return l.entries &#x2F;&#x2F; log.Infof(“stabled &#x3D; %d”, l.stabled) } &#x2F;&#x2F; TODO 什么时候 修改 committed sendAppend 时没有修改handle AppendEntriesRespinse 时修改了。 &#x2F;&#x2F; TODO 怎么处理 新 leader 中未 commit 的部分？从当前的逻辑来看，只有 当前 term 也产生 log 时 才会顺便更新前面的log &#x2F;&#x2F; TODO 第一次收到 Lead 的消息时，需要进怎么样的处理 &#x2F;&#x2F; TODO noop Entry 会发送吗， &#x2F;&#x2F; TODO r.Prs[r.id].Match 是 handlePropose 时修改。 TODO 搞清楚 Next 和 Match 之间的关系。TODO 快照的同步逻辑 参考 "},{"date":"2024-04-22T13:14:49.243Z","url":"/2024/04/22/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E5%8E%86/MimaPJ/MMPJ%20%E6%97%A5%E5%BF%97/","categories":[["undefined",""]],"content":"不行：nohup docker compose up &gt; &#x2F;dev&#x2F;null 2&gt;&amp;1 &amp;行：docker compose up -d121.48.161.86 插如新的字典信息：INSERT INTO File (filename, size, isSecret, fileType, accessGroupId, lineCount) VALUES (‘base01_FFXXFF.txt’, 25705231224, 0 , 0 , 1, 1000000000), (‘base02_FFXXFF.txt’, 25705106232, 0 , 0 , 1, 1000000000), (‘base03_FFXXFF.txt’, 25705041692, 0 , 0 , 1, 1000000000), (‘base04_FFXXFF.txt’, 25704512508, 0 , 0 , 1, 1000000000), (‘base05_FFXXFF.txt’, 25704626392, 0 , 0 , 1, 1000000000), (‘base06_FFXXFF.txt’, 9911144907, 0 , 0 , 1, 385579419); 关于剩余时间的添加： 已运行时间&#x2F;百分比， "},{"date":"2024-04-22T13:14:49.158Z","url":"/2024/04/22/%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE/STL/","categories":[["undefined",""]],"content":"Mapmap是STL的一个关联容器，它提供一对一的hash。 第一个可以称为关键字(key)，每个关键字只能在map中出现一次；第二个可能称为该关键字的值(value)； map以模板(泛型)方式实现，可以存储任意类型的数据，包括使用者自定义的数据类型。Map主要用于资料一对一映射(one-to-one)的情況，map內部的实现自建一颗红黑树，这颗树具有对数据自动排序的功能。在map内部所有的数据都是有序的，后边我们会见识到有序的好处。比如一个班级中，每个学生的学号跟他的姓名就存在著一对一映射的关系。 map的功能自动建立key － value的对应。key 和 value可以是任意你需要的类型。 使用 map使用map得包含map类所在的头文件 map对象是模板类，需要关键字和存储对象两个模板参数： 这样就定义了一个用int作为索引,并拥有相关联的指向string的指针. 为了使用方便，可以对模板类进行一下类型定义， 构造函数map共提供了6个构造函数，这块涉及到内存分配器这些东西，略过不表，在下面我们将接触到一些map的构造方法，这里要说下的就是，我们通常用如下方法构造一个map： 插入元素 第一种和第二种在效果上是完成一样的，用insert函数插入数据，在数据的 插入上涉及到集合的唯一性这个概念，即当map中有这个关键字时，insert操作是不能在插入数据的，但是用数组方式就不同了，它可以覆盖以前该关键字对 应的值，用程序说明如下： 上面这两条语句执行后，map中001这个关键字对应的值是“student_one”，第二条语句并没有生效。 那么这就涉及到我们怎么知道insert语句是否插入成功的问题了，可以用pair来获得是否插入成功，程序如下 我们通过pair的第二个变量来知道是否插入成功，它的第一个变量返回的是一个map的迭代器，如果插入成功的话Insert_Pair.second应该是true的，否则为false。 查找元素当所查找的关键key出现时，它返回数据所在对象的位置，如果沒有，返回iter与end函数的值相同。 刪除与清空元素 的大小在往map里面插入了数据，我们怎么知道当前已经插入了多少数据呢，可以用size函数，用法如下： map的基本操作函数： hash_maphash_map基于hash table（哈希表）。 哈希表最大的优点，就是把数据的存储和查找消耗的时间大大降低，几乎可以看成是常数时间；而代价仅仅是消耗比较多的内存。然而在当前可利用内存越来越多的情况下，用空间换时间的做法是值得的。另外，编码比较容易也是它的特点之一。 hash_map是一个哈希表容器，通常指的是std::unordered_map。在C++11标准之后，std::unordered_map已经取代了旧的std::hash_map。在std::unordered_map中，元素的存储位置是由哈希函数计算得出的。 基本原理使用一个下标范围比较大的数组来存储元素。可以设计一个函数（哈希函数，也叫做散列函数），使得每个元素的关键字都与一个函数值（即数组下标，hash值）相对应，于是用这个数组单元来存储这个元素；也可以简单的理解为，按照关键字为每一个元素“分类”，然后将这个元素存储在相应“类”所对应的地方，称为桶。 但是，不能够保证每个元素的关键字与函数值是一一对应的，因此极有可能出现对于不同的元素，却计算出了相同的函数值，这样就产生了“冲突”，换句话说，就是把不同的元素分在了相同的“类”之中。 总的来说，“直接定址”与“解决冲突”是哈希表的两大特点。 hash_map，首先分配一大片内存，形成许多桶。是利用hash函数，对key进行映射到不同区域（桶）进行保存。其插入过程是： 得到key通过hash函数得到hash值得到桶号(一般都为hash值对桶数求模)存放key和value在桶内。其取值过程是: 得到key通过hash函数得到hash值得到桶号(一般都为hash值对桶数求模)比较桶的内部元素是否与key相等，若都不相等，则没有找到。取出相等的记录的value。hash_map中直接地址用hash函数生成，解决冲突，用比较函数解决。这里可以看出，如果每个桶内部只有一个元素，那么查找的时候只有一次比较。当许多桶内没有值时，许多查询就会更快了(指查不到的时候). 由此可见，要实现哈希表, 和用户相关的是：hash函数和比较函数。这两个参数刚好是我们在使用hash_map时需要指定的参数。 例子 你没有指定hash函数和比较函数的时候，你会有一个缺省的函数，看看hash_map的声明，你会更加明白。下面是SGI STL的声明： 也就是说，在上例中，有以下等同关系： hash_map 的hash函数hash&lt; int&gt;到底是什么样子？看看源码: 原来是个函数对象。在SGI STL中，提供了以下hash函数： 也就是说，如果你的key使用的是以上类型中的一种，你都可以使用缺省的hash函数。当然你自己也可以定义自己的hash函数。对于自定义变量，你只能如此，例如对于string，就必须自定义hash函数。例如： 在声明自己的哈希函数时要注意以下几点： 1、使用struct，然后重载operator().2、返回是size_t3、参数是你要hash的key的类型。4、函数是const类型的。 现在可以对开头的string 进行哈希化了 . 直接替换成下面的声明即可： "},{"date":"2024-04-22T13:14:49.158Z","url":"/2024/04/22/%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE/%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/","categories":[["undefined",""]],"content":"两数之和关键两两匹配，且只匹配一次， 方法1思路两次for循环，计算每两个之间的和 空间复杂度1 时间复杂度$n^2$ 方法2思路先排序，再遍历 没有意义方法3思路用哈希，只遍历一次。 时间复杂度n 空间复杂度n"},{"date":"2024-04-22T13:14:49.156Z","url":"/2024/04/22/%E7%94%9F%E6%B4%BB%E7%90%90%E4%BA%8B/%E7%A5%9D%E8%AF%8D/","categories":[["undefined",""]],"content":"在这个充满喜悦和爱的日子里，我们聚集一堂，庆祝我亲爱的外甥和他的爱人步入婚姻的殿堂。作为家族的长辈，我感到无比的骄傲和幸福。 回望过去，我见证了我的外甥从一个天真无邪的孩童成长为今天这个成熟、负责任的年轻人。今天，他终于与心爱的人一起，准备开始自己的新生活，我感到非常的开心。 在此，我想对你们两位年轻人说几句话：亲爱的昕琦，作为你的外公，我深深为你感到骄傲。我着你长大，也看到你的善良、你的稳重与勇敢，我就知道，你绝对会是个好伴侣，也会是家里的那个靠山。李娜，我欢迎你加入我们的大家庭。我们也见过不少面了，你带来的，不仅是你俩之间的爱情，还有你的笑声、以及你那会让周围人都感到舒服的幽默感。我看得出来，你是那种能在任何情况下都能让昕琦感到支持和安心的人。我真心觉得，无论是好天还是坏天，你都会和（你的名字）肩并肩，一起撑起这个家。 接下来，我真心希望，从今往后的每一天，你们都能更加深入地了解对方，更加珍惜彼此。咱们都知道，生活偶尔会给我们来点小挑战，但只要你们一直手拉手，心连心，那么没有什么困难是过不去的。 最重要的，我期待你们能一起建立一个充满爱、笑声不断、幸福满满的家。就像你们从小在我们这个大家庭中感受到的那样温馨和美好。 此刻，我想要对你们表达我所有的祝福。愿你们的爱情永远美丽如今日，愿你们对未来的承诺，比今天的誓言更加坚定不移。 祝福你们，新婚快乐。 谢谢大家。"},{"date":"2024-04-22T13:14:49.125Z","url":"/2024/04/22/%E8%B5%9B%E4%BA%8B%E6%B4%BB%E5%8A%A8/%E8%93%9D%E6%A1%A5%E6%9D%AF/","categories":[["undefined",""]],"content":"院校报名时间：2023年10月7日-一—2023年12月15日 省赛：2024年4月13日9:00-13:00（软件类）&#x2F;9:00-14:00（电子类) 国赛：2024年6月中旬第十五届蓝桥杯大赛软件赛知识点大纲"},{"date":"2024-04-22T13:14:49.124Z","url":"/2024/04/22/%E8%B5%9B%E4%BA%8B%E6%B4%BB%E5%8A%A8/%E7%99%BE%E5%BA%A6%E4%B9%8B%E6%98%9F%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E5%A4%A7%E8%B5%9B/","categories":[["undefined",""]],"content":"6月开始，到12月"},{"date":"2024-04-22T13:14:49.123Z","url":"/2024/04/22/%E8%B5%9B%E4%BA%8B%E6%B4%BB%E5%8A%A8/%E5%95%8A%20%E6%AF%94%E8%B5%9B%E6%B1%87%E6%80%BB/","categories":[["undefined",""]],"content":"[[华为软件精英挑战赛]]每年 三月份 [[Google Code Jam]]已停办 [[ICPC 国际大学生程序设计竞赛]]可以尝试，且大概五月多才开始，可以提前准备。到那时本科入学五年内。但是，主要还是大学生 [[CCPC 中国大学生网络预选赛]]研究生不能参加最终决赛。算了，但是可以参考题目。 [[蓝桥杯]]可以 大概 4月份。报名 截至三月份 [[中兴捧月]]4月初 报名"},{"date":"2024-04-22T13:14:49.111Z","url":"/2024/04/22/%E8%B5%9B%E4%BA%8B%E6%B4%BB%E5%8A%A8/%E5%8D%8E%E4%B8%BA%E8%BD%AF%E4%BB%B6%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B/","categories":[["undefined",""]],"content":"时间线![[Pasted image 20240307091119.png]] 赛题● 华为基于自身ICT基础设施能力，通过对港口场景的洞察和理解，以智慧、绿色、高效、安全为目标，数字化、智能化为手段，联合生态伙伴，助力世界一流港口建设。 ● 本次赛题抽象自华为云智能港口真实业务难题，选手通过算法完成运输船只智能泊靠、运输机器人智能拣货装货等任务，以最大化提升港口物流效率。 初赛练习题 题目概述⚫目标赚取更多的资金。⚫ 程序操控方式选手作为运输公司来运输货物赚取资金，每个选手有 5 艘轮船、10 个机器人。选手需要使用机器人来执行移动、搬运等动作来完成物品递送任务，同时赚取利润。在运行结束时，选手拥有的资金数即为最终分数，所获得的资金越高越好。初赛时间为 15,000 帧（最多 5 分钟）。⚫ 程序交付方式选手程序通过标准输入和标准输出与判题器进行交互。判题器运行帧率为每秒 50帧，对于每一帧，判题器都会把场上的实时信息通过标准输入传递给选手程序，同时从选手程序读取机器人的操控指令作用到各个机器人上。每一帧有1000&#x2F;50&#x3D;20ms 的时间，由于判题器需保留 5ms 执行计算来模拟真实场景，故选手程序需要在 15ms 内做出每一帧的决策，如果超过 15ms 未做出决策，则系统将直接忽略这一帧的控制进入下一帧，并且在选手程序返回控制指令前，不会再发送状态数据给程序。注意，你不需要让自己的程序具备处理 50FPS 的性能，程序处理帧率低于 50FPS也能正常运行（例如只处理 10FPS 也可以），但是处理更高的帧率可以让你实现更高精度的控制。程序的输入和输出格式请参考 3.4 输入与输出格式。⚫ 判题器使用今年的比赛判题器与数据集完全开放给大家下载，并且做了跨平台设计（Windows&#x2F;Linux&#x2F;MacOS），大家可以根据自己习惯选择对应版本下载。但是请注意，比赛平台使用 Linux，因此无论你选择何种平台开发调试，都必须确保你的代码可以在 Linux 下编译运行。运行判题器中的 run_simple_demo 可快速运行一个 DEMO，运行界面如下： 题目分析环境配置docker run -itd -v &#x2F;home&#x2F;fengyuening&#x2F;container_18_04&#x2F;:&#x2F;root&#x2F; –gpus all –name fyn_ubuntu_18_04 ubuntu:18.04 ![[Y[Y%]@RY36}%AGFAHROTPCN.png]] ![[Pasted image 20240308135154.png]] 碰撞检测模块关键点 一个机器人只能拉一个货物 可以用 std::pthread 机器人不能斜走，只能横竖来走 考虑数据集的变化会导致算法出现的问题 如何避免多个机器人同时锁定同一个资源 怎么定义资源的性价比， 考虑距离港口的距离 考虑资源的价值，基本的性价比 考虑港口的负载 怎么防止机器人扎堆存在， 每个机器固定一个覆盖范围： 根据 怎么避免多个机器人锁定同一个货物 如果锁定了同一个货物 由于每一次到来 机器人不会与货物相撞我想让你担任心理健康顾问。我将为您提供一个寻求指导和建议的人，以管理他们的情绪、压力、焦虑和其他心理健康问题。您应该利用您的认知行为疗法、冥想技巧、正念练习和其他治疗方法的知识来制定个人可以实施的策略，以改善他们的整体健康状况。我的第一个请求是“我需要一个可以帮助我控制抑郁症状的人。 地图上有一个指针读到了当前机器人的位置，先简单点，后面的车躲避前面的车。l ![[Pasted image 20240309134153.png]] 类似于这样的方法，前面的小车标记的其实是下一个地点。先清除，先规划， 后续可以考虑通过判断移动来决定是否清除 存储机器人的上一个位置 在读到新的位置之后判断如果没有更改则不会清除这个位置，那么前面的机器人也能识别到之后机器人的位置 十个码头 五艘船 一开始如何评价这十个码头中最有价值的5个码头。 这里的码头类似与这样的实现： 机器人 主动搜索可行工作台还是工作台召唤机器人–&gt;由码头管理所有的产品信息，并且在机器人靠码头之后下发下一个产品目标 如果机器人靠码头之后这里的船没有到， 最终如果时间即将到达，应该立刻把轮船全部开走 假设当前机器人会在固定的码头上运输物品：关于机器人获取目标物品： 如果总是每次停靠码头后进行计算：会导致第一次计算时需要单独从机器人的角度出发，计算机器人到物品再到码头的时间。 如果总是从机器人出发，需要每次计算机器人到每一个物品的性价比考虑最优的选择， 但是实际上只有第一次是这么算的，之后全部在码头计算的，不如直接算在码头头上。 那么就是说现在是用码头来维护性价比信息。 两者统一 让机器人搜索最近最有价值的物品 可以考虑让所有的码头同时接收物品，船由码头的物品情况来选择： 但是这样的话码头中的物品其实是动态增加的，船也来动态的选择一个码头。 先寻路再避让一个问题：寻路算法 （如果C上方和下方都是障碍物的话会走入死胡同吗？不会，根据算法，这时候C会被直接放到关闭列表，没有发生任何节点的F更新和父节点更新）即不会有任何的节点以这个节点为父亲节点。A寻路算法C++简单实现_c++ 实现a算法-CSDN博客 如何给机器人目标 目前 机器人可以实现对目标的锁定和移动， 如何快速 当前要求分析总体要求：机器人 A* 的优化当前效果：输入：map start end输出：path：start-&gt;end每一次计算都是重新计算，有些耗费算历 JSP 算法 A*算法的一些概念current： 当前节点openset 开启节点集合，集合内节点有待进一步探索拓展closedset 关闭节点结合，集合内节点后续不再进行拓展，已经扩展过了neighbor 邻居，与当前节点相邻的节点parent(x) 节点 x 的父节点，即算法寻得的路径中节点 parent(x)的下一节点为 xG 值表示从起点到当前点路径耗费；H 值是一个期待值，当前点到终点的理论路径耗费；、F&#x3D;G+H表示经过该节点到终点的路径耗费 PS 算法的两条定义，三个规则定义一：强迫邻居（forced neighbour）：如果点 n 是 x 的邻居，并且点 n 的邻居有阻挡（不可行走的格子），并且从 parent(x)、x、n 的路径长度比其他任何从 parent(x)到 n 且不经过 x 的路径短，其中parent(x)为路径中 x 的前一个点，则 n 为 x 的强迫邻居，x 为 n 的跳点），例如图 2 中，寻找从 S 到 E的路径时，K 为 I 的强迫邻居（I 为 K 的跳点）。这里不认为从 H 到 K 能走，因为对角线有阻挡（这点和论文不一致，但和代码一致，因为如果 H 到 K 能直接到达，会走进 H 右边的阻挡区，大部分的 JPS 开源代码根据论文都认为 H 到 K能走，所以存在穿越阻挡的情况），如果需要 H 到 K 可走，则 K 为 H的强迫邻居（H 为 K的跳点）。 定义二：跳点（jump point）：（1）如果点 y 是起点或目标点，则 y 是跳点，例如图 2 中，S 是起点也是跳点，E 是目标点也是跳点；（2）如果 y 有邻居且是强迫邻居则 y 是跳点， 例 如 I 是跳点，请注意此类跳点和强迫邻居是伴生系，从上文强迫邻居的定义来看 n 是强迫邻居，x 是跳点，二者的关系是伴生的，例如图 2 中 K 的邻居只有I 是跳点，M 虽然也是 K的邻居，但 M 不是跳点，因为 K 不是 M 的强迫邻居；（3）如果 parent(y)到 y 是对角线移动，并且 y 经过水平或垂直方向移动可以到达跳点，则 y 是跳点，例如图 2 中 G 是跳点，因为 parent(G)为 S，S 到 G 为对角线移动，从 G 到跳点 I 为垂直方向移动，I 是跳点，所以 G 也是跳点。 规则一：JPS 搜索跳点的过程中，如果直线方向（为了和对角线区分，直线方向代表水平方向、垂直方向，下文所说的直线均为水平方向和垂直方向）、对角线方向都可以移动，则首先在直线方向搜索跳点，再在对角线方向搜索跳点。规则二：（1）如果从 parent(x)到 x 是直线移动，n 是 x 的邻居，若有从 parent(x)到 n 的路径不经过 x 且路径长度小于或等于从 parent(x)经过 x 到 n 的路径，则走到 x 后下一个点不会走到 n；（2）果从 parent(x)到 x 是对角线移动，n 是 x 的邻居，若有从 parent(x)到 n 的路径不经过 x 且路径长度小于从parent(x)经过 x 到 n 的路径，则走到 x 后下一个点不会走到 n（相关证明见论文）。规则三：只有跳点才会加入 openset，因为跳点会改变行走方向，而非跳点不会改变行走方向，最后寻找出来的路径点也只会是跳点集合的子集。"},{"date":"2024-04-22T13:14:49.111Z","url":"/2024/04/22/%E8%B5%9B%E4%BA%8B%E6%B4%BB%E5%8A%A8/%E5%8D%8E%E4%B8%BA%E8%BD%AF%E4%BB%B6%E7%B2%BE%E8%8B%B1%E6%8C%91%E6%88%98%E8%B5%9B%E2%80%94%E2%80%94%E9%85%8D%E7%BD%AE/","categories":[["undefined",""]],"content":""},{"date":"2024-04-22T13:14:49.110Z","url":"/2024/04/22/%E8%B5%9B%E4%BA%8B%E6%B4%BB%E5%8A%A8/%E4%B8%AD%E5%85%B4%E6%8D%A7%E6%9C%88/","categories":[["undefined",""]],"content":"![[Pasted image 20240418154011.png]]4月8日赛题发布后有且仅有一次修改，，4月初开始报名"},{"date":"2024-04-22T13:14:49.090Z","url":"/2024/04/22/%E8%B5%9B%E4%BA%8B%E6%B4%BB%E5%8A%A8/CCPC%20%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%BD%91%E7%BB%9C%E9%A2%84%E9%80%89%E8%B5%9B/","categories":[["undefined",""]],"content":"时间线8 月开始网络预选赛"},{"date":"2024-04-22T13:14:49.090Z","url":"/2024/04/22/%E8%B5%9B%E4%BA%8B%E6%B4%BB%E5%8A%A8/ICPC%20%E5%9B%BD%E9%99%85%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B/","categories":[["undefined",""]],"content":"每个正式参赛队的队员必须由且仅由 3 名学生组成，参赛队员必须来自同一学校且是所代表学校正式注册的学生， 参赛时距其高中毕业时间不超过 5年。每个队伍必须有 1 名教练， 教练必须是所在院校在职教师，负责竞赛活动的指导与组织工作。 ICPC竞赛内容包括但不限于经典的计算机科学分支，如程序设计语言、数据结构、算法设计方法、计算理论等，以及高等数学、计算几何、组合数学、图论、概率论、数论、博弈论、动态规划、人工智能、计算机图形学、搜索等。 3人共用1台电脑。 比赛时间为5个小时。 比赛题目为10 - 12道英文题。 每题可选用C、C++或Java语言之一编写。 程序实时测试，动态排名。 正确完成一题，得到一只代表该题颜色的气球。 比赛评判与排名规则如下： （1）选手提交的每一题必须在规定的时间限制、内存限制内正确完成，才算通过。 （2）每道题用时将从比赛开始到试题解答被判定为正确为止，期间每一次提交运行结果被判错误的话，将被罚20分钟时间，未正确解答的试题不计时（例如：A、B两队同时开始作答，两队先后完成了题目，A队比B队早交10分钟，但是，A队的作答错误，被打回修改，10分钟后再次提交正确的作答，原则上A、B两队用时相同，但是由于A队被打回，在时间上需要罚掉20分钟，因此，A队总用时比B队多20分钟，如此类推）。 时间线第48届（2023）ICPCQEC Final 2024年1月12-14日上海大学武汉邀请赛2024年5月1-2日武汉大学陕西邀请赛2024年5月18-19日 西北工业大学昆明邀请赛2024年5月25-26日云南大学丝绸之路邀请赛 2024年5月25-26日宁夏理工学院第一次网络选拔赛2024年9月？日第二次网络选拔赛2024年9月？日西安站 2024年?月?日 西北工业大学?南京站2024年?月?日 南京航空航天大学?沈阳站2024年?月?日 东北大学?港澳站2024年?月?日 香港理工大学、澳门大学?杭州站2024年?月?日杭州师范大学武汉站2024年?月?日武汉大学昆明站2024年?月?日云南大学第49届（2024）ICPCECFinal2025年1月？日上海大学?"},{"date":"2024-04-22T13:14:49.090Z","url":"/2024/04/22/%E8%B5%9B%E4%BA%8B%E6%B4%BB%E5%8A%A8/TEMP/","categories":[["undefined",""]]},{"date":"2024-04-22T13:14:49.090Z","url":"/2024/04/22/%E8%B5%9B%E4%BA%8B%E6%B4%BB%E5%8A%A8/Google%20Code%20Jam/","categories":[["undefined",""]]},{"date":"2024-04-22T13:14:49.078Z","url":"/2024/04/22/%E6%97%A5%E7%A8%8B%E5%AE%89%E6%8E%92/%E8%BF%91%E6%9C%9F%E8%AE%A1%E5%88%92/","categories":[["undefined",""]],"content":"2024-02-04 整理一下github如何搭建博客，其实就是建立一个仓库 这里的 内容相对来说其实其实就只是一个文档的上传功能， 整理一下 csdn 确实需要，但是这个整理是在逐渐完善文档之后才需要发布的 浏览工作网站，定成长路线。 需要下载几个软件 晚上睡觉看看 优化一下 go 的grpc，Go的 确实需要整理 整理一下 论文方向 老师要求看看大模型和字典生成相关的事情。 我是重点看大模型，–分支看看字典相关的事情 整理 图说数据库内容， 就两个部分，读文档，百度，google 抄就完事了 基本安排晚上睡觉看看博客啥的，就业相关晚上看看grpc 相关 和 go 语言， 优先 grpc 相关，整理为一个开源项目早上整理文档，数据库基本知识。 就业APP一、BOSS■优点1.回复率高。直接和hr或者boss谈的机制，能让你投递岗位后很快就能直接得到回应。2.招聘周期短，功能性岗位多。例如推销，营销，人力，行政，后勤等岗位，boss直聘的招聘效率都不错。■缺点1.薪资虚假，很多公司标的薪资和实际薪姿不符。2.审核把关不严，岗位质量良不齐。■推荐人群（应届生，工作1-3年的求职者）刚开始找工作或者是刚离职的求职者，往往急需更换工作，缩短求职周期，期待得到及时的面试反馈。 二、智联招聘&#x2F;前程无忧■优点1.基础性岗位，传统行业岗位都比较全面，大中小各类规模的企业覆盖比较齐全。基本上你想找的工作行业和类别，智联招聘都囊括了。2.软件下沉做得比较好，三四线及以下城市工作岗位比较多。■缺点1.反馈很慢，投递简历后石沉大海的情况比比皆是。2.推荐岗位不匹配，岗位和薪资挂羊头卖狗肉的情况也比较多，需要你擦亮眼睛。■推荐人群（全年龄段求职者）适合广撒网，多积粮的求职者，由于岗位覆盖面全，适合传统行业，基础性岗位，小众专业的求职者找工作，在三四线城市比较适用。 三、拉勾■优点1.主打互联网行业，垂直度和专业程度比较高。2.页面简洁，互联网岗位的搜索和筛选都很便捷。3.同类岗位推荐和一键投递功能不要太爽有木有，投简历就完事儿了！■缺点1.反馈也很慢，需要耐心等待。2.垂直度高既是优点也是缺点，其他行业的岗位不够全面。■推荐人群（互联网求职者首选）一线和新一线城市互联网岗位求职者的首选，想进互联网大厂，拉钩是个很好的选择。"},{"date":"2024-04-22T13:14:49.077Z","url":"/2024/04/22/%E6%97%A5%E7%A8%8B%E5%AE%89%E6%8E%92/%E8%8D%89%E7%A8%BF/","categories":[["undefined",""]],"content":"bellcrackit &#x3D;&gt; mimacracker bell &#x3D;&gt;mmck  Found existing lock.pid, checking if python process is running…Ignoringlock.pid filebecause PID is not existent anymore or not running python!Starting client ‘s3-python-0.7.1’…Please enter the url to the API of your MimaCracker installation: ![[Pasted image 20240319155428.png]]"},{"date":"2024-04-22T13:14:49.076Z","url":"/2024/04/22/%E6%97%A5%E7%A8%8B%E5%AE%89%E6%8E%92/%E5%B7%A5%E4%BD%9C%E6%97%A5%E5%BF%97/","categories":[["undefined",""]],"content":"[[近期计划]][[今日计划]] [[日程安排&#x2F;TEMP]] [[草稿]]"},{"date":"2024-04-22T13:14:49.057Z","url":"/2024/04/22/%E6%97%A5%E7%A8%8B%E5%AE%89%E6%8E%92/IDL/","categories":[["undefined",""]],"content":"认为神经网络将会成为未来最具潜力的模型，并指出该领域的进一步研究和发展依赖于规范数据集的建立和多种方法的结合运用 介绍当前存在的分类以及它们的瓶颈"},{"date":"2024-04-22T13:14:49.057Z","url":"/2024/04/22/%E6%97%A5%E7%A8%8B%E5%AE%89%E6%8E%92/%E4%BB%8A%E6%97%A5%E8%AE%A1%E5%88%92/","categories":[["undefined",""]],"content":" tinykv — OK 论文 —— OK 复现——-进一步实验。 老师要求的网络配置，使用虚拟机进行模拟。 周五下午来做。 准备实现，破解空间的测量，周五下午来做。 "},{"date":"2024-04-22T13:14:49.057Z","url":"/2024/04/22/%E6%97%A5%E7%A8%8B%E5%AE%89%E6%8E%92/TEMP/","categories":[["undefined",""]],"content":" 网络的端口服务测试 ok 密码空间优化 ok zip推荐 ok 剩余时间—后续 ok 下载相关离线包 ok bug — 密钥空间 ：因为后续计算密钥空间了！！，前面某些密钥空间是 0 bug—- .后缀 的 归档任务 详情无法打开。 bug — 进度： ok bug — 文件删除问题： TODO: 运行 md5 破解。do ing 下载字典有点久 整理包。 —- 整理指令。————— TODO Factory::getTaskFactory()-&gt;set($task, Task::KEYSPACE, $keyspace); DServerLog::log(DServerLog::TRACE, “已保存 Keyspace”, [$this-&gt;agent, $task]); $this-&gt;sendResponse(array( PResponseSendKeyspace::ACTION &#x3D;&gt; PActions::SEND_KEYSPACE, PResponseSendKeyspace::RESPONSE &#x3D;&gt; PValues::SUCCESS, PResponseSendKeyspace::KEYSPACE &#x3D;&gt; PValues::OK ) ); $task-&gt;setKeyspace(Factory::getFileFactory()-&gt;get($fileId)-&gt;getLineCount());Factory::getFileFactory()-&gt;get($fileId)-&gt;getLineCount(), msgs &#x3D;[{MsgRequestVote 1 1 0 0 0 [] 0 false {} [] 0} {MsgRequestVote 1 1 0 0 0 [] 0 false {} [] 0}{MsgRequestVote 2 1 0 0 0 [] 0 false {} [] 0} {MsgRequestVote 2 1 0 0 0 [] 0 false {} [] 0}{MsgRequestVote 3 1 0 0 0 [] 0 false {} [] 0} {MsgRequestVote 3 1 0 0 0 [] 0 false {} [] 0}],want [{MsgRequestVote 2 1 2 0 0 [] 0 false {} [] 0} {MsgRequestVote 3 1 2 0 0 [] 0 false {} [] 0}]"},{"date":"2024-04-22T13:14:49.042Z","url":"/2024/04/22/%E8%AE%A4%E7%9F%A5%E7%A4%BE%E4%BC%9A/caremyself/","categories":[["undefined",""]],"content":"知道它的目的，规划自己的行为 先适合，在选择进不进 核心价值观 基础+收集岗位信息 穷人最球多巴胺，富人追求内啡肽。 感觉自己思考问题的方式有问题，现在有点太在乎别人的看法，在乎别人对别人的看法。换种思考方式。。。。。。。。。。 你做过哪些 sql 优化。"},{"date":"2024-04-22T13:14:49.041Z","url":"/2024/04/22/%E8%AE%A4%E7%9F%A5%E7%A4%BE%E4%BC%9A/careall/","categories":[["undefined",""]],"content":"关于AIAI大模型通用但不好用大但不强成本还很高不找到好的商业闭环谁给你钱烧到AGI？"},{"date":"2024-04-22T13:14:49.007Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%95%B0%E6%8D%AE%E9%9B%86/","categories":[["undefined",""]],"content":" 数据集 网址 密码 Index of &#x2F;passwords&#x2F; (skullsecurity.org) WPA 密码  密码  "},{"date":"2024-04-22T13:14:49.007Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","categories":[["undefined",""]],"content":"监督学习与无监督学习监督学习任务：学习一个映射函数，给定任意输入响应做一个好的预测输出。本质：学习输入到输出的映射的统计规律。常见情景：回归，分类，标注。（区别在于变量的取值类型）（1）当输入变量和输出变量均为连续值变量时得到回归任务，它主要用于学习输入变量和输出变量之间的数值映射关系，常见的回归任务有价格预测、趋势预测等，处理回归任务时常用的机器学习模型有最小二乘回归、非线性回归等。（2）无论其输入变量为离散值还是连续值，当输出变量为有限个离散值时得到分类任务，分类任务是被人们讨论和应用最广泛的任务，它通常用于分门别类，常见的分类任务有图片类别识别、用户分类、文本分类等，处理分类任务时常用的机器学习模型有：k近邻、朴素贝叶斯、决策树、逻辑斯蒂回归模型、支持向量机、神经网络等。（3）当输入变量和输出变量均为变量序列时得到标注任务，它是分类问题的一种推广，用于学习输入序列和输出序列的映射关系，典型的标注任务有自然语言处理中的词性标注、信息抽取等，处理标注任务时常用的机器学习模型有隐马尔科夫模型和条件随机场等 分类监督学习是学习一个模型，然后利用该模型对给定的输入预测相应的输出，我们可将模型写成函数形式 Y&#x3D;f(X) 或条件概率分布形式 P(Y|X) 。 判别模型&amp;生成模型：根据条件概率的计算方式 [[判别模型]] 建模方式：直接对 P(Y|X) 进行建模，它试图描述在给定输入特征 X 的情况下，标签信息 Y 的分布， 典型判别模型包括： 近邻法、感知机、决策树、逻辑回归和条件随机场等。 评价：判别模型对条件概率模型直接建模，无法反映训练数据本身的概率特性，但是以分类问题为例，判别模型在寻找最优分类面的过程中，学习了不同类别数据之间的差异。另外，判别模型可以对数据进行各种程度上的抽象、降维，因此可以简化学习问题，学习准确率更高。 [[生成模型]] 对数据特征 X 和标签 Y 的联合分布 p(X,Y) 进行建模，然后利用条件概率公式，即可计算 p(Y|X) ，如下所示: $p(Y|X) &#x3D; \\frac {p(X,Y)}{p(X)}$ 一般将其转换为易为计算的方式，如下所示 $p(Y|X) &#x3D; \\frac {p(X|Y)*p(Y)}{p(X)}$ 举例：朴素贝叶斯方法和隐马尔科夫模型等。 在朴素贝叶斯方法中，我们通过训练集学习到先验概率分布 p(Y) 和条件概率分布 p(Y|X)，则可得到联合概率分布 p(X,Y)； 隐马尔可夫模型中，我们通过训练集学习到初始概率分布、状态转移概率矩阵和观测概率矩阵，则得到了一个可以表示状态序列与观测序列联合分布的马尔可夫模型。 评价：生成模型直接学习联合分布，可以更好地表示数据的分布，更好反映同类数据的相似度。当样本数量比较大时，生成模型往往可以更好地收敛到真实模型上，其收敛速度快。另外，生成模型可以处理含有隐变量的情况，而判别模型对此无能为力。生成模型也可以通过计算边缘分布而检测某些异常值。但实践中，生成模型计算开销一般比较大，而且多数情况下其效果不如判别模型。 无监督学习和监督学习比较： 无监督学习和监督学习最大的区别就是标签的有无。在监督学习中，训练模型的任务是学习输入特征到标签的映射， 而无监督学习中只有样本的特征向量，故无监督学习的任务是对数据进行深入“挖掘”，其本质是学习数据中的统计规律或潜在结构。对于无监督学习的深入研究对深度学习的复兴上起到了关键的作用。 相比于无监督学习除了拥有额外的标签信息外，还需要有测试样本。机器学习模型在训练集中学习“规律”，然后对测试集使用这种“规律”来评价模型的效果，而无监督学习不需要测试样本，整个过程只需要训练集的参与。 另外，无监督学习相比于监督学习一般拥有更好的拓展性，它在完成训练目标的同时，通常还额外学习到了样本的表示，我们可以将这些表示直接用于其他的任务。常见任务：降维、聚类、概率模型估计。（1）降维任务主要用于处理数据的高维度问题，真实数据的特征维度过大容易造成模型的拟合度与可用性降低，我们可以通过降维算法对高维度数据进行“压缩”使之变成低维度向量，从而提高数据的可用性，常用的算法有主成分分析、因子分析、隐含狄利克雷分布等，包括早期的自编码器也可用于数据降维。（2）聚类任务主要将样本依据一定的规则进行类别分配，即通过衡量样本之间的距离、密度等指标，将关系“近”的样本聚为同一类，以此实现样本的自动分类，常用的算法有层次聚类、k-means聚类、谱聚类等。 （3）在概率模型估计任务中，对于一个可以生成样本的概率模型，我们使用样本对概率模型的结构、参数进行学习，使得概率模型生成的样本与训练样本最相似。其中一种比较简单的概率密度估计任务便是对随机变量的概率密度函数进行学习，常用的算法有极大似然估计、对抗生成网络、变分自编码器等，这部分内容非常丰富。 半监督学习 半监督学习是介于监督学习和无监督学习的一种方式，即只有小部分训练样本带有标签信息，而大多数训练样本的标签信息空缺。半监督学习包括直推和归纳两类模式，直推半监督学习只对给定的训练数据进行处理，它使用训练数据集中有类别标签和无类别标签的样本进行训练，预测其中无标签样本的标签信息；归纳半监督学习不仅预测训练数据集中无标签样本的标签，更主要的是预测未知样本的标签，两者的区别在于需要预测标签的样本是否出现在训练集中。半监督学习一般用于四类学习场景：半监督分类、半监督回归、半监督聚类、半监督降维等。 上下文无关语法上下文无关语法（Context-Free Grammar）和概率上下文无关语法（Probabilistic Context-Free Grammar）的概念。 上下文无关语法是一种形式语言描述方法，用于定义一类语言的语法结构。它由一组产生式规则组成，每个规则包含一个非终结符和一个由非终结符和终结符组成的字符串。其中，α是一个单变量，表示非终结符，而β是由变量或最终值组成的字符串。这些产生式规则定义了从一个起始样本开始，通过替换非终结符，逐步生成包含所有最终值的字符串集合。上下文无关语法的特点是，无论α出现在哪个上下文中，都可以自由地用β替换，而不需要考虑α的上下文。 概率上下文无关语法是在上下文无关语法的基础上引入了概率特性。每个产生式规则都被赋予一个概率值，表示该规则被应用的概率。这样，概率上下文无关语法可以用于建模具有统计特性的语言。例如，在自然语言处理中，可以使用概率上下文无关语法来生成句子或解析句子的结构，并为每个规则分配适当的概率。 总结来说，上下文无关语法是一种用于描述语言的语法结构的方法，其中产生式规则定义了从起始样本开始生成所有最终值的字符串。概率上下文无关语法在上下文无关语法的基础上引入了概率特性，使其适用于建模具有统计特性的语言。 多视图学习多视图学习（Multi-view Learning）是一种机器学习方法，旨在利用来自多个视图或多个特征表示的数据来改善学习性能。在多视图学习中，数据样本可以从不同的视角或特征空间中获取多个不同的表示。通过综合这些多个视图的信息，多视图学习可以提供更全面和准确的数据描述，从而改善模型的泛化能力和学习结果。 传统的机器学习方法通常假设数据特征是从单个视图或特征空间中提取的，因此忽略了不同视图之间的相关性和互补性。而多视图学习则通过融合多个视图的信息来克服这个限制。它可以应用于各种领域和任务，如模式识别、图像处理、文本分类、推荐系统等。 多视图学习的关键挑战是如何有效地利用不同视图之间的相关性。常见的多视图学习方法包括以下几种： 基于特征融合的方法：将不同视图的特征进行融合，生成一个更综合和丰富的特征表示。常见的融合方法包括特征级融合、决策级融合和模型级融合等。 基于共享表示学习的方法：通过学习一个共享的低维表示空间，将不同视图的数据映射到该共享空间中。这样可以使不同视图之间的相关性更加明显，便于后续的学习和推理。 基于多示例学习的方法：将多个视图看作是一个示例的不同表示，通过多示例学习的方式来进行模型训练和预测。这种方法适用于存在不完全标注的数据集，其中每个示例可能有多个视图的表示。 多视图学习方法可以提供更全面和准确的数据建模，从而改善学习性能。它可以利用不同视图的互补信息，提取更丰富的特征表示，并减少数据表示的不确定性。这使得多视图学习成为处理复杂数据和提高模型性能的有效工具。 随机投影（SimHash）TODO： SimHash 算法：SimHash 是一种用于计算文本或数据的哈希值的算法。它的主要思想是将文本或数据转换为二进制向量，其中相似的文本或数据会产生相似的哈希值。这种相似性哈希算法被广泛应用于文本去重、相似文档聚类和相似性搜索等任务。 随机投影：随机投影是一种降维技术，用于将高维数据映射到低维空间。在随机投影 SimHash 中，通过使用随机生成的投影矩阵将高维特征向量映射到低维二进制码。 哈希函数：随机投影 SimHash 使用哈希函数来将投影后的低维向量转换为二进制码。常用的哈希函数是符号哈希函数，它根据投影后的特征向量的符号（正负）来决定对应二进制码的取值（0 或 1）。 相似性匹配：通过计算 SimHash 值之间的汉明距离（Hamming Distance），可以判断文本或数据之间的相似性。汉明距离是指两个等长字符串之间相对位置不同的字符的个数。汉明距离越小，表示文本或数据之间越相似。 线性特征 &amp; 非线性特征 线性特征：特征和目标的关系可以用一条直线来拟合。 非线性特征：特征和目标之间的关系不可以用一条直线来拟合 弱分类器 &amp; 强分类器 弱分类器：准确率在 60% ~80%之间，即：比随即预测好，但是准确率不高。e.g. CART（分类与回归树） 强分类器：准确率在90%以上。 分类任务 &amp; 回归任务 分类任务（Classification） 目标：将输入实例分配到预定义的类别中。 过程：模型通过学习输入特征与响应类别之间的关系，来预测新的未知示例所属类别。 输出：输出是离散的，通常是表示类别的标签或类别的概率分布。 回归任务（Regression） 目标：预测连续的数值输出。 过程：模型通过学习输入特征与响应输出值之间的关系，来预测新的未知示例的数值结果。 输出：这是一个连续的数值输出。 注意：有些机器学习算法可以同时用于分类和回归任务，例如决策树和支持向量机等。这些算法可以根据任务的要求进行适当的调整和配置。 LR、DT、SVM的对比 所谓分类问题就是在特征空间内寻找决策边界线。而三种算法决定了生成的边界线的不同形状。 如何在多维特征空间中选择合适的算法： 先选逻辑回归，如果效果不怎么样，可以将它的结果作为基准来参考 试试决策树（随机森林）是否可以大幅度提升模型性能。即使没有把它当作最终模型，也可以使用随机森林来移除噪声变量。 如果特征的数量和观测样本特别多，那么当资源和时间充足时，使用SVM不失为一种选择。 逻辑回归（Logistic Regression） 目的：解决分类问题。 核心：特征权重的线性组合、sigmoid 函数的计算和损失函数的最小化。 特点：逻辑回归的决策边界总是一条直线（或者一个平面，在更高维度上是超平面）。 优势： 适用于处理接近线性可分的分类问题。 结果不是一个离散值或者确切的类别。而是一个与每个观测样本相关的概率列表，所以可以用不同的标准和常用的性能指标来分析这个概率分数，得到不同的分类结果。 时间和内存需求上相当高效。可以用于分布式数据，用较少的资源处理大型数据 低内存消耗：逻辑回归模型只需要存储特征权重，而不需要存储大量的训练数据。相比之下，其他复杂的模型（如神经网络）可能需要存储大量的中间参数和计算图，导致更高的内存消耗。 对数据中小噪声的棒鲁性很好。 逻辑回归广泛应用于工业问题上。 地位：解决工业规模问题最流行的算法 缺点： 在效率和算法实现的易用性方面并不出众。 当特征数目很大并且还丢失了大部分数据时，逻辑回归就会表现的力不从心。 当类别变量过多时也会力不从心 对于非线性特征，需要进行转换。 依赖于全部数据。 决策树（Decisoin Trees） 目的：解决分类问题 &amp; 逻辑回归问题。 结构：按照层次结构的规则生成的。 特性：对单向变换或者非线性特征并不关心。(不需要变换来捕获数据中的非线性相关性，可以用他的划分方式自适应处理非线性关系)。 单向变换：如指数、对数变换。 优势：如果边界是非线性的，并且能通过不断将特征空间分为矩形来模拟，那么决策树是比逻辑回归更好的选择。 直观的决策规则； 可以处理非线性特征； 考虑了变量之间的相互作用； 缺点： 训练集上的效果高于测试集，即过拟合【随机森林克服了此缺点】； 没有将排名分数作为直接结果； 针对离散数据的分类决策树 定义：预测任务的输入和输出都是离散值 例子：ID3、C4.5 原始决策树：不断选择，优先选择信息熵最小的特征进行分组 信息熵：越大，表示特征的信息量越大，越离散，按照这个特征分组之后，样本的混乱程度越大。e.g. 特征某个水平的值出现的概率与取对数的积和。 ID3：使用信息增益来度量特征对分类的帮助大小 信息增益：使用一个特征对数据进行分组之后各组样本的有序程度会更高，熵会降低，分组前后熵的差值就是这个特征带来的信息增益。信息增益越大，说明这个特征越有助于分组。分组之前算一次，分组之后算一次。 C4.5 算法：在信息增益的基础上构造了一个新的特征质量度量指标：信息中增益比 信息增益比：按照性别划分之后对成年的的信息增益&#x2F;分组之前对性别的信息增益 支持向量机（Support Vector Machine,SVM） 目的：解决分类问题 &amp; 逻辑回归问题 特点：依靠边界样本来建立需要的分离曲线。它可以处理非线性决策边界。（对边界依赖） 结构：把特征空间映射到核空间，使得各个类别线性可分。把特征空间又增加一个维度。 核函数：SVM使用核函数来将输入特征映射到高维特征空间，从而使得原本在低维空间中非线性可分的问题在高维空间中变得线性可分。核函数的作用是通过计算样本在高维空间中的内积来隐式地表示非线性特征之间的相互作用。常用的核函数包括多项式核函数、高斯核函数（径向基函数）等。 大间隔原则：SVM的优化目标是找到一个最大间隔的超平面来划分不同类别的样本。通过最大化间隔，SVM能够在特征空间中找到一条边界，使得不同类别的样本尽可能分开。这种大间隔原则使得SVM对于非线性特征之间的相互作用更加鲁棒，能够更好地处理非线性关系。 非线性核函数：除了线性核函数，SVM还可以使用非线性核函数，如多项式核函数和高斯核函数。这些核函数能够捕捉非线性特征之间的相互作用，将数据映射到高维特征空间中，并在高维空间中构建一个线性超平面来进行分类。这样，SVM能够处理非线性特征之间的相互作用，提高模型的表达能力。 优点 能够处理大型特征空间 能够处理非线性特征之间的相互作用 无需依赖整个数据 缺点： 当观测样本很多时，效率并不是很高 有时候很难找到一个合适的核函数 分类与回归树（Classification and Regression Tree,CART） 概念：一种经典决策树，可以用来处理涉及连续数据的分类或者回归任务。 思想：一些学者采用类似随机投影的思路，将自变量的取值空间切分为若干个碎块，并假设这个空间碎块内的所有样本的因变量取值接近(甚至相同)——在这种思想的指导下，出现了一种非常经典的回归模型，即CART回归树。 由来： 特征为连续变量：不能直接使用特征取值，选择用于分割样本的特征取值 输出为连续变量：基尼系数和信息增益并不能作为分组质量的表征。使用！回归树！ 关键：设计一个标准，用来指导机器按照最有利于准确计算因变量的情况来切分特征空间。 e.g.:切分特征空间的标准：MES 代码： 集成学习概念：通过组合多个基本模型的预测结果，以获得更好的整体预测性能。 目的：组合多个弱分类器或者回归器来创建一个强分类器或者回归器。 bagging 通过对原数据集的抽样，得到多份采样数据集，使用弱学习器分别在这多份采样数据集上学习， 而后使用集成策略将结果整合起来（e.g. 分类问题用投票法，回归问题用加权法） e.g. 随机森林（Random Forest） stacking 使用不同的学习方法学习同一份数据，得到多个学习器， 而后使用另一个学习器，学习以上多个学习器的输出到真实标签的映射 boosting 按序处理多个弱学习器，排在后的学习器重点学习排在前的学习器无法处理好的那些数据 e.g. Ada Boost 一个学习器学完后，根据其对数据集分类的正确与否，调整下一个学习器学习时，数据集各条数据被采样到的概率，达成调整数据集分布的作用。而后多个学习器按照各自的正确率集成在一 随机森林 是决策树一个非常优秀的扩展，同时也剥夺了商业规则和易解释性。 树很多，使用多数投票规则使得模型变得更加复杂， 决策树变量之间也存在相互作用。 变分贝叶斯方法参考：Variational Bayesian methods变分贝叶斯方法是一系列用于逼近贝叶斯推理和机器学习中出现的棘手积分的技术。它们通常用于由观察变量（通常称为“数据”）以及未知参数和潜在变量组成的复杂统计模型，这三种类型的随机变量之间具有各种关系，正如图形模型所描述的那样。正如贝叶斯推理中的典型情况一样，参数和潜在变量被分组为“未观察到的变量”。变分贝叶斯方法主要用于两个目的： 为未观测变量的后验概率提供分析近似，以便对这些变量进行统计推断。 导出观察数据的边际可能性（有时称为证据）的下限（即给定模型的数据的边际概率，对未观察的变量进行边缘化）。这通常用于执行模型选择，一般思想是给定模型的边际，可能性较高表明该模型对数据的拟合更好，因此所讨论的模型是生成数据的模型的概率更大。 （另请参阅贝叶斯因子文章。）。。。。。 拉普拉斯平滑（Laplace smoothing），也称为加一平滑（Add-One smoothing），是一种用于处理概率估计中的零概率问题的技术。它是一种简单而常用的平滑方法，可用于解决在计算概率时可能出现的数据稀疏性和零概率的情况。 在概率估计中，当我们根据样本数据计算事件的概率时，有时会遇到某些事件在样本中未出现的情况，导致概率估计为零。这在实际应用中可能不太合理，因为我们不能简单地认为未观察到的事件的概率为零。 拉普拉斯平滑通过在计算概率时为每个事件的计数值（或频率）增加一个常数（通常为1），来解决零概率问题。这个常数被称为平滑因子或平滑参数。通过这种方法，即使某个事件在样本中未出现，它的概率仍然会被估计为一个非零值。 拉普拉斯平滑的概率估计公式如下：P(x) &#x3D; (count(x) + 1) &#x2F; (N + V) 其中，P(x)表示事件x的平滑概率，count(x)表示在样本中观察到事件x的次数，N表示总观测次数，V表示事件的可能取值数量（即事件的种类数）。 应用方面，拉普拉斯平滑广泛用于自然语言处理（NLP）中的语言模型，特别是n-gram语言模型。在n-gram模型中，用于估计概率的数据通常是文本中的n个连续词语序列。拉普拉斯平滑可以解决在计算概率时可能出现的未观察到的n-gram序列的问题，提高语言模型的鲁棒性和泛化能力。 除了语言模型，拉普拉斯平滑还可以应用于其他概率估计问题，如朴素贝叶斯分类器、信息检索中的查询扩展和推荐系统等。它可以有效地处理数据稀疏性问题，并提供更合理的概率估计结果。 强化学习当谈到强化学习（Reinforcement Learning，RL）时，我们在机器学习中讨论的是一种范例和方法。它主要用于描述和解决智能代理与环境交互的学习问题，目标是通过学习一种策略或行为序列来最大化累积的回报或实现特定目标。 在强化学习中，我们有一个智能代理（agent），它根据环境的状态（state）选择动作（action），并与环境进行交互。环境会根据代理采取的动作以及当前的状态，返回给代理一个奖励信号（reward）和下一个状态。代理的目标是通过与环境的交互，通过尝试和错误的方式来学习一个最佳策略，以使得长期累积的回报最大化。 在强化学习中，代理通过学习价值函数（value function）或策略函数（policy function）来指导其决策过程。价值函数可以评估给定状态或状态动作对的价值，而策略函数定义了在给定状态下选择动作的方式。代理通过与环境的交互不断更新这些函数，以改进其决策能力。 强化学习的一个重要概念是探索（exploration）与利用（exploitation）的权衡。探索是指代理通过尝试新的动作来发现更多的知识，而利用是指代理根据已知信息选择最优动作以获得最大回报。强化学习算法需要在探索和利用之间找到平衡，以达到最佳的学习效果。 总结起来，强化学习是一种机器学习方法，用于解决智能代理与环境交互的学习问题。代理通过学习策略或行为序列来最大化累积回报或实现特定目标。在这个过程中，代理通过与环境的交互不断更新价值函数和策略函数，以改进其决策能力。探索与利用的权衡是强化学习中需要解决的重要问题之一。 参考资料：   CART回归： "},{"date":"2024-04-22T13:14:49.007Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","categories":[["undefined",""]],"content":"自注意力机制 TODO 下面是关于NLP中常用的一些知识的简要说明： Attention（注意力机制）： Attention是一种用于提升神经网络模型性能的机制，特别是在序列任务中。它允许模型在处理输入序列时将重点放在相关的部分上。通过计算每个输入位置的权重，模型可以自适应地决定要关注哪些部分。注意力机制在机器翻译、文本摘要、问答系统等任务中得到广泛应用。 Transformer（变换器）： Transformer是一种基于注意力机制的神经网络架构，用于处理序列数据。它在自然语言处理任务中取得了重大突破，并成为许多最先进的模型的基础。Transformer通过自注意力机制（self-attention）来捕捉输入序列中的依赖关系，避免了传统循环神经网络中的顺序计算，并且能够并行处理输入。Transformer的典型应用包括机器翻译（如Google的Transformer模型）和语言模型。 BERT（Bidirectional Encoder Representations from Transformers）： BERT是一种预训练的语言表示模型，基于Transformer架构。通过在大规模文本数据上进行无监督的预训练，BERT可以学习出通用的语言表示，然后可以在各种下游任务上进行微调。BERT引入了掩码语言模型（Masked Language Model, MLM）和下一句预测（Next Sentence Prediction, NSP）等任务来训练模型。BERT的出现对各种NLP任务，如文本分类、命名实体识别、问答系统等都产生了显著影响。 GPT（Generative Pre-trained Transformer）： GPT是一种基于Transformer架构的预训练语言模型，用于生成文本。GPT通过在大规模文本数据上进行自监督的预训练，学习出对输入序列的概率分布建模能力。然后，可以使用该模型生成具有连贯性和语法正确性的文本。GPT模型在生成式任务中表现出色，如文本生成、对话系统、机器写作等。 Prompt（提示）： Prompt是指在进行自然语言处理任务时，向模型提供一种任务描述或问题陈述的方式。通过给定一个显式的提示文本，模型可以更好地理解任务需求和上下文，从而产生更准确的输出。Prompt工程化是近年来在NLP任务中的一种重要技术，它可以帮助改进模型的可控性、减少模型的偏见，并提高模型的性能。 下面是这些概念之间的关系： Attention（注意力机制）是Transformer（变换器）模型的核心组件之一。Transformer通过自注意力机制实现了对输入序列的建模，其中每个位置可以根据其与其他位置的相关性来调整其重要性。 Transformer是一种神经网络架构，被广泛用于自然语言处理任务。它的设计中包含了多头注意力机制，可以同时关注不同位置的不同方面。Transformer的出现使得处理长序列数据变得更加高效，并在机器翻译、文本生成等任务中取得了显著的性能提升。 BERT（Bidirectional Encoder Representations from Transformers）是基于Transformer的预训练语言模型。它通过大规模的无监督预训练，在理解上下文和建模语言表示方面取得了巨大成功。BERT的预训练模型可以通过微调适应各种下游任务，如文本分类、命名实体识别等。 GPT（Generative Pre-trained Transformer）也是基于Transformer的预训练语言模型，但其目标是生成连贯的文本。GPT通过自监督学习来提前训练一个语言模型，然后可以用于生成各种文本，如文章、对话等。GPT模型在生成式任务中表现出色，可以产生具有语法正确性和连贯性的文本。 Prompt（提示）是在进行NLP任务时向模型提供任务描述或问题陈述的方式。Prompt的引入主要是为了改进模型的可控性和减少模型的偏见。通过设计合适的提示文本，可以引导模型在特定任务上产生更准确、更符合预期的输出。 总体而言，Attention是一种机制，Transformer是一种基于Attention的网络架构，BERT和GPT是基于Transformer的预训练语言模型，而Prompt是一种用于指导模型输出的技术手段。它们在NLP领域中相互关联，相互借鉴，共同推动了自然语言处理技术的发展。 深度卷积神经网络至少在网络的一层中使用卷积运算来代替一般的矩阵乘法运算的神经网络，因此命名为卷积神经网络 卷积核卷积核（kernel） ，一个权重矩阵，逐步在二维输入数据上“扫描”，卷积核“滑动”的同时，计算权重矩阵和扫描所得的数据矩阵的乘积，然后把结果汇总成一个输出像素。 大小一般为奇数：原因 更容易 padding，计算结果为整数 使用奇数大小的卷积核还可以保持对称性。对称性在卷积操作中是重要的，因为它可以确保输出特征图的空间分辨率与输入特征图相同。如果使用偶数大小的卷积核，由于缺少中心像素，可能会导致输出特征图的空间分辨率减小。 在CNN中，进行卷积操作时一般会以卷积核模块的一个位置为基准进行滑动，这个基准通常就是卷积核模块的中心。 卷积核大小为奇数时，它具有一个中心元素，这使得在进行卷积操作时，可以确保输入图像的每个像素都有对应的卷积核元素与之对齐。这种中心对齐的特性有助于提取局部特征，同时减少了信息丢失的可能性。 卷积（Convolution）所谓的卷积运算，其实它被称为互相关（cross-correlation）运算：将图像矩阵中，从左到右，由上到下，取与滤波器同等大小的一部分，每一部分中的值与滤波器中的值对应相乘后求和，最后的结果组成一个矩阵，其中没有对核进行翻转。 数学上，给定两个函数 f(x) 和 g(x) 的卷积运算表示为： (f * g)(x) &#x3D; ∫[−∞,∞] f(t)g(x−t) dt 对于每一个 x 得到的值 是 g 在 x 偏移 t 处 的值乘以 f(t) (权重) 的累加值， 核心操作，用于提取特征 卷积运算具有一些重要的性质，例如交换律、结合律和分配律。 深度学习里面所谓的卷积运算，其实它被称为互相关（cross-correlation）运算：将图像矩阵中，从左到右，由上到下，取与滤波器同等大小的一部分（f(x-t)），每一部分中的值与滤波器中的值（g(t)）对应相乘后求和，最后的结果组成一个矩阵，其中没有对核进行翻转。 填充（Padding）避免信息损失 输入图像与卷积核进行卷积后的结果中损失了部分值，输入图像的边缘被“修剪”掉了（边缘处只检测了部分像素点，丢失了图片边界处的众多信息）。这是因为边缘上的像素永远不会位于卷积核中心，而卷积核也没法扩展到边缘区域以外。 这个结果我们是不能接受的，有时我们还希望输入和输出的大小应该保持一致。为解决这个问题，可以在进行卷积操作前，对原矩阵进行边界填充（Padding），也就是在矩阵的边界上填充一些值，以增加矩阵的大小，通常都用“”来进行填充的。 通过填充的方法，当卷积核扫描输入数据时，它能延伸到边缘以外的伪像素，从而使输出和输入size相同。 常用的两种padding： （1）valid padding：不进行任何处理，只使用原始图像，不允许卷积核超出原始图像边界 （2）same padding：进行填充，允许卷积核超出原始图像边界，并使得卷积后结果的大小与原来的一致 作者：2kb的卷心菜  出处：bilibili 步长(Stride)压缩一部分信息，或者使输出的尺寸小于输入的尺寸 每天五分钟计算机视觉：卷积步长(Stride) (baidu.com) 滑动卷积核时，我们会先从输入的左上角开始，每次往左滑动一列或者往下滑动一行逐一计算输出，我们将每次滑动的行数和列数称为Stride，在之前的图片中，Stride&#x3D;1；在下图中，Stride&#x3D;2。  出处：bilibili-2kb的卷心菜 通道比如 RGB 有 红绿蓝三个 通道 这里就要涉及到“卷积核”和“filter”这两个术语的区别。在只有一个通道的情况下，“卷积核”就相当于“filter”，这两个概念是可以互换的。但在一般情况下，它们是两个完全不同的概念。每个“filter”实际上恰好是“卷积核”的一个集合，在当前层，每个通道都对应一个卷积核，且这个卷积核是独一无二的。 多通道卷积的计算过程：将矩阵与滤波器对应的每一个通道进行卷积运算，最后相加，形成一个单通道输出，加上偏置项后，我们得到了一个最终的单通道输出。如果存在多个filter，这时我们可以把这些最终的单通道输出组合成一个总输出。 某一层输出特征图的通道数&#x3D;当前层滤波器的个数。如上图所示，当只有一个filter时，输出特征图（4×4）的通道数为1；当有2个filter时，输出特征图（4×4×2）的通道数为2。 降采样 概念：降采样指的是成比例缩小特征图宽和高的过程 例子：比如从（W，H）变为（W&#x2F;2，H&#x2F;2） 方法： stride 大于 1 的 pooling stride 大于 1 的 conv stride 大于 1 的 reorg（在YOLOv2的论文里叫passthrough layer） 比较： 1 和 2 在深度卷积神经网络中使用非常普遍，3 比较小众，由Joseph Redmon在YOLOv2中首次提出。 1 和 2 的对比在Striving for Simplicity: The All Convolutional Net中有详述 用stride&#x3D;2的conv降采样的卷积神经网络效果与使用pooling降采样的卷积神经网络效果相当；卷积神经网络小的时候，使用pooling降采样效果可能更好，卷积神经网络大的时候，使用stride&#x3D;2的conv降采样效果可能更好。 pooling提供了一种非线性，这种非线性需要较深的conv叠加才能实现，因此当网络比较浅的时候，pooling有一定优势；但是当网络很深的时候，多层叠加的conv可以学到pooling所能提供的非线性，甚至能根据训练集学到比pooling更好的非线性，因此当网络比较深的时候，不使用pooling没多大关系，甚至更好。 pooling的非线性是固定的，不可学习的，这种非线性其实就是一种先验。 3 中降采样的优势在于能够较好的保留低层次的信息。1 和 2 的降采样方式，好处是抽取的特征具有更强的语义性，坏处是会丢失一些细节信息。而3这种降采样方式与1、2相反，3 提取的特征语义性不强，但是能保留大量细节信息。所以当我们既需要降采样，又需要不丢失细节信息的时候，3是一个非常合适的选择。 升采样将输入特征图的尺寸放大或增加分辨率。 通常与卷积和池化等操作结合使用，用于逆向传播梯度、特征图的恢复或生成更高分辨率的输出。 基本目标是增加特征图的空间尺寸，以便更好地捕获细节信息、提高特征的表达能力或生成更高分辨率的输出。 方法： 反卷积（Transpose Convolution）：反卷积是一种常见的升采样方式，也称为转置卷积、分数步长卷积。它通过在输入特征图之间插入零值，并使用带有适当步长的卷积核进行卷积操作来放大特征图的尺寸。反卷积操作可以增加特征图的空间尺寸，并在某种程度上恢复输入特征图的细节。 双线性插值（Bilinear Interpolation）：双线性插值是一种基于插值的升采样方法，通过对输入特征图中的每个像素进行插值计算来生成更大尺寸的特征图。它使用周围四个像素的权重进行插值，保持了图像的平滑性和连续性。 最近邻插值（Nearest Neighbor Interpolation）：最近邻插值是一种简单的升采样方法，它将输入特征图中每个像素的值复制到放大后的特征图的相应位置。它在放大时不进行插值计算，而是直接使用最近邻像素的值。这种方法简单高效，但可能会导致输出特征图的锯齿状边缘。 其他一些升采样技术，如子像素卷积（Subpixel Convolution）、转置卷积的变种（如反池化操作）、像素重排（Pixel Shuffle）等。这些方法在不同的应用场景中具有各自的优势和适用性。 池化层是一种常用的操作层，用于减小特征图的空间尺寸、降低计算量，并增强模型的平移不变性。输入特征图的局部区域进行聚合或采样来生成池化特征图。 池化操作：通常在每个输入特征图的局部区域上应用，通过对区域内的特征进行聚合或采样，生成一个单一的值或特征。这个聚合或采样过程可以是简单的求最大值（最大池化，Max Pooling）或求平均值（平均池化，Average Pooling），也可以是其他聚合方式，如Lp范数池化。 在池化操作中，可以通过调整池化窗口的大小和步幅来控制输出特征图的大小和感受野。池化窗口是应用池化操作的局部区域的大小，步幅是池化窗口在输入特征图上移动的距离。 常见的池化操作及其特点： 最大池化（Max Pooling）：在池化窗口内选择最大值作为池化特征。最大池化有助于保留显著的特征，提高模型的平移不变性和鲁棒性。 平均池化（Average Pooling）：在池化窗口内求特征的平均值作为池化特征。平均池化可以减少特征图的空间维度，并平滑特征。 Lp范数池化（Lp-norm Pooling）：在池化窗口内对特征进行Lp范数归一化，得到池化特征。Lp范数池化可以对特征进行归一化，并引入更多非线性。 池化操作在CNN中具有以下优势： 减小特征图的空间尺寸，降低计算量和内存需求。 提取特征的局部不变性，使模型对目标在图像中的位置变化具有鲁棒性。 减少模型过拟合的风险，通过减少参数数量和引入局部平均化。 池化层通常与卷积层交替使用，以构建深层次的卷积神经网络结构。它在图像分类、目标检测、图像分割等任务中广泛应用，并在提高模型性能和减少计算资源消耗方面发挥着重要作用。 batch size“batch size”（批大小）是指在一次训练迭代中同时输入模型的样本数量。批大小决定了在每一次参数更新时，模型所看到的样本数量。 理解批大小可以参考以下几点： 样本数量：批大小表示一次训练中同时处理的样本数量。例如，如果批大小为32，则在每次参数更新时，模型将同时处理32个样本。 内存和计算效率：较大的批大小可以提高计算效率，因为同时处理多个样本可以充分利用并行计算的能力。然而，较大的批大小可能需要更多的内存存储模型的中间结果。 梯度估计：在训练过程中，批大小还会影响对梯度的估计。较大的批大小可以提供更准确的梯度估计，因为它们包含了更多的样本信息。然而，较小的批大小可能导致模型收敛更快，因为它们更频繁地更新参数。 泛化能力：较大的批大小可能会导致模型过度拟合训练数据，因为它们更倾向于记住样本特定的细节。较小的批大小可以提供更好的泛化能力，因为它们更强迫模型学习更一般化的特征。 epoch（时期） 训练过程通常涉及多个epoch，因为一次完整的训练可能不足以使模型达到最佳性能。通过进行多个epoch，模型可以多次观察和学习数据的不同方面，并逐渐改善其性能。每个epoch之间的样本顺序通常会被随机化，以避免模型对样本顺序的依赖。 向前步骤向前步骤（Forward Propagation）： 在向前步骤中，输入样本通过模型的前向计算过程，从输入层经过一系列的神经网络层传递，最终得到模型的输出预测结果。 在每一层中，通过对输入数据进行线性变换（加权和）和非线性变换（激活函数），将信息从前一层传递到后一层，直到到达输出层。 向前步骤的目的是计算出模型的预测结果，以便与真实标签进行比较，并计算出损失函数的值。 反向传播步骤反向传播步骤（Backpropagation）： 在反向传播步骤中，根据向前步骤中计算得到的损失函数值，通过链式法则计算每个参数对损失的贡献度，并更新模型的参数。 反向传播通过对损失函数关于模型参数的偏导数进行计算，从输出层向输入层逐层传递，以确定每个参数的梯度。 梯度表示了损失函数对每个参数的变化率，利用梯度可以确定参数的更新方向和大小，以最小化损失函数 梯度的惩罚程度梯度的惩罚程度是指在计算梯度时对其进行的限制或惩罚的程度。在深度学习中，常用的梯度惩罚方法是通过添加正则化项或其他惩罚项来约束模型的参数更新。 目的防止参数更新过大或过快，以避免模型的过拟合或不稳定情况。通过对梯度进行惩罚，可以限制参数更新的幅度，使其保持在合理的范围内。 梯度惩罚的程度可以通过调整惩罚项的权重或超参数来控制。增加梯度惩罚的程度意味着更强烈地限制梯度的大小或方向，从而减缓参数更新的速度。相反，减少梯度惩罚的程度会使梯度对参数更新的限制更加宽松。 惩罚程度的选择通常需要根据具体问题和数据集的特点进行调整。过强的梯度惩罚可能导致模型无法学习到有效的特征，而过弱的梯度惩罚可能导致模型过拟合或训练不稳定。因此，需要根据实际情况进行实验和调优，选择合适的梯度惩罚程度以获得最佳的模型性能。 自动编码器自动编码器（Autoencoder，AE）是一种无监督学习模型，用于学习数据的特征表示和压缩。它由编码器（Encoder）和解码器（Decoder）组成。 欠完备自动编码器（Undercomplete Autoencoder）： 欠完备自动编码器是指编码器的维度低于输入数据的维度。这种设置迫使模型学习数据的主要特征，因为编码器无法完全捕获原始数据的所有信息。通过限制编码器的容量，欠完备自动编码器可以捕捉数据中最显著的特征，从而实现特征选择和降维。 正则化自动编码器（Regularized Autoencoder）： 正则化自动编码器通过在损失函数中引入额外的正则化项来约束模型的学习过程，以防止过拟合。常见的正则化方法包括L1正则化和L2正则化。L1正则化通过增加编码器的稀疏性，鼓励模型只使用输入数据的少数关键特征。L2正则化通过限制权重的大小，使模型对输入数据的小变化具有鲁棒性。 变分自动编码器（Variational Autoencoder，VAE）： 变分自动编码器是一种生成性模型，与判别性模型（欠完备自动编码器和正则化自动编码器）不同，它可以生成新的数据样本。VAE通过在潜在空间中引入随机性，使得模型能够在潜在空间中进行随机采样，并通过解码器生成新的样本。在训练过程中，VAE通过最大化“证据下界”（evidence lower bound，ELBO）来优化模型参数，从而实现对潜在空间的建模。这使得VAE能够学习到数据的潜在分布，并通过从该分布中采样生成新的数据样本。 CIFAR-10 和 CIFAR-100CIFAR数据集是 Visual Dictionary (Teaching computers to recognize objects) 的子集，由三个教授收集，主要来自google和各类搜索引擎的图片。 cifar10和cifar100(简介&amp;可视化)_cifar10和cifar100区别-CSDN博客 MNIST数据集 MNIST数据集是NIST（National Institute of Standards and Technology，美国国家标准与技术研究所）数据集的一个子集，MNIST 数据集可在  获取，主要包括四个文件："},{"date":"2024-04-22T13:14:49.007Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B/Transformer/","categories":[["undefined",""]],"content":"  扩展 Attention 来加速训练，并且在特定任务上Transformer 表现比 Google NMT 模型还要好，最大好处是可并行 提出Attention is All You Need其中的 TF 应用是 Tensor2Tensor 的子模块。 粗略概述 编码组件 六层编码器首尾相连：完全结构相同，但是不共享参数。 对于每一个编码器 self-attention 层：帮助模型在编码某一个此时能看到别的单词 前向网络：每个self-attention的输出流向一个前向网络，每个输入位置对应的前向网络是独立互不干扰的。 连接层 解码组件 六层解码器首尾相连。每个输入位置对应的前向网络是独立互不干扰的。 对于每一个解码器： self-attention 层： attention 层：该层有助于解码器能够关注到输入句子的相关部分，以便更好的生成与输入有关的输出。（生成每个输出时，根据输入序列的不同部分动态地分配注意力权重。） 前向网络： 待继续，有点累了。 Transformer有两个版本：Transformer base和Transformer Big。两者结构其实是一样的，主要区别是包含的Transformer Block数量不同，Transformer base包含12个Block叠加，而Transformer Big则扩张一倍，包含24个Block。无疑Transformer Big在网络深度，参数量以及计算量相对Transformer base翻倍，所以是相对重的一个模型，但是效果也最好。"},{"date":"2024-04-22T13:14:49.007Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B/GAN/","categories":[["undefined",""]],"content":"GAN关键 生成对抗网络（GANs）将当前用于判别机器学习的深度神经网络的进步转化为（隐式）生成建模。 GAN 训练一个生成式深度学习网络 G，将多维随机样本 z （来自高斯分布或者均匀分布）作为输入，从所需分布中生成一个样本。 GANs 将密度估计问题转换为 二元分类问题，其中对 G 参数的学习是通过能耐区分真假数据的判别深度神经网络 D 来实现的。 更正式地说，GAN 解决的优化问题可以概括如下： ![[Pasted image 20240415162616.png]] HashCat Per position Markov Chains. 分类 基于积分概率指标：IPM-based GAN 为 GAN 训练 提供稳定性，学习过程中相对稳定， 基于非积分概率指标：non-IPM-based GAN 缺点在学习阶段不稳定，模型优化困难。 WGAN概述Arjovsky 等人 2017 提出的 WGAN 通过采用 Wasserstein 距离作为损失来提高标准 GAN 的训练稳定性。 这种方法的好处包括减少模式崩溃和有意义的学习曲线，这有助于确定最佳超参数。 WGAN 纳入了新的成本函数； 然而，WGAN 的实验重点是生成逼真的图像。 古尔拉贾尼等人。 优化与传统的GAN相比，WGAN通过引入Wasserstein距离取代了传统GAN中使用的JS散度或KL散度，从而在训练过程中提供了更稳定的梯度信号。 策略为了实现Wasserstein距离的近似，WGAN引入了一个判别器的参数范数约束，即Lipschitz限制。这通过对判别器的权重进行剪裁或权重正则化来实现。然而，这种限制方法可能难以实施并且效果不稳定。 IWGAN可以更有效地找到全局最优值。 他们引入了梯度惩罚的概念来代替 WGAN 的梯度裁剪。 古尔拉贾尼等人。 提出使用IWGAN来解决文本生成问题。 在 Gulrajani 等人的 IWGAN 中，G 和 D 都由简单的残差 CNN 组成。 残差架构使得 GAN 的训练快速且稳定[30,31]。 G 将潜在噪声向量作为输入，通过将其转发到其卷积层来对其进行转换，并输出 32 个 one-hot 字符向量的序列。 G的输出层采用softmax非线性函数，并将其转发给D。假样本的每个输出特征由argmax函数的结果决定，argmax函数将G生成的每个输出向量作为输入。 Relativistic Average GAN相对论平均生成对抗网络（Relativistic Average GAN）是生成对抗网络（GAN）的一种变体，旨在改善生成对抗网络的训练和生成效果。 背景传统的GAN中，生成器试图生成逼真的样本，而判别器则根据样本的真实性进行分类。然而，这种方式可能导致生成器和判别器陷入不稳定的训练过程，因为生成器的更新依赖于判别器的反馈，而判别器的反馈又依赖于生成器生成的样本。 特点相对论平均GAN通过引入相对论平均策略来解决这个问题。在这种策略下，生成器和判别器之间的对抗性比较被重新定义，以便更准确地反映出生成样本的真实性。具体而言，相对论平均GAN引入了两个新的损失函数：相对论平均生成器损失和相对论平均判别器损失。 相对论平均生成器损失（Relativistic Average Generator Loss）是通过对真实样本和生成样本进行比较来度量生成器的性能。它通过计算生成样本在判别器给出真实样本的概率上的平均值来评估生成器的生成能力。 相对论平均判别器损失（Relativistic Average Discriminator Loss）用于度量判别器的性能。它通过比较真实样本和生成样本之间的相对概率来评估判别器的辨别能力。 训练过程 从真实样本中随机采样。 使用生成器生成一批样本。 计算相对论平均生成器损失和相对论平均判别器损失。 更新生成器的参数以减小相对论平均生成器损失。 更新判别器的参数以减小相对论平均判别器损失。 重复步骤1-5，直到达到预定的训练轮数或生成样本达到所需的质量。 优点改善了生成样本的质量和训练的稳定性。通过引入相对论平均策略，生成器和判别器能够更准确地估计样本的真实性，并促使它们相互逼近，提高生成样本的质量。这种模型在图像生成、文本生成和其他生成任务中都有应用。 WGAN-GPWGAN-GP通过引入梯度惩罚解决了WGAN中的限制问题。梯度惩罚是通过在判别器的输出和真实样本之间的采样点上计算梯度范数的平均值，并将其与预定义的惩罚因子相乘来实现的。这个惩罚项鼓励判别器在整个输入空间上保持平滑的梯度，从而使判别器满足Lipschitz连续性的要求。 具体来说，在WGAN-GP中，生成器和判别器的训练过程如下： 从真实数据和生成器生成的样本中采样。 在采样点上计算判别器的输出。 计算判别器的梯度惩罚，并将其添加到判别器的损失函数中。 更新判别器的参数以最小化损失函数。 对生成器进行更新，最大化判别器对生成样本的输出。 WGAN-GP相对于传统的GAN具有几个优点。首先，它提供了更稳定的训练过程，减少了训练中的模式崩溃和模式衍生问题。其次，通过梯度惩罚，WGAN-GP避免了对判别器权重的剪裁或正则化，使得模型的训练更简单和可靠。最后，WGAN-GP在生成器和判别器之间提供了更准确的梯度信号，从而改善了生成样本的质量和多样性。"},{"date":"2024-04-22T13:14:49.007Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B/%E5%8F%98%E5%88%86%E7%BC%96%E7%A0%81%E5%99%A8/","categories":[["undefined",""]],"content":"Variational Auto-Encoders (VAEs) 概念变分自编码器（Variational Autoencoders，VAEs）是一种生成模型，结合了自编码器和概率推断的思想。它被用于学习数据的潜在表示，并可以生成与原始数据相似的新样本。 构成VAEs的基本结构由两部分组成：编码器（Encoder）和解码器（Decoder）。编码器将输入数据映射到一个潜在空间中的潜在变量（也称为编码），而解码器则将潜在变量映射回重构的输入数据。 与传统的自编码器不同，VAEs引入了概率推断的概念，其中潜在变量被建模为潜在空间中的概率分布。具体来说，VAEs假设潜在变量服从一个先验分布（通常是高斯分布），并通过编码器将输入数据映射到潜在空间的均值和方差参数上。然后，从该潜在分布中采样一个潜在变量，并通过解码器将其映射回重构的输入数据空间。 训练VAEs的过程涉及最大化观测数据的对数似然性，并最小化潜在变量与先验分布之间的差异，即最小化重构误差和潜在变量的KL散度。 优点VAEs的一个关键优点是它们提供了对潜在空间的连续、平滑的控制，这使得可以在潜在空间中进行插值和操作，生成具有多样性和连续变化的新样本。 应用VAEs在生成模型、数据压缩、数据降维和生成样本等任务中得到广泛应用，并成为深度学习领域中重要的模型之一。 总结总结起来，变分自编码器（VAEs）是一种结合了自编码器和概率推断的生成模型。它通过学习潜在变量的概率分布来表示数据，并可以生成与原始数据相似的新样本。VAEs提供了对潜在空间的平滑控制，具有广泛的应用潜力。"},{"date":"2024-04-22T13:14:49.007Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/","categories":[["undefined",""]],"content":"DDPMs DDIMs DiTs 扩散模型，在机器学习领域，也被称为扩散概率模型，或者基于得分的生成模型，是一类潜变量生成模型。 扩散模型由三个主要部分组成：正向过程，反向过程和采样过程扩散模型的目标是学习生成给定数据集概率分布的扩散过程。她们通过对数据点在潜在空间中扩散的方式来进行建模并学习数据集的潜在结构。就计算机视觉而言，扩散模型可以应用于各种任务，包括图像去噪、修复、超分辨率和图像生成。这通常涉及训练神经网络以顺序对高斯噪声模糊的图像进行去噪。 2() 3() 该模型经过训练可以逆转向图像添加噪声的过程。训练收敛后，它可以用于图像生成，从由随机噪声组成的图像开始，让网络迭代去噪。 OpenAI 于 2022 年 4 月 13 日发布的文本到图像模型 DALL-E 2 是一个示例，该示例将扩散模型用于模型的先验（在给定文本标题的情况下生成图像嵌入）和生成最终图像的解码器。 4() 扩散模型最近在自然语言处理 (NLP) 中得到应用， 5() 特别是在文本生成 6() 7() 和摘要等领域。 8() 扩散模型通常被表述为马尔可夫链并使用变分推理进行训练。 9() 计算机视觉中使用的通用扩散建模框架的示例包括去噪扩散概率模型、噪声条件评分网络和随机微分方程。 10() 类型属于无监督生成模型 原理扩散模型由三个主要部分组成：正向过程（扩散过程），反向过程和采样过程，生成过程（逆向过程、推断过程）为了避免混淆，本文以 $x_1,x_2,…,x_T$ 表示不同的时间步中的样本，以 $X_1,X_2,…,X_T$ 表示不同时间步对应的随机变量， 以 $p(X_T)$ 表示随机变量的概率分布，以 $N(x_T,\\mu,\\sum)$ ，表示在分布 $N(\\mu, \\sum)$ 中 $X&#x3D;x_T$ 时的概率样本概率值。 正向过程（扩散过程）通过对任意的初始样本 $x_0$ 连续的添加 $T$ 次高斯噪声，可获得一条样本的轨迹 $x_1,x_2,…,x_T$ ，并且当 $T$ 趋于无穷时，原始样本 $x_0$ 的特征完全消失，成为标准高斯噪声。从概率分布的角度而言，如果定义初始样本（训练样本）的概率分布为 $q(X_0)$ 则通过无限次地扩散动作，时宪历从初始样本分布到高斯分布的映射，即 $q(X_T) &#x3D; N(0,I)$。当然扩散过程连续添加高斯噪声不是任意的，其具体的限定规则为$q(X_t|x_t-1) &#x3D; N(\\sqrt {1-\\beta_t}x_{t-1},\\beta_tI)$ ,其中 $\\beta_1 &lt;\\beta_2 &lt;…&lt; \\beta_T$由上式可知，在给定 $t-1$ 时刻的样本 $x_{t-1}$ 的情况下，t时刻样本的分布为高斯分布。由此式可以看出，该调见高斯分布的均值参数只与 $x_{t-1}$ 有关， 与前面时间的样本无关，因而随机过程 ${X_t}$ 是一个马尔科夫过程。TODO ： 反向过程采样过程"},{"date":"2024-04-22T13:14:49.007Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/","categories":[["undefined",""]],"content":"参考 概念指一类机器学习模型，其目标是学习数据的概率分布，并通过该概率分布生成新的样本数据。生成模型可以根据已知数据的统计特征，学习数据的分布模式，然后使用这个模式生成新的数据样本。生成模型被广泛应用于数据生成、图像合成、文本生成等领域。 分类按照概率密度函数的处理方式分类![[Pasted image 20240412152711.png]] 按照是否有监督 有监督的生成模型。 无监督的生成模型。 [[扩散模型]] 基本任务 生成样本，例如生成逼真的人脸图片、生成高质量的语音等； 也可以通过改进生成模型从而实现样本之间的映射转换，例如图像风格迁移、语音增强等。 原理：生成样本整体流程生成模型接受随机噪声 z 作为输入，然后产生输出样本 x 训练和推理过程分析 由 N 个样本构成的集合，假设它们是彼此独立都采样来源于某个未知的概率分布 q(X)； 问题：如何在不知道 q(X) 的情况下产生一个模型使得，生成的样本在分布中。 解决：构建生成模型 p(X,θ)，通过 N 个样本来学习到最好的参数 θ，使得 p(X,θ)&#x3D;q(X)两个问题 如何设计模型 不同的模型有各自的考虑，比如 玻尔兹曼机 使用基于能量的模型，完全可见置信网络对模型进行的链式解析等等。 如何训练 显式生成模型使用的训练准则为极大似然估计。这里有分两种情况： 对似然函数本身直接进行优化的精确推断方法。（例如流模型，自回归模型） 对似然函数近似值进行优化的近似推断方法。（例如VAE和玻尔兹曼机） 隐式生成模型首先使用两类样本学习到了 p(X,θ) 和 q(X) 的距离，然后再以减少距离为目标训练生成模型。 潜变量生成模型潜变量生成模型（Latent Variable Generative Model）是一种统计模型，用于描述数据的生成过程。它假设存在一组潜在的变量（也称为隐藏变量或潜变量），这些变量无法直接观测到，但对生成数据起到重要作用。 潜变量生成模型的基本思想是，通过学习数据中隐藏的潜在结构和变量，可以生成与观测数据相似的新样本。这种生成过程通常基于概率分布模型，如高斯混合模型（Gaussian Mixture Model，GMM）、隐马尔可夫模型（Hidden Markov Model，HMM）、变分自编码器（Variational Autoencoder，VAE）等。 在潜变量生成模型中，潜变量表示了数据中的潜在特征或隐含结构，它们对生成数据的分布产生影响。通过对潜变量和观测变量之间的关系进行建模，生成模型可以通过给定潜变量的取值来生成对应的观测数据。 潜变量生成模型的应用广泛，包括图像生成、文本生成、语音生成等。通过学习和探索数据中的潜在结构，潜变量生成模型可以生成具有多样性和创造性的新样本，进而用于数据增强、生成对抗网络（GAN）的训练、数据压缩和降维等任务。 总而言之，潜变量生成模型是一种通过建模潜在变量与观测数据之间的关系来生成数据的统计模型。它利用隐藏的潜在结构和变量来模拟数据的生成过程，并可用于生成新样本和数据分析中的其他任务。"},{"date":"2024-04-22T13:14:48.991Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E5%BE%AE%E4%BF%A1%E9%A1%B9%E7%9B%AE/","categories":[["undefined",""]],"content":"地址 ChatGPT微信开发，轻松拿捏_keyerror: ‘wxsid-CSDN博客  运行nohup python3 app.py &amp; tail -f nohup.outnohup .&#x2F;clash-linux-amd64 &gt; &#x2F;dev&#x2F;null 2&gt;&amp;1 &amp;nohup docker compose up &gt; &#x2F;dev&#x2F;null 2&gt;&amp;1 &amp; 问题[WARNING][2024-02-27 04:41:40][chat_gpt_bot.py:150] - [CHATGPT] APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host&#x3D;’api.openai.com’, port&#x3D;443): Max retries exceeded with url: &#x2F;v1&#x2F;chat&#x2F;completions (Caused by ProxyError(‘Cannot connect to proxy.’, NewConnectionError(‘&lt;urllib3.connection.HTTPSConnection object at 0x7f75d9c45e50&gt;: Failed to establish a new connection: [Errno 111] Connection refused’)))[WARNING][2024-02-27 04:41:40][chat_gpt_bot.py:150] - [CHATGPT] APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host&#x3D;’api.openai.com’, port&#x3D;443): Max retries exceeded with url: &#x2F;v1&#x2F;chat&#x2F;completions (Caused by ProxyError(‘Cannot connect to proxy.’, NewConnectionError(‘&lt;urllib3.connection.HTTPSConnection object at 0x7f75d9c45fa0&gt;: Failed to establish a new connection: [Errno 111] Connection refused’)))[INFO][2024-02-27 04:41:40][wechat_channel.py:218] - [WX] sendMsg&#x3D;Reply(type&#x3D;ERROR, content&#x3D;[ERROR]我连接不到你的网络), receiver&#x3D;@dade582ddc06e63c6ec1175c680f90ff82f77df7c20d9450bfb147cb5a925968 解决【完美解决：openai.error.APIConnectionError: Error communicating with OpenAI:】_openai.openaierror: the api_key client option must-CSDN博客 pip3 show openai查看安装目录cd /opt/conda/envs/fyn_python3_8/lib/python3.8/site-packages/openai 问题1[INFO][2024-02-27 05:34:07][chat_gpt_bot.py:49] - [CHATGPT] query&#x3D;哈哈哈[ERROR][2024-02-27 05:34:07][chat_gpt_bot.py:155] - [CHATGPT] Exception: local variable ‘proxy’ referenced before assignmentTraceback (most recent call last): File “&#x2F;root&#x2F;test&#x2F;GptWchat&#x2F;chatgpt-on-wechat&#x2F;bot&#x2F;chatgpt&#x2F;chat_gpt_bot.py”, line 123, in reply_text response &#x3D; openai.ChatCompletion.create(api_key&#x3D;api_key, messages&#x3D;session.messages, **args) File “&#x2F;opt&#x2F;conda&#x2F;envs&#x2F;fyn_python3_8&#x2F;lib&#x2F;python3.8&#x2F;site-packages&#x2F;openai&#x2F;api_resources&#x2F;chat_completion.py”, line 25, in create return super().create(*args, **kwargs) File “&#x2F;opt&#x2F;conda&#x2F;envs&#x2F;fyn_python3_8&#x2F;lib&#x2F;python3.8&#x2F;site-packages&#x2F;openai&#x2F;api_resources&#x2F;abstract&#x2F;engine_api_resource.py”, line 153, in create response, _, api_key &#x3D; requestor.request( File “&#x2F;opt&#x2F;conda&#x2F;envs&#x2F;fyn_python3_8&#x2F;lib&#x2F;python3.8&#x2F;site-packages&#x2F;openai&#x2F;api_requestor.py”, line 288, in request result &#x3D; self.request_raw( File “&#x2F;opt&#x2F;conda&#x2F;envs&#x2F;fyn_python3_8&#x2F;lib&#x2F;python3.8&#x2F;site-packages&#x2F;openai&#x2F;api_requestor.py”, line 609, in request_raw proxies&#x3D;proxy,UnboundLocalError: local variable ‘proxy’ referenced before assignment[INFO][2024-02-27 05:34:08][wechat_channel.py:218] - [WX] sendMsg&#x3D;Reply(type&#x3D;ERROR, content&#x3D;[ERROR]我现在有点累了，等会再来吧), receiver&#x3D;@5dc3bf914f7a577d4798603da08adc85720678732d3a0413329ceebb3041f0b7 [bot] 这段代码是在Python中定义一个特殊的 __setitem__ 方法，它是一个魔法方法或者叫做运算符重载。这个方法在我们试图给一个对象的实例分配值时被调用。这里看起来像是在实现一个优先级队列或者是一个以某种方式排序的字典。 下面是代码行的详细解释： def __setitem__(self, key, value):: 定义一个名为 __setitem__ 的方法，它接收三个参数：self（表示实例自身，类似于 Java 或 C++ 中的 this），key（字典中要设置的键），value（该键所对应的值）。 if key in self:: 如果提供的键已经在字典中存在。 super().__setitem__(key, value): 使用超类（可能是 dict 或其它字典类型）的 __setitem__ 方法来设置键值对。 for i, (priority, k) in enumerate(self.heap):: 遍历 heap 列表中的每一项，i 是索引，priority 和 k 分别是元组中的两个项目，其中 priority 是优先级（或排序函数的结果），k 是键。 if k == key:: 如果当前键等于要设置的键。 self.heap[i] = (self.sort_func(key, value), key): 使用 sort_func 函数计算新的优先级，并更新 heap 列表中对应的项。 heapq.heapify(self.heap): 重新调整 heap 列表以保持堆的特性（堆是完全二叉树，且任一节点的值都大于或等于其子节点的值）。 break: 找到了正确的键，所以退出循环。 self.sorted_keys = None: 重置 sorted_keys。 else:: 如果字典中不存在这个键。 super().__setitem__(key, value): 使用超类的 __setitem__ 方法来设置新的键值对。 heapq.heappush(self.heap, (self.sort_func(key, value), key)): 在 heap 列表中添加新的元素，元素是一个元组，包含 sort_func 函数计算出的优先级和键。 self.sorted_keys = None: 重置 sorted_keys。 这段代码的主要目的是实现一个可以自我排序的字典或者类似结构，键值对的排序取决于 sort_func 函数的返回结果。 2import ntworkimport pilk"},{"date":"2024-04-22T13:14:48.991Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%85%A5%E9%97%A8/","categories":[["undefined",""]],"content":"大模型基础 Generative AI 的工作原理 LLM 可以做什么, 不能做什么 LLM 技术精要 经典开源 LLM GAI[[GAI 的应用]][[GAI 的工作原理]] 一些资料 chatgptwchat"},{"date":"2024-04-22T13:14:48.983Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/MoEs/","categories":[["undefined",""]],"content":"Mixture of Experts深入理解混合专家模型 相较于密集型模型，预训练速度更快 拥有比同等参数更快的推理速度 对显存要求高，因为需要将所有的专家模型都加载到内存中。 虽然在微调方面存在挑战，有光明前景 概念引入在有限的计算资源下，相较于用更多步骤训练一个小型模型，训练一个大型模型即便步骤更少效果通常更好。MoEs 让模型以远低于传统密集模型的计算成本进行预训练，这意味着你可以在相同的计算预算下显著扩大模型或数据集的规模。 原理构成 稀疏 MoE 层：代替了传统的密集型前馈网络（FFN）层。包含若干“专家”，每个专家都是一个独立的神经网络。实际上，这些专家通常是FFN，但它们也可以是更复杂的网络，，甚至可以是 MoE 本身，形成一个层级结构的 MoE。 一个门控网络或路由器：用于决定那些 Token 分配给哪个专家。例如，在下图中，“More”这个 Token 被分配给第二个专家![[Pasted image 20240310201128.png]] 一个 token 可以分配给多个专家，如何高效的将 Token 分配给合适的专家，是使用 MoE 技术时需要考虑的关键问题之一。 这个路由器由一系列可学习的参数构成。它与模型的其他部分一起进行训练。总结：在 Transformer 中，我们将每一个 FFN（前馈网络）层替换为 MoE 层，由一个门控层和若干”专家“组成。 挑战 训练：在预训练阶段的计算效率极高，但在微调时往往难以适应新场景，容易造成过拟合现象。 推理：尽管 MoE 模型可能包含大量参数，但是在推理过程中只有部分参数被使用，（所以它的推理速度远快于参数相同的模型）但是所有参数都需加载到内存中，因此对内存的需求相当大。 历史 1991 提出 2010~2015 两个不同的领域推动了 MoE 发展 专家作为主键 条件计算 引入稀疏性概念在 NLP 领域 快速发展（本文重点），在计算机视觉等也有探索。 稀疏性稀疏性基于条件计算的概念，不同于密集型模型中所有参数对所有输入都有效，稀疏性让我们能只激活系统的部分区域。 条件计算（即网络的某些部分仅针对特定样本激活）的概念使得在不增加计算量的情况下扩大模型规模成为可能，从而在每层 MoE 中使用了数千名专家。(密集型模型 + 稀疏性 &#x3D;&#x3D;&gt; 稀疏模型) 问题在 MoE 中，当数据通过活跃的专家时，实际的批量大小会减小。例如，如果我们的批量输入包含 10 个 Token，可能有五个 Token 由一个专家处理，另外五个 Token 分别由五个不同的专家处理，这导致批量大小不均匀，资源利用率低下。下文中的 优化 MoE 性能 一节将讨论更多挑战及其解决方案。 解决那我们该如何解决这些问题呢？通过一个学习型的门控网络 (G)，决定将输入的哪些部分分配给哪些专家 (E)： 在这种设置中，所有专家都参与处理所有输入——这是一种加权乘法过程。但如果 G 的值为 0 呢？这种情况下，就无需计算相应专家的操作，从而节约了计算资源。那么，典型的门控函数是什么样的呢？在传统设置中，我们通常使用一个简单的网络配合 softmax 函数。这个网络会学习如何选择最合适的专家处理输入。 为了让门控学习如何路由到不同的专家，需要路由到一个以上的专家，因此至少需要选择两个专家。Switch Transformers 章节将重新审视这一决策。 我们为什么要加入噪声？这是为了实现负载均衡！ 为多专家系统 MoEs 负载均衡 tokens如果所有的 tokens 都被发送到少数几个受欢迎的专家，这将导致训练效率低下。在标准的多专家系统训练中，门控网络倾向于主要激活相同的几位专家。这会形成自我加强的循环，因为得到优先训练的专家会被更频繁地选择。为了减轻这种情况，引入了一种辅助损失来鼓励平等对待所有专家。这种损失确保所有专家获得大致相同数量的训练样本。后续章节还将探讨“专家容量”的概念，这涉及到一个专家能处理的 tokens 数量上限。在 transformers 中，这种辅助损失可以通过 aux_loss 参数来调节。 MoEs 和 TransformersTransformers 模型展示了一个明显的趋势：增加参数的数量可以显著提高性能。Google 的 GShard 项目正是在这方面进行了深入探索，试图将 Transformers 模型扩展到超过 6000 亿个参数。 专家在学习中角色和专长解码器的专家倾向于特定的 Token 组或基础概念。例如可能形成专门处理标点符号或转悠名词的专家，而解码器的专家则在专业化方面表现的较为平均。 增加专家数量对预训练的影响增加更多的专家可以提高样本效率和加速训练过程，但增益逐渐减少（特别是在达到 256 或 512 个专家后），并且在推理过程中需要更多的 VRAM。在大规模应用中研究的 Switch Transformers 的特性，在小规模应用中也得到了验证，即便是每层只有 2、4 或 8 个专家 微调 MoE 技术Mixtral 软件已经在 transformers 4.36.0 版本中得到支持，您可以通过运行 pip install &quot;transformers==4.36.0 --upgrade&quot; 命令进行安装。密集型模型和稀疏型模型在过拟合上表现出明显不同的特点。稀疏型模型更易于过拟合，因此我们可以尝试在专家系统内部应用更强的正则化手段，例如不同层次的 dropout 率——对密集层和稀疏层分别设置不同的 dropout 率。 在微调过程中，一个关键的决策是是否采用辅助损失。ST-MoE 的研究人员尝试关闭辅助损失，并发现即使高达 11% 的 Token 被丢弃，模型的质量也几乎不受影响。这表明 Token 丢弃可能是一种有效的防止过拟合的正则化策略。 另一个尝试是冻结所有非专家层的权重，结果如预期那样导致了性能大幅下降，因为 MoE 层占据了网络的大部分。相反，仅冻结 MoE 层的参数几乎能达到更新所有参数的效果。这种方法可以加速微调过程，同时减少内存使用。通过仅冻结 MoE 层，我们不仅能加快训练速度，还能保持模型的质量。这些发现同样源于 ST-MoE 的研究论文。 何时选择稀疏 MoEs 和稠密模型？在多机器、高吞吐量的场景中，专家系统是非常有效的。如果预训练的计算预算有限，那么稀疏模型将是更佳的选择。对于 VRAM 较少、吞吐量低的情况，稠密模型则更为合适。 注意： 我们不能直接比较稀疏和稠密模型之间的参数数量，因为这两种模型代表的是完全不同的概念。 稀疏模型： 定义：稀疏模型是一种模型设计策略，其中模型结构被设计为具有少量的参数或专家（Experts），每个专家对应于模型中的一个子模型。 特点： 模型结构简单，仅包含少量的专家。s 每个专家专注于处理特定的输入模式或任务。 专家之间在参数空间上是相互独立的。 在推理阶段，只有少量的专家被激活并参与预测，以提高计算效率。 优点： 可以处理复杂的输入模式，并对不同的任务进行专门化处理。 可以节省模型的参数量和计算资源。 缺点： 需要设计合适的选择机制和激活策略，以确保在不同输入情况下激活合适的专家。 对于一些特定的输入模式，可能需要较大的专家数量才能达到较好的性能。 稠密模型： 定义：稠密模型是一种模型设计策略，其中模型结构被设计为具有更多的参数和层级，以便能够更全面地学习输入数据的特征和表示。 特点： 模型结构相对更复杂，包含多个参数较多的层。 在训练过程中，模型可以通过反向传播算法端到端地学习输入数据的特征。 通常用于处理输入模式和任务较为均衡的情况。 优点： 可以捕获输入数据中更丰富的特征，并具有更强的表达能力。 可以适应不同的输入模式和任务，并在训练过程中自动学习特征的权重和表示。 缺点： 参数较多，需要更多的计算资源和内存空间。 在处理特定输入模式和任务时，可能存在过拟合的风险。 ![[Pasted image 20240311104630.png]] 最新进展 The Sparsely-Gated Mixture-of-Experts Layer (2017) 稀疏门控专家混合层（2017） GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (2020) GShard：通过条件计算和自动分片扩展巨型模型（2020） MegaBlocks: Efficient Sparse Training with Mixture-of-Experts (2022) MegaBlocks：高效稀疏训练与专家混合（2022） Mixture-of-Experts Meets Instruction Tuning (2023) 混合专家遇见指令调整（2023） 万字长文详解 MoE - 超越ChatGPT的开源混合专家模型 - 知乎 (zhihu.com) 1. Diederik P. Kingma, Jimmy Ba. “Adam: A Method for Stochastic Optimization.” International Conference on Learning Representations(2014). 2. Ashish Vaswani, Noam M. Shazeer et al. “Attention is All you Need.” Neural Information Processing Systems(2017). 3. Jimmy Ba, J. Kiros et al. “Layer Normalization.” arXiv.org(2016). 4. Rongjie Yi, Liwei Guo et al. “EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models.” arXiv.org (2023). 5. Alexander Hauptmann. “From Syntax to Meaning in Natural Language Processing.” AAAI Conference on Artificial Intelligence (1991). 6. Chenguang Zhu, Yichong Xu et al. “Knowledge-Augmented Methods for Natural Language Processing.” Annual Meeting of the Association for Computational Linguistics (2023). 7. Dastan Hussen Maulud, Subhi R. M. Zeebaree et al. “State of Art for Semantic Analysis of Natural Language Processing.” Qubahan Academic Journal (2021). 8. Neha Yadav. “Applications Associated With Morphological Analysis And Generation In Natural Language Processing.” (2017). 284-286."},{"date":"2024-04-22T13:14:48.983Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/Prompt/","categories":[["undefined",""]],"content":"生物老师:我想让你扮演一名生物老师。我将提供一些问题或概念，你的工作是用易于理解的术语来解释它们。这可能包括提供解决问题的分步说明、用视觉演示各种技术或建议在线资源以供进一步研究。 马屁精：现在请你扮演一位马屁精，不管我说的内容有多么荒谬，你都能恰如其分的拍我的马屁 文学老师:我想让你扮演一名大学文学老师。我将提供一些问题或概念，你的工作是精准回答我的问题，用易于理解的专业术语来解释它们。这可能包括提供解决问题的依据、纠正我的错误以及给出示例的等。 内容: 使用 GPT-4, 设计 Prompt 优化 图说数据库系统 的文本内容. 基本要求: 优化自己负责部分的一个小节, 丰富内容, 优化章节结构, 语言风格等. 采用将大问题分解为多个小问题的方式进行优化, 使用多个 Prompt 对比生成的结果. 最后, 对比丰富后与丰富前的文本. (保留优化前的文本) 提升文章的独特性：Rewrite the existing document tomake it more imaginative, engaging, and unique. 将文档转为引|人入胜的故事：Transform the existingdocument into a compelling story that highlights thechallenges faced and the solutions provided. 提升文档说服力：Refine the existing document byincorporating persuasive language and techniques tomake it more convincing and impactful. 提升文档的吸引力：Add emotional language and sensorydetails to the existing document to make it morerelatable and engaging. 使内容更加简洁：Refine the existing document byremoving unnecessaryinformation and making it moreconcise and to-the-point. 强调急迫感：Refine the existing document by adding asense of urgency and emphasizing the need forimmediate action. 突出重点：Emphasize important information using boldor italic text. 让模型使用类比或比喻的方法解释复杂问题：Explain complexideas using analogies or comparisons. 添加现实中的例子：Include case studies or real-worldexamples to make concepts more relatable. 我想让你担任心理健康顾问。我将为您提供一个寻求指导和建议的人，以管理他们的情绪、压力、焦虑和其他心理健康问题。您应该利用您的认知行为疗法、冥想技巧、正念练习和其他治疗方法的知识来制定个人可以实施的策略，以改善他们的整体健康状况。我的第一个请求是“我需要一个可以帮助我控制抑郁症状的人。"},{"date":"2024-04-22T13:14:48.983Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/RAG%20%E9%A1%B9%E7%9B%AE/","categories":[["undefined",""]],"content":"![[Pasted image 20240303155734.png]] ChatGPT基于循环神经网络（RNN）和注意力机制的模型架构。它是一个生成式模型，可以根据之前的聊天信息生成响应。 模型的原理如下： 输入编码：在每个对话轮次中，聊天历史被编码为一个输入向量序列。这些向量可以是词向量、字符向量或其他表示形式，根据具体的实现方式而定。 上下文理解：模型使用编码后的输入向量序列，通过循环神经网络（如长短期记忆网络，LSTM）或变种（如GPT-3中的Transformer网络）来理解上下文。这些模型会捕捉到前面对话中的语义和语法结构，并对其进行建模。 注意力机制：模型使用注意力机制来关注历史上下文中与当前生成响应相关的部分。通过对不同部分的注意力分配权重，模型可以更好地理解和应答对话。 响应生成：模型将上下文理解与当前对话的目标进行合并，然后通过解码器生成响应。解码器会根据上下文和已生成内容预测下一个最有可能的词或子序列。 迭代训练：模型通过最大似然估计（MLE）或其他适当的训练目标进行训练，以使生成的响应与训练数据中的目标响应尽可能一致。 上下文维护：模型会维护一个有限的上下文窗口，以限制对话历史的长度。这有助于控制模型的记忆和计算需求，并防止信息过载。 在每个对话轮次中，聊天历史被编码为一个输入向量序列。这些向量可以是词向量、字符向量或其他表示形式，根据具体的实现方式而定。模型使用编码后的输入向量序列，通过循环神经网络（如长短期记忆网络，LSTM）或变种（如GPT-3中的Transformer网络）来理解上下文，这些模型会捕捉到前面对话中的语义和语法结构，并对其进行建模。模型将上下文理解与当前对话的目标进行合并，然后通过解码器生成响应。模型会维护一个有限的上下文窗口，以限制对话历史的长度。这有助于控制模型的记忆和计算需求，并防止信息过载。 这里 memory 模块 保持一个聊天记录的列表，作为历史记录的缓冲区，并且每次都将这些消息与问题一起传递给聊天机器人。 在每个对话轮次中，ChatGPT通过将聊天历史编码为一个输入向量序列来处理上下文信息。这些向量可以采用词向量、字符向量或其他形式的表示，具体取决于实现的方式。 然后，通过使用神经网络（如GPT-3中的Transformer网络），模型能够全面理解上下文，并捕捉前面对话中的语义和语法结构。这样，模型可以建立起对对话历史的深入理解。 模型将上下文理解与当前对话目标相结合，然后通过解码器生成响应。解码器利用上下文信息和已经生成的内容来预测下一个最可能的词或子序列，从而生成连贯的回复。 为了控制模型的记忆和计算需求，并避免信息过载，ChatGPT维护一个有限的上下文窗口，限制对话历史的长度。这种方式确保模型在适应多轮对话时能够保持高效性，并生成准确、有逻辑的回应。"},{"date":"2024-04-22T13:14:48.983Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/RAG%20%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA/","categories":[["undefined",""]],"content":"[[RAG 项目]] 概念检索增强生成（RAG——Retrieval Augmented Generation）是指对大型语言模型输出进行优化，使其能够在生成响应之前引用训练数据来源之外的权威知识库。大型语言模型（LLM）用海量数据进行训练，使用数十亿个参数为回答问题、翻译语言和完成句子等任务生成原始输出。在 LLM 本就强大的功能基础上，RAG 将其扩展为能访问特定领域或组织的内部知识库，所有这些都无需重新训练模型。这是一种经济高效地改进 LLM 输出的方法，让它在各种情境下都能保持相关性、准确性和实用性。 ![[Pasted image 20240303130455.png]]数据提取——embedding（向量化）——创建索引——检索——自动排序（Rerank）——LLM归纳生成 LLM 面临的已知挑战包括： 在没有答案的情况下提供虚假信息。 当用户需要特定的当前响应时，提供过时或通用的信息。 从非权威来源创建响应。 由于术语混淆，不同的培训来源使用相同的术语来谈论不同的事情，因此会产生不准确的响应。 Lang Chain 起源像 ChatGPT 这样的 大语言模型 或者 LLM 可以回答许多话题的问题，但是，一个孤立的 LLM 只知道它的训练内容，这并不包括某些个人信息数据，如果您可以和这些数据与 LLM 进行对话，就会非常有用 Lang Chain 介绍用于构建 LLM 应用的开源开发框架 Lang Chain 组件 prompts 提示 models 模型 indexes 索引 chains 链 agents 代理 Lang Chain 使用——如何与数据对话![[Pasted image 20240303174734.png]]核心： 向量存储 加载数据 拆分为有意义的块 索引–检索文档 语义搜索：最简单的方式 使得 LLM 能回答 文档相关内容 记忆功能 文档加载器从不同格式和来源的数据中访问和转换数据的具体细节，转换为标准化格式，即加载到一个标准的文档对象中，该对象由内容和相关元数据组成。 Lang Chain 中有 80 种 数据加载器 文档加载器之间可以组合为一个通用加载器 可以同时加载多个文件 字符切割 为什么需要字符切割 RAG是一种基于检索的生成模型，它结合了检索和生成的能力。在RAG中，检索阶段用于从大型文本语料库中检索相关的上下文，然后再通过生成阶段生成响应或答案。 切割文本可以帮助RAG模型更好地处理长文本，并提高生成的效率和质量。长文本可能包含大量冗余信息或无关信息，这可能会对模型的性能产生负面影响。通过将文本切割成较小的片段，RAG模型可以在生成阶段更好地处理和理解这些部分，并减少不必要的计算。 另外，文本切割还可以帮助限制输入的长度，以满足模型的输入限制或资源限制。通过将文本切割成块，可以更好地管理模型的内存和计算需求。 总的来说，在转换为向量存储之前，还要对原始数据分为有意义的块，作为后续索引的单位？？。 ![[Pasted image 20240303183026.png]] 导入包第一类 RecursiveCharacterTextSplitter”和”CharacterTextSplitter”都是用于文本分割的类，但它们有不同的实现方式和适用场景。 RecursiveCharacterTextSplitter（递归字符文本分割器）： 递归字符文本分割器是基于递归的分割方法，它将文本逐层地切割成更小的片段。 它使用递归算法，将文本分割成单个字符或字符的子序列。例如，将”Hello”分割为[“H”, “e”, “l”, “l”, “o”]。 递归字符文本分割器适用于一些需要对文本进行字符级别处理的任务，例如生成文本的字符级别表示或字符级别的语言建模。 CharacterTextSplitter（字符文本分割器）： 字符文本分割器是将文本按照固定长度切割成块的方法。 它将文本按照指定的块大小分割成连续的字符序列。例如，将”Hello, world!”以块大小为5分割为[“Hello”, “, wor”, “ld!”]。 字符文本分割器适用于一些需要对文本进行块级别处理的任务，例如将文本输入模型进行批处理或限制输入长度。 默认的分隔符是‘\\n’，如果没有这个分隔符，即使到达 长度也不会分隔。 带有回溯的正则表示 第二类 因为有许多基于 token 计数设计的 LLM 上下文窗口 第三类 包的使用只有两个成员函数 嵌入将文本转为数字格式将块放入索引之中 转换为向量 向量存储与使用 边缘情况检索的失效问题原因：基于语义进行查找。 重复的查询结果 查询不能准确把握语义 高级检索MMR 最大边际相关性如果选择与嵌入空间中查询最相似的文档，实际上可能会错过一些多样化的信息指定搜索源这样做的代价是需要对语言模型进行更多的调用，但密非常适合将最终答案集中在最重要的内容上 对话检索链在检索问答链的基础上添加了一个新的部分，不仅有记忆，它将历史记录和新问题压缩成一个独立的问题以便传递给向量存储以查找相关文档 记忆功能保持一个聊天记录的列表，作为历史记录的缓冲区，并且每次都将这些消息与问题一起传递给聊天机器人。 Feel free to copy this code and modify it to add your own features. You can try alternate memory and retriever models by changing the configuration in load_db function and the convchain method. Panel and Param have many useful features and widgets you can use to extend the GUI.请随意复制此代码并对其进行修改以添加您自己的功能。 您可以通过更改“load_db”函数和“convchain”方法中的配置来尝试替代内存和检索器模型。 Panel 和 Param 有许多有用的功能和小部件，可用于扩展 GUI。 "},{"date":"2024-04-22T13:14:48.974Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/AI%20%E5%AD%A6%E4%B9%A0/","categories":[["undefined",""]],"content":"教程书籍：动手学深度学习 基础知识[[机器学习基本概念]][[深度学习基本概念]] 常见模型[[GAN]][[变分编码器]][[扩散模型]][[大模型入门]] 自注意力机制是一种用于处理序列数据的机制，它允许模型在编码一个特定位置的单词时，可以关注到其他位置的单词。在这种机制下，每个单词都会与序列中的其他单词进行交互，以便更好地理解上下文。 在自注意力机制中，输入序列被表示为一个矩阵，其中每一行代表一个输入位置，每列代表该位置对应的特征。 在自注意力机制中，每个输入位置都与其他位置进行交互，并通过计算注意力权重来获取其他位置的信息。这个注意力权重决定了每个位置对其他位置的关注程度，从而可以捕捉到全局的上下文信息。"},{"date":"2024-04-22T13:14:48.974Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/AI%20%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95/","categories":[["undefined",""]],"content":"2023年值得关注的十篇人工智能研究论文 - 知乎 (zhihu.com)"},{"date":"2024-04-22T13:14:48.974Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/Finetuning/","categories":[["undefined",""]],"content":""},{"date":"2024-04-22T13:14:48.974Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/GAI%20%E7%9A%84%E5%BA%94%E7%94%A8/","categories":[["undefined",""]],"content":"个人环境搭建APIopenaiapi 使用文档 wenxin[[微信项目]]教程：如何申请文心一言&amp;文心千帆大模型API调用资格、获取access_token，并使用SpringBoot接入文心一言API_文心一言api申请-CSDN博客 在线调试工具 token24.0339bde53ae4eed0ed979b9f9959b20e.2592000.1711684739.282335-53912329申请：[api 文档](使用网页调试工具获取access_token - 千帆大模型平台 | 百度智能云文档 (baidu.com)) github Copilot 免费使用 gpt-4  我希望你充当激励教练。我将为您提供一些关于某人的目标和挑战的信息，而您的工作就是想出可以帮助此人实现目标的策略。这可能涉及提供积极的肯定、提供有用的建议或建议他们可以采取哪些行动来实现最终目标。我的第一个请求是“我需要帮助来激励自己在为即将到来的考试学习时保持纪律”。 常用的大模型应用技术Prompt[[Prompt]] Course OpenAI Prompt 指南 ChatGPT Prompt Engineering for Developers ChatGPT Prompt Engineering for Developers - 中文字幕版 Project 内容: 使用 GPT-4, 设计 Prompt 优化 图说数据库系统 的文本内容. 基本要求: 优化自己负责部分的一个小节, 丰富内容, 优化章节结构, 语言风格等. 采用将大问题分解为多个小问题的方式进行优化, 使用多个 Prompt 对比生成的结果. 最后, 对比丰富后与丰富前的文本. (保留优化前的文本) 进阶: 其他任意 Prompt 工作都可. 例如: 优化翻译结果, 优化特定领域结果(数据库专家, 文档编写专家等) 实验室提供的 GPT-4 Web  其他资料 GPTs 的 Prompt, 可用于参考 RAG[[RAG 检索增强]] Course Retrieval Augmented Generation (RAG) 简介 - 中文字幕版 LangChain Chat with Your Data LangChain Chat with Your Data - 中文字幕版 Project 利用指定的书籍文档, 构建 RAG 系统. 利用 RAG 系统优化 图说数据库系统 的文本内容. 基本要求: 优化自己部分的一个小节, 丰富内容, 修正错误. 与原文本, Prompt 生成的文本进行对比. 进阶: 构建其他任意 RAG 系统. 例如: 分布式课程 RAG 系统, 实验室文档 RAG 系统, 医疗 RAG 系统等. 实验室提供的 API-Key 可以用于 机器人的 聊天记录进行保存 其他资料 Building and Evaluating Advanced RAG Functions, Tools and Agents with LangChain devv.ai 是如何构建高效的 RAG 系统的 复杂 RAG 的技术考虑 MoE[[MoEs]] Course 深入理解混合专家模型 微调[[Finetuning]] Course Finetuning Large Language Models吴恩达《微调大型语言模型》| Finetuning Large Language Models（中英字幕）_哔哩哔哩_bilibili "},{"date":"2024-04-22T13:14:48.974Z","url":"/2024/04/22/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/GAI%20%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/","categories":[["undefined",""]],"content":" 基础概念NLP分类： 中间任务（序列标注）：中文分词，词性标注，NER，句法分析，指代消解，语义 Parser 等， 最终任务：文本分类，文本相似性计算，机器翻译，文本摘要。 自然语言理解类任务：本质上是分类任务：分类任务、句子关系判断 自然语言生成类任务 NLP 三大特征提取器（CNN|RNN|TF） 一个特征抽取器是否适配问题领域的特点，有时候决定了它的成败，而很多模型改进的方向，其实就是改造得使得它更匹配领域问题的特性 解决 NLP 任务 最重要的就是 模型的特征提取能力 现状 RNN 基本完成它的历史使命。 CNN 如果改造得当，有希望有自己在NLP领域的一席之地。 Transformer 最主流的特征提取器 RNN 现状RNN 在 NLP 中的演进 RNN 采取线性序列结构不断从前往后输入信息，这种线性结构在反向传播时存在优化困难问题，因为反向传播路径太长，容易导致严重的梯度消失或者梯度爆炸问题。 引入 LSTM 和 GRU 模型，通过增加中间状态信息直接向后转播，以此缓解梯度消失问题，获得很好的效果。 不断优化，从图像领域引入 Attention 机制，叠加网络使得层更深。 引入 Encoder-Decoder 框架。 RNN 的优势 RNN 的结构天然适配解决 NLP 问题，NLP 问题的输入往往是一个不定长的线性序列句子。 而 RNN 本身结构 就是一个可以接纳不定长输入的由前向后进行信息线性传导的网络结构 LSTM 引入三个门， 对于捕获长距离特征也是非常有效的 RNN 的问题 老模型先天不如新来的 CNN Transformer 。 RNN 本身的序列依赖结构对于大规模并行计算来说相当不友好。而 CNN 和 Transformer 不存在这种问题。本质： 时间步有前后依赖 RNN 并行改造 保留连续时间步的隐层连接 在隐层单元之间并行计算， 部分打断连续时间步 （这样改进之后有点像简化的 CNN）失去原本样貌 CNN 现状怀旧版 CNN 输入层 卷积层：特征提取层，卷积核（filter） pooling 层：对 Filter 的特征进行降维操作（从一个卷积核获得的特征向量里只选中并保留最强的那一个特征） 输出层 怀旧版 CNN 的问题 卷积层如何捕获远距离特征 CNN 卷积层捕获的实际上是单词 的 k-gram 片段信息，k 的大小决定了能捕获多远距离的特征 不是覆盖连续区域，在同样的滑动窗口大小的前提下，覆盖不连续区域 增加卷积层层数。 Pooling 层 Pooling的操作逻辑是：从一个卷积核获得的特征向量里只选中并保留最强的那一个特征，所以到了Pooling层，位置信息就被扔掉了，这在NLP里其实是有信息损失的。 所以在 NLP 领域里，目前 CNN 的一个发展趋势是抛弃 Pooling 层，靠全卷积层来叠加网络深度 怀旧版 CNN 的优势并行计算能力：单层卷积层，首先对于某个卷积核来说，每个滑动窗口位置之间没有依赖关系，所以完全可以并行计算；另外，不同的卷积核之间也没什么相互影响，所以也可以并行计算。 Transformer 登场[[Transformer]]![[Pasted image 20240302135631.png]]Transformer 模型由编码器（Encoder）和解码器（Decoder）两部分组成，这两部分都采用了多层的自注意力（Self-Attention）和前馈神经网络（Feed-Forward Neural Network）。 编码器（Encoder）：编码器的主要任务是理解输入的信息，并将其转化为一种内部表示形式。在 Transformer 中，编码器接收一系列输入（比如一个句子中的每个词），并通过自注意力机制和前馈神经网络，将每个输入转化为一个向量。这个向量包含了输入的信息，以及它与其他输入的关系。编码器由多个这样的层堆叠在一起，每一层都会进一步提炼这些向量。 解码器（Decoder）：解码器的主要任务是根据编码器的输出生成最终的输出。在 Transformer 中，解码器也是由多个自注意力机制和前馈神经网络的层组成。但解码器有两个自注意力层，一个是对自身的输入进行自注意力计算，另一个是对编码器的输出进行自注意力计算。这使得解码器在生成每个输出时，都能考虑到所有的输入和已经生成的输出。 区别和联系：编码器和解码器的主要区别在于，编码器只需要理解输入，而解码器需要理解输入并生成输出。因此，解码器比编码器多了一个自注意力层，用于理解已经生成的输出。编码器和解码器的联系在于，它们都使用了自注意力机制和前馈神经网络，而且解码器在生成输出时，会使用编码器的输出。 在 GPT 中，只使用了 Transformer 的解码器部分，因为 GPT 的任务是生成文本，不需要理解输入。而在 BERT 中，只使用了 Transformer 的编码器部分，因为 BERT 的任务是理解文本，不需要生成输出。 目前 Transformer 不仅统一了 NLP 诸多领域，也逐步替换图像处理各种任务被广泛使用的 CNN 等其他模型的进程之中；类似的，多模态模型也目前 基本都采用了 Transformer 模型 Transformer 问题因为输入的第一层网络是Muli-head self attention层，我们知道，Self attention会让当前输入单词和句子中任意单词发生关系，然后集成到一个embedding向量里，但是当所有信息到了embedding后，位置信息并没有被编码进去。 Transformer 如何解决问题 如何解决不定长问题： 类似 CNN 假定输入的最大长度，不够用 padding 补充， 如何解决位置编码问题： 必须要有一个位置编码。 Transformer 用位置函数来进行编码； Bert 模型则给每一个单词一个 Position embedding，和 单词 embedding 加起来形成单词的输入； 如何解决长距离依赖问题。 elf attention天然就能解决这个问题，因为在集成信息的时候，当前单词和句子中任意单词都发生了联系，所以一步到位就把这个事情做掉了。不像RNN需要通过隐层节点序列往后传，也不像CNN需要通过增加网络深度来捕获远距离特征，Transformer在这点上明显方案是相对简单直观的。 三者比较 从语义特征提取能力来说，目前实验支持如下结论：Transformer在这方面的能力非常显著地超过RNN和CNN（在考察语义类能力的任务WSD中，Transformer超过RNN和CNN大约4-8个绝对百分点），RNN和CNN两者能力差不太多。 在长距离特征捕获能力方面，目前在特定的长距离特征捕获能力测试任务中（主语-谓语一致性检测，比如we……..are…），实验支持如下结论：原生CNN特征抽取器在这方面极为显著地弱于RNN和Transformer，Transformer微弱优于RNN模型(尤其在主语谓语距离小于13时)，能力由强到弱排序为Transformer&gt;RNN&gt;&gt;CNN; 但在比较远的距离上（主语谓语距离大于13），RNN微弱优于Transformer，所以综合看，可以认为Transformer和RNN在这方面能力差不太多，而CNN则显著弱于前两者。 从综合特征抽取能力角度衡量，Transformer显著强于RNN和CNN，而RNN和CNN的表现差不太 RNN在并行计算方面有严重缺陷，这是它本身的序列依赖特性导致的，所谓成也萧何败也萧何，它的这个线形序列依赖性非常符合解决NLP任务，这也是为何RNN一引入到NLP就很快流行起来的原因，但是也正是这个线形序列依赖特性，导致它在并行计算方面要想获得质的飞跃，看起来困难重重，近乎是不太可能完成的任务。而对于CNN和Transformer来说，因为它们不存在网络中间状态不同时间步输入的依赖关系，所以可以非常方便及自由地做并行计算改造，这个也好理解。并行计算能力由高到低排序如下：Transformer和CNN差不多，都远远远远强于RNN。 待完成！！！！ Bert|GPT|Transformer 区分BERT（Bidirectional Encoder Representations from Transformers）、GPT（Generative Pretrained Transformer）和Transformer 是三种在自然语言处理（NLP）领域广泛使用的模型或模型架构。它们之间的关系可以从以下几个方面来理解： Transformer：Transformer 是一种模型架构，它在 “Attention is All You Need” 这篇论文中首次被提出。Transformer 模型的主要特点是它完全放弃了传统的 RNN（循环神经网络）或 CNN（卷积神经网络）结构，而是完全依赖于 self-attention 机制来处理序列数据。这种结构使得 Transformer 模型在处理长距离依赖和并行计算方面具有优势。 GPT：GPT 是 OpenAI 开发的一种基于 Transformer 的模型。GPT 使用了 Transformer 的解码器部分，并且采用了单向（从左到右）的自注意力机制。这使得 GPT 在生成文本（如写作、翻译等任务）方面表现出色。 BERT：BERT 是 Google 开发的一种基于 Transformer 的模型。与 GPT 不同，BERT 使用了 Transformer 的编码器部分，并且采用了双向的自注意力机制。这使得 BERT 能够理解文本中的上下文信息，因此在理解、分类、问答等任务中表现优秀。 总的来说，Transformer 是一种模型架构，而 GPT 和 BERT 都是基于这种架构的模型，但它们在具体实现和应用上有所不同。 时间线前范式转换1.0 bert 和 gpt 模型出现以前，NLP 领域流行的技术是深度学习模型 NLP 领域的深度学习 大量改进的 LSTM 模型 | 少量的改进 CNN 模型作为特征抽取器 以 sequence to sequence（encode-decoder）+ Attention 作为各种具体任务典型。 目标：如何有效增加模型深层或模型的参数容量。 即：怎么才能往encoder 和decoder 里不断叠加更深的 LSTM 或 CNN 层，来达成增加层数和模型容量的目标。 分类： 中间任务：中文分词，词性标注，NER，句法分析，指代消解，语义 Parser 等， 最终任务：文本分类，文本相似性计算，机器翻译，文本摘要。 自然语言理解类任务：本质上是分类任务 自然语言生成类任务 评价：总体而言不是很成功，或者说和非深度学习方法相比，带来的优势i不算很大。 NLP 深度学习不算成功的原因： 训练数据总量的限制。 LSTM | CNN 特征提取器表达能力不强。 Ber|GPT 这两个预训练模型的出现，代表 NLP 领域的飞跃 NLP 研究子领域日渐消亡 中间任务不应该出现，这是 NLP 技术发展水平不够高的一种体现。很难一步做好有难度的最终任务。 Bert|GPT 出现后没有必要做这些中间任务了，因为 Bert|GPT 已经把这些中间任务作为语言学特征，吸收到了 Transformer 的参数里， NLP 不同子领域技术方法和技术日渐统一。 NLP 领域特征提取器都逐渐从 LSTM|CNN 统一到 Transformer 上。 大多数 NLP 子领域的研发模式切换到了两阶段模式，： 模型预训练阶段 应用微调 或者： Zero Shot Few Shot "},{"date":"2024-04-22T13:14:48.922Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E8%AF%AD%E6%B3%95%E5%AF%86%E7%A0%81%E5%88%86%E6%9E%90/","categories":[["undefined",""]],"content":"密码模型是数据驱动的结构，它捕获密码样本中的规律性，并有助于分析密码创建模式，这经常被用于密码猜测攻击。语义PCFG[21]是一种捕获句法和语义信息的概率上下文无关文法(PCFG)。它假设，除了随机序列外，人们还会选择有意义的单词组合，这些组合在进行大规模分析时会形成规则模式。这些模式与自然语言中发现的模式相似，但并不严格遵守自然语言语法规则。当主要在英语数据集上训练时，语义PCFG被证明优于Weir等人的PCFG。[23]在以LinkedIn、MySpace、RockYou和Gamigo列表为目标的猜测会话中[11，21]。语义密码模型是语言密码模型家族中的一员，它依赖于语言资源和过程，如分析、分割和分类。语言模型仍然很有价值，因为它们为密码列表的构成提供了可解释的描述，使研究人员能够详细研究用户群体之间的差异和密码策略的影响。然而，关于这种语言建模行为的重要问题仍然没有得到回答。不同级别的信息(例如，句法、语义)对概括的个体贡献以及因此的猜测性能是未知的。从小样本中学习模式的能力还没有被很好地理解，控制过度匹配的参数的有效性也没有被很好地理解，例如语义专一性–语义语法训练的一个免费参数–和概率平滑方法，这是所有语言学方法共同的。此外，在引入语义模型后，蒙特卡罗强度评估使在非常长的会话中估计猜测成功成为可能[7]。这使得对语义模型的原始评估过时了，因为它仅限于破解多达30亿次猜测的尝试。较新的模型通常使用蒙特卡罗评估，这使得与较旧的模型进行比较变得困难。特别是，Melicher等人的神经密码模型。[16]取得了优异的性能，超过了其他高分自动裂解方法，包括Weir等人的PCFG的改进版本。"},{"date":"2024-04-22T13:14:48.920Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%9C%AA%E5%91%BD%E5%90%8D/","categories":[["undefined",""]]},{"date":"2024-04-22T13:14:48.920Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E8%AF%AD%E4%B9%89%E5%AF%86%E7%A0%81%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%AF%86%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%BC%8F%E5%88%86%E6%9E%90/","categories":[["undefined",""]],"content":"在本文中，我们对学习密码中的语言模式进行了细致入微的研究，并使用最新的评估方法更新了语义 PCFG 的性能，以提供更长的猜测会话的结果 [7]。,我们通过部署在高性能计算基础设施上的大参数扫描实验来实现这一点。,在以下部分中，我们报告了比较 PCFG 性能的实验结果 (a) 使用和不使用语义符号（WordNet 含义和专有名称）进行训练； (b) 经过不同级别的语义概括训练； (c) 使用不同大小的密码列表进行训练； (d) 使用最大似然估计（一种将零概率分配给未见字符串的方法）和概率平滑（将概率质量分配给未见字符串）进行训练 [6]。重要的是，我们将三代 PCFG（Weir 等人[23]、Veras 等人[21] 和 Komanduri [12]）与最新的神经网络模型之一（Melicher 等人[16]）进行了比较。 我们的实验设置包括使用 RockYou 列表训练的语法，以及对从 LinkedIn 和 000webhost 泄露的密码进行测试的语法，以及使用相同数据的独立交叉验证设置。,此外，我们使用语言语法来定性研究最近三个密码泄露的模式：000webhost、Comcast 和 Mate1。,我们提出了这些泄漏的高级图形模型，讨论了语法规则的相似性，并进行了交叉泄漏破解实验。总之，本文研究了多个参数对 PCFG 和神经网络模型猜测性能的影响，并对最近的密码泄漏进行了基于语法的定性分析。 语义 PCFG文本处理流将每个密码分解为令牌（即终端符号）并为其分配语言属性，从而产生形式的元组（令牌、POS、意义）。 分词： Norvig 的统计算法 [18]，该算法选择具有最高联合概率的分词，其中每个标记的概率使用二元模型计算（来自 Google Web Trillion）词语料库）。 词性标注是通过退避标注器完成的，该标注器由在布朗语料库和 WordNet 以及其他已知与密码相关的利基命名实体语料库（城市，给定）上训练的一般统计模型（三元组、二元组和一元组）组成。名和姓氏）。请注意，拼写错误（例如，passwrd）和替换（例如，passw0rd）不按词性语法和语义语法进行分类。将来可以添加将拼写错误的单词分类为语义类别的模块 语义，见下 语义模型语义模型主要关注单词的意义和语义关系，它利用WordNet等语义资源对单词进行语义标记，以便捕捉密码中的语义规律。能够提供更详细的密码结构分析，有助于更好地理解密码样本中的模式和提供更可解释的密码强度建议。语义模型在小样本训练和需要解释性较强的情况下显示出更好的猜测性能。 语义该词的意义取自 WordNet 语料库 [17]，这是一种语言树结构，其中概念通过代表 IS-A 关系的边连接起来，就像狗 IS-A 动物 IS-A 哺乳动物 IS-A 生物一样。每个被词性标注器标注为名词或动词的字母字符串都会收到一个 WordNet 意义键形式的语义标签。单词可能有多种含义，因此我们选择 WordNet 频率最高的含义。为了使词义具有泛化能力，需要建立映射，以便将低级词义分组为更广泛的类别。这可以通过选择由一组抽象类（内部树节点）组成的树切割来完成，每个抽象类代表其所有后代。因此，如果“运动”类是树木砍伐成员，则任何出现“棒球”的情况。都将在训练的语法中被标记为“sport”。这种映射具有扩大语法推理范围的效果，因为通过抽象类间接提高了未见过单词的概率。在实践中，包含数据中频率相似的词义的 WordNet 子树往往会比包含异常值的子树映射到更抽象的含义。请注意，高频词更有可能映射到特定类别，通常意味着根本没有抽象（例如，love → love.n.01）。 词性模型关注单词的语法属性，比如名词、动词、形容词等，它利用词性标记器对单词进行词性标记，以便捕捉密码中的语法规律。词性模型则更适合在短时间内进行密码猜测，因为它能够更灵活地捕捉密码中的语法规律，尤其在初始猜测阶段表现更好。 终端平滑 在 Weir 等人的 PCFG 和语义 PCF G 中，终端概率是通过最大似然（ML）来估计的。在机器学习中，选择模型的参数是为了最大化给定模型的数据的概率；因此，不会将概率质量分配给看不见的字符串。从理论上讲，这会影响小样本的学习，语义泛化的有效性取决于终端概率平滑。终端平滑需要两个决定：将哪些词汇添加到语法中以及如何为其分配概率。 Wordnet 涵盖了大量单词。我们称之为先验词汇，由以各种方式变形的每个 Wordnet 引理组成：名词以单数和复数形式出现，动词以所有词形变化形式出现。根据可用资源，可以通过分配给语义类别的附加单词列表来丰富先验词汇表（类似于我们处理名称的方式）。 除了先验终端之外，后验词汇表还包括数据中观察到的每个终端。给定非终结符号的终结串为 θˆ i &#x3D; xi + α N + αd ， (1) 其中 xi 是观测频率； N 是非终结符号下观测频率之和； α 称为伪计数，可以被解释为假定先验观察字符串的次数（当 α &#x3D; 0 时，默认为 ML）； d 是给定非终结符的词汇大小。 该估计器 ( ˆθi ) 称为加性平滑或拉普拉斯平滑，当假设先验一致时，它相当于贝叶斯估计量 [15]。当词汇量非常大时，加法平滑可能不准确，就像三元组一样，在这种情况下，更复杂的估计量更合适，比如 Good -Komanduri 使用图灵估计器来训练 PCFG [12]。由于我们的词汇量约为数十万，拉普拉斯估计器就足够了，并且实现更简单。 蒙特卡罗强度估计器是一种用于测量密码破解成功率的方法。它通过计算测试集中每个语法的密码的概率，然后估计猜测次数。具体来说，猜测次数指的是在尝试所有语法中概率最高的密码之前需要尝试的次数。蒙特卡罗估计器允许我们在无限长度的会话中测量成功率，而无需进行大量的猜测枚举，这在计算上是非常昂贵的。因此，蒙特卡罗强度估计器是一种用于评估密码破解成功率的有效方法。 基于语义信息，各个数据集之间的关系？基于语义信息，RockYou、Comcast、Mate1和000webhost泄露的密码列表之间存在一些相似之处。研究发现，RockYou和Mate1泄露的密码具有很大的语义和结构重叠，表明这两个泄露数据具有相似的语义模式。而在000webhost密码列表中，密码具有明显的统一结构，但在语义上却缺乏一致性，至少在英语语言语法中如此。此外，Comcast密码列表中出现了许多包含类别”worker”和”hacker”的模式，还有包含类别”defender”和”windows”的模式，以及服务名称本身的模式。这些发现揭示了密码泄露的语义偏好与服务的人口统计和主题之间的关联，以及密码政策对所选密码结构的影响。 （a）LinkedIn 密码缺乏强英语语义依赖性； (b) LinkedIn 具有较强的英语语义依赖性，但与 RockYou 中的不同； (c) 英文密码一般缺乏强语义依赖性。 总结这篇文章主要研究了语义密码模型和密码中的语言模式，通过对不同语言模型的大规模分析，比如PCFG和神经网络模型，以及对RockYou，LinkedIn，Mate1，Comcast和000webhost等密码泄露数据进行定性分析和比较。研究发现，PCFG在猜测非随机密码方面表现出色，尤其是在小样本训练集上。此外，语义模型在特定情况下表现出更好的猜测性能。在对不同密码列表的语言模式进行比较时，研究发现了一些有趣的区别，比如Mate1和RockYou泄露数据中共享的语义模式。总的来说，这项研究提供了关于密码模型和语言模式的深入见解，特别是在不同类型的密码列表中的比较分析。 重点是语义密码模型的参数，这是一个用词性和语义信息训练的 PCFG。 未经概率平滑训练的语法往往会过度拟合，平滑可以为用小样本训练的 PCFG 提供强大的能力：仅用 1,000 个 RockYou 密码训练的语法就能够猜测近 500 万个 000webhost 密码，训练样本大小增加到超过 1000 万个密码时，收益递减。 分离了有语义和没有语义的 POS 的影响，发现向语法中添加语义信息的好处与添加 POS 信息带来的收益相比很小，并且取决于训练数据的大小。（维拉斯等人。证明具有语义和词性信息的语法在很大程度上优于 Weir 等人的语法。） 发现 Komanduri PCFG 比其他测试的 PCFG 具有更好的猜测能力，并且存在 Melicher 等人的神经模型的情况。比我们测试的 PCFG 的猜测能力更差。 利用语义模型的解释能力来定性检查最近的密码泄露事件 RockYou 和 Mate1 泄露事件在语义和结构上有很大的重叠， 而 000webhost 密码具有非常统一的结构，但语义统一性很小，至少从英语语法来看是这样。 "},{"date":"2024-04-22T13:14:48.905Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8F%A3%E4%BB%A4%E5%AE%89%E5%85%A8%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6/","categories":[["undefined",""]]},{"date":"2024-04-22T13:14:48.905Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%98%8E%E6%96%87%E5%8F%A3%E4%BB%A4%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/","categories":[["undefined",""]],"content":"引言口令常用，口令脆弱 口令破解主要关注点在于利用尽可能小的生成子点集合覆盖尽可能多的真实口令 Markov PCFG NN反向传播算法是神经网络的核心算法。该算法的思想是，通过对预估结果的评价的反馈判断模型参数中导致预测结果变坏的量，对其加以纠正，使得模型逐渐趋向最优。 "},{"date":"2024-04-22T13:14:48.905Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%A4%8D%E7%8E%B0/%E5%A4%8D%E7%8E%B0%EF%BC%9APassGPT/","categories":[["undefined",""]],"content":"基本流程数据集：RockYou password leak 使用论文预训练模型： 最多 10 个字符密码训练的 PassGPT 此处。 最多 16 个字符的密码训练的版本需要我们团队的研究批准，可以在此处找到。 自己训练模型： 密码 tokenizer ：字符级（防止像 NLP 那样将 字符 串为分词），在模型下保留有意义的概率分布。 自定义一个配置文件： CONFIG_PATH ，可以使用默认的文件 yaml 文件. 训练模型： 模型使用 进一步的生成参数，例如“–temper”、“–top_p”或“–top_k” 环境conda + python3.11requirements.txt 问题问题： ‘utf-8’ 解析出错 UnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xf1 in position 5079963: invalid continuation byte 解决1：使用默认字符替换，默认是 “?”。with open(args.train_path, &quot;r&quot;, encoding=&quot;utf-8&quot;, errors=&#39;replace&#39;) as f 解决2：定位到没有解析的字符行，重新输入 问题： 包缺失 问题：显存不够 换服务器，当前服务器显卡资源不足：![[Pasted image 20240322093550.png]] 新服务器![[Pasted image 20240322093300.png]] 问题： 显示 Driver Version 过低（导致训练时不能使用 GPU） 此时不一定是 Driver Version 版本的问题，在实际中只要 显卡 显卡驱动兼容，那么就可以使用显卡跑程序。 通过 降低 pytorch 的版本到 2.0.0 ，成功运行。 ![[Pasted image 20240322011742.png]] ![[Pasted image 20240322091910.png]] 当前的进度目前的进度： PassGPT 训练过程结束（最大字符 长度 设置为 10） ![[Pasted image 20240322102651.png]] 当前实验存在的问题：运行模型 需要 的配置较高。模型显存需求估计 下一步计划之前是跑了模型的训练部分，下一步就是使用模型生成密码，并且分析数据。 计划的实验目标是： 1. 论文中没有提到只是说生成一定的数量的密码之后对测试数据集的覆盖率达到了多少，但是并没有提到那些 类型数据 是模型难以拟合的，我主要想看一下哪些没有匹配到的密码长什么样子。 2. 对比以下 开源的预训练 10 字符 模型 和 此次训练的模型的效果，验证一下论文的效果。 目前的一个小 idea，参考 [[MoEs]] ，但了解还比较粗浅。 最后感觉论文读的还是太少了，需要接着调研论文 模型运行结果："},{"date":"2024-04-22T13:14:48.845Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/TEMP/","categories":[["undefined",""]],"content":"TODO 完成 GuessFuse 阅读。 阅读 GuessFuse 提到的多种模型结合方法。（也可能是密码强度估计器。） 有一种[33]介绍了一种称为 hyPassGu 的混合猜测框架。 hyPassGu 通过限制每个模型生成目标类型的密码并分别确定猜测次数，利用 PCFG 和马尔可夫模型的优势。 尽管声称 hyPassGu 可以应用于 PCFG 和马尔可夫之外的其他模型， 和MOE 的思路有点像 MOE 相关的论文和博客阅读。 密码生成思路参考 [[GuessFuse]] 感觉结构可以 相关工作道德考虑。尽管我们使用的数据集是公开的，并广泛用于密码猜测研究 [8]、[27]、[32]–[34]，但这些数据集包含敏感的个人信息。 因此，我们仅分析密码数据的分布特征，并报告汇总的统计信息。 我们不会将任何数据用于学术研究以外的目的，因此不会增加受影响个人的风险。"},{"date":"2024-04-22T13:14:48.844Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/REDPACK/","categories":[["undefined",""]],"content":"摘要 传统基于规则的方法依赖于专家知识 最新的基于深度学习方法，passGAN rpassGAN 在某些方面不如 PCFG 不同的深度学习方法有不同的候选密码空间，且无法重叠。 本文提出 REDPACK ，解决了 GAN 方法的不足，以有效的方法集合了多个候选密码生成器模型。 使用 rPassGAN 的候选密码鉴别器作为密码选择器。 有选择的搜集密码，我们实现了更真实的密码字典。 在符号，数字，大小写字母组成的密码字典上测试我们的系统。 优于 基于规则，GAN，PCFG， 生成候选密码减少 65% 时，破解性能仅损失 20%。 引言 基于规则破解。基于专家的知识扩展性差。 先进的密码破解很有用 防御角度：作为密码强度估计器 攻击角度：通过多种方式利用先进的密码破解方法。 个人角度：破解自己忘记了的密码。 社会层面：执法机构需要破解密码来收集犯罪证据。 旧的：Recurrent PassGAN对 PassGAN 的改进，弥补基于规则的方法和数据驱动的方法，例如马尔科夫模型等。 模型足够表示用户使用密码的模式和上下文结构。 深度学习不需要密码属性和结构的先验知识，只从训练数据中学习特征。 普通 PassGAN：使用了Wasserstein GAN梯度惩罚（WGAN-GP）[11]模型。 WGAN-GP 的基础深度神经网络是基于卷积神经网络 (CNN) 的残差网络。 rPassGAN 通过修改PassGAN的基础神经网络类型和结构来提高密码破解性能。 rPassD2CGAN（rPassGAN 的双鉴别器版本）的性能优于 rPassGAN。然而，在rPassD2CGAN的训练过程中，它有时会变得不稳定。因此，我们通过使用另一个模型 rPassD2SGAN 来克服这个弱点。我们通过多个实验证明了 rPassGAN 生成的候选密码对于增强密码强度估计器的有效性。 与概率上下文无关语法（PCFG）和其他马尔可夫模型（例如 OMEN [1]）相比，rPassGAN 有时破解的密码较少。 新的：REDPACK核心：通过少量猜测来最大化密码破解性能。策略： 有选择地收集更真实的候选密码，以提高密码破解的效率。如果可以使用各种模型的候选密码字典。 相对论平均标准 GAN [13] 的判别器作为估计器运行，它评估来自各种预先生成的候选者的输入密码的真实性。 强调：通常只使用 GAN 生成器，我们还使用了 GAN 鉴别器。贡献： GAN 判别器用于帮助生成密码。 REDPACK 能用多个预生成的候选密码制作更有效的候选字典。 为 REDPACK 构建了一个自定义规则集。 文章结构 第 2 节：相关密码猜测模型 第 3 节：GANs 和 relativistic average GANs 第 4 节：REDPACK 的概念和架构 第 5 节：训练超参数的配置过程。实验结果 第 6 节：结论 相关工作 基于规则的方法： hashcat ，JTR 开源软件多种攻击方式。 马尔科夫模型。 缺点：有明确的密钥空间。 马尔可夫和 PCFG。 使用符号：数字，字母，符号，单词。组合包含在特定概率分布中 使用马尔可夫过滤器消除滴概率密码 使用彩虹标加速。 图灵机处理特殊符号密码。 一些改进 PCFG 使用学习语法结构，密码组合。 一些优化。 深度学习方法 PassGAN ：WGAN-GP 代价函数 + CNN rPassGAN：RNN 效果：能破解 PCFG 等其他模型无法破解的密码。 背景知识GAN [[GAN]] : GAN, WGAN, IWGAN 。其中 IWGAN 实验激发了 Hitaj 等人的兴趣。 [9]将IWGAN应用于密码猜测问题。 他们将创建的模型称为 PassGAN。 Relativistic average GAN GAN 可分为 IPM 和 non-IPM， non-IPM 在学习阶段不稳定，模型优化困难。 IPM： Jolicoeur [13] 分析了 SGAN 的损失函数，以分析非基于 IPM 的 GAN 的局限性，并提出了一种相对论 GAN 来解决这个问题。 来源： 问题：古德费洛等人。 [10]证明，当判别器以 0.5 的概率对真实数据进行分类时，GAN 训练达到全局最优。 然而，在许多情况下，判别器将真实数据和假数据都分类为真实数据，这对于训练好的生成器来说是不利的。 这是因为判别器没有意识到学习过程中一半的数据是假的； 基于 IPM 的 GAN 在学习过程中相对稳定，因为它隐含地解释了这一事实。 从散度最小化的角度来看，训练判别器以增加 D(xf)，而 D(xr) 不会相应减少，其中 xr 和 xf 分别表示真实数据和假数据。 为了解决这个问题，Jolicoeur [13] 设计了判别器的输出，使其依赖于真实数据和虚假数据。 relativistic GAN ![[Pasted image 20240416105310.png]] 其中 C(x) 是假定的批评者 (D(x) &#x3D; σ(C(x)))。 方程（2）可以解释为判别器对给定真实数据比假数据更真实的概率的估计（D(xf, xr)可以以相反的方式解释）。 如果根据等式（2）设置判别器，则与 SGAN 的生成器仅依赖于假数据不同，相对论性 GAN 的生成器将同时依赖于真实数据和假数据。 然而，它在计算损失时具有 O(m2) 复杂度（m 表示小批量的大小），因为它计算小批量中真数据和假数据之间的批评家的成对差异。 RaGAN 为了解决这个问题，Jolicoeur [13] 提出了一种相对论平均 GAN（RaGAN），它对某些给定数据取相反类型数据的期望。 RaGAN 使用以下损失函数来学习判别器和生成器： ![[Pasted image 20240416105456.png]] 其中 LD 和 LG 分别表示学习判别器和生成器的损失。 方程(3)的复杂度为O(m)。 RaGAN 的判别器平均估计某些给定数据比相反类型的数据更真实的概率。 Jolicoeur [13] 使用不同的数据集表明，与其他 GAN 模型相比，训练 RaGAN 更快、更可靠，并且 RaGAN 的生成器生成了高质量的假数据。 方法介绍：REDPACK概述 背景：之前 GAN 训练完成之后使用 GAN 生成器实现目的，作者在之前的论文中提到了基于 RNN 的 GAN 方法 rPassGAN ，本文使用的就是 rPassGAN 的 判别器 REDPACK：GAN 的判别器是在模型训练结束之后使用的。 ![[Pasted image 20240416111123.png]] 训练阶段： 生成器生成假密码，鉴别器区分，对抗训练，直到收敛。 选择阶段： 使用多个密码生成器产生候选密码。作为训练阶段的结果。（本文使用了 三个或者四个） 鉴别器计算每个候选密码真实程度的概率。 将高概率的候选密码提供给 Hashcat等密码破解工具。 鉴别器训练结构架构：![[Pasted image 20240416143214.png]]优化基于 RNN 的 GAN （判别器优化：使用了 RaGAN 和 IWGAN 的概念来实现了更强大的判别器）的流程： G 根据给定的任意噪声分布生成假密码，图中绿色路径； 根据等式 (3) ，当且仅当假密码的批评值大于图中蓝色存储所描述的真实数据时，D 才确定假密码为真实密码。 蓝色路径上的真实密码在相反情况下被视为真实的。(这里的相反可能是指的比较对象和) D 给出梯度作为惩罚，鼓励 G 生成更真实的密码，用红色路径表示。这样的流程使得 D 会生成比标准 GAN 更强大的判断。训练过程，算法描述如下： ![[Pasted image 20240416155902.png]] 主要源自 IWGAN，但使用相对论平均 GAN 的损失函数。 大多数符号遵循方程 (3)。 主要区别在于第 9 行和第 15 行中的损失函数。 这些损失函数取决于相对论判别器，估计一种类型密码的批评者相对于相反类型密码的平均批评者。 这种直接比较使 G 能够快速收敛到最佳点并产生高质量的假密码。 此外，由于我们基于 RNN 的 GAN 采用​​ IWGAN，因此我们将梯度惩罚添加到判别器的损失函数中。 此惩罚迫使 ^x 的梯度 2-范数小于 1，其中 ^x 是点对 (xr, xf) 之间直线上的随机样本。 这为 GAN 的训练提供了极大的稳定性。 另一个重要因素是从第 13 行到第 16 行优化 G 的迭代。尽管大多数 GAN 都有一个在给定 G 上优化 D 的循环，但这不足以最大化 GAN 的性能。 因此，我们添加了训练 G 的循环来稳定和增强我们的 GAN。 根据我们在第 5 节中描述的实验，该因素对 REDPACK 的破解性能具有关键影响。 一般情况下，一旦模型被优化，生成器就被用作生成模型。 相反，我们利用鉴别器作为 REDPACK 的真实密码估计器。 密码生成器选择流程图：![[Pasted image 20240416171606.png]] 多个候选密码生成器，比如：Hashcat、PCFG、rPassGAN 与 WGAN-GP 以及 rPassGAN 与 RaGAN-GP。 注意：不同损失函数的 rPassGAN 可以作为不同的 生成器使用，因为它们有不同的密码破解结果。 注意：单鉴别器 rPassGAN 和双鉴别器 rPassGAN 也是。 每个候选密码生成器生成的 10 亿个候选密码从字符串转换为张量，传递给鉴别器（D），估计密码真实性。 由最大概率选择器选择最高的密码，转换为 字符串形式。 这些候选者被传输给 Hashcat 破解。 评估实验数据准备 大多数之前的研究使用的是 Rockyou 和 Linkedln。 但是本文在性能测试中使用到了包含长密码和 4 级密码的额外密码字典。所以使用 Melicher 等人的密码分类。 [27]。总共有七个训练和破解数据集： Rockyou 和 LinkedIn 包含一些 4 级密码。 我们还使用了来自 Hashes.org 的四个破解密码字典，其中提供了多个破解和泄露的明文密码。 ![[Pasted image 20240416194910.png]] 超参数配置 G:D：RaSGAN 通常使用 1:1 或者 1:10 批次大小： 通常使用 128。使用 64 的批量大小可以来应对训练不稳定。 epoch具体如下图：![[Pasted image 20240416201514.png]] 实验环境Tensorflow-gpu 1.10.1 和 Python 版本 3.5.4 进行 GPU 计算。 所有实验均在韩国大学NMLab的OpenHPC系统上进行。 发达。 OpenHPC的每个节点运行在具有32GB内存的CentOS 7服务器上； 这些节点使用 Intel Xeon E5 2.20GHz CPU(x2) 和 Nvidia TitanXP 12GB GPU(x4)。 实验过程 根据我们之前的研究[8,12]推断，决定破解性能的主要因素是epoch、G&#x2F;D比和循环神经网络（RNN）细胞类型。 如果 Epoch 过大，会导致过拟合。如图 4a 所示。 提高破解性能，需要在模型中应用较大范围的 G&#x2F;D 比，但是比较费时。从而退一步： 因此，我们对RaSGAN-GP成本函数使用1:1和1:10的G&#x2F;D比进行了实验，如图4b所示。 虽然两者都可能不是最优值，但它们足以说明模型的有效性。 最后要考虑的因素是神经单元类型。对于基于 RNN 的相对论判别器，有必要确定长短期记忆 (LSTM) [28] 和门控循环单元 (GRU) [33] 之间哪个更好。![[Pasted image 20240416202041.png]]结论：整个实验中，200k 训练周期、1:10 G&#x2F;D 比率和 GRU 单元类型被确定为密码破解的最佳设置并确保通用性，如图 5 所示。![[Pasted image 20240416202513.png]] 实验结果所有实验，我们的结果都优于其他模型。 REDPACK 在数据集 2 到数据集 7 上的破解性能比任何单一密码猜测模型高出 5-20%。 在数据集1（短长密码）的情况下，PCFG在早期破解阶段表现出了很强的性能。 下半程的破解，REDPACK最终反超了PCFG。 然而，随着密码长度变长（从 dataset3 到 dataset7），REDPACK 的性能明显优于 PCFG 和其他密码候选生成器。 也就是说，对于复杂且长的密码，REDPACK的有效性得到了明显的体现。 对于数据集 6 和 7，由于训练数据量较小，我们的团队无法创建 10 亿个 Hashcat 候选者。 在表3中，最大概率选择器从图3中选择的每个模型的候选数量与单个模型的密码破解性能成正比。 这个结果表明REDPACK不是随机选择候选者而是选择性地选择候选者。 此外，这意味着 REDPACK 的鉴别器可以正确评估生成真实密码的概率。 REDPACK 的限制 选择了更现实的密码候选者。 然而，更现实的密码候选选择并不总是能保证有效的密码破解。将候选密码的数量压缩了，但它也遗漏了一些对密码破解可能很重要的候选密码。 将 OMEN 作为密码候选生成器组件时EDPACK 的候选字典的破解性能恶化。 原因： OMEN 和 PCFG 都有相似的特性。 它们都以高阶概率生成候选密码（本身就筛选了一些密码） 对 OMEN 和 PCFG 中的密码候选集应用随机洗牌。 这种简单的方法不能完全消除破解性能的损失。 基于三种不同方法（Hashcat：基于规则、PCFG：基于概率、rPassGAN：基于深度学习）的模型用作生成器时，REDPACK 在我们的实验中效率最高。 后续工作自定义了一套规则集。 base64 是 hashcat 最高效的规则集。 搜集：将7条Hashcat规则（best64、dive、specific、generate、InsidePro-PasswordPro、Incisive-leetspeak、T0X1Cv1）组合成一个巨大的规则文件。 测试：每条规则记录有助于密码破解的机会数量。 选择：对于REDPACKU4的自定义Hashcat规则集，选择了100条规则（与best64相同的数量）试验证明有效的。 "},{"date":"2024-04-22T13:14:48.842Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Pass_RNN/","categories":[["undefined",""]],"content":"摘要 密码攻击频繁 利用密码的相似性和可预测性之间的相关性。使用基于规则的方法，但将规则推导、分类和预测委托给循环神经网络 (RNN)。 试验结果：尝试的猜测次数限定为10次，结果 5 次就达到了 83% 准确率。其他模型的两倍 模型能有效进行有针对性的在线密码猜测，而不会恢复或者锁定。 引言 密码使用频繁，密码有规律性。 密码攻击频繁，使用之前网站泄漏的密码攻击同一用户的新密码。 密码攻击方式多样，字典，规则，模型。 基于旧的密码，规则能在很少的次数下，对新的网站达到很高的破解率。 本文使用基于规则的方法，使用神经网络自动执行猜测过程。 导出修改模式， 建立分类，神经网络方法 生成密码猜测，BiLSTM模型 本文建立了一个实验模型，可以在不知道猜测模式的情况下，进行预测。 使用基于字符 LSTM 编码器解码器模型 本文将 RNN 和 预训练Transformer 用于（研究较少的）密码短语猜测领域， 构建了一个具有注意力机制的双向 LSTM 模型，预测短语的模式（用少量短语生成完整的密码）， 比传统方法所需要的猜测次数明显减少。 密码猜测——针对性在线猜测数据集 来源：2018 ，6000，0000 个密码 处理： 数据集中每个用户至少两个密码，只选择两个构成密码对。 消除重复密码对，和多次出现的密码，剩余 1700，0000 唯一密码对。 两个指标来评价数据分布： 编辑距离是将一个字符串转换为另一个字符串所需的编辑（例如，替换、插入或删除）次数。数据集中，密码之间的编辑距离范围为 0 到 17，如图 1 所示。大多数密码对的编辑距离范围为 1 到 11。编辑距离可以帮助设置密码重用规则易于用户理解（例如，确保后续密码与原始密码相差三个字符）。 表现出最高相似度的密码将具有最小的 Levenshtein 距离和最高的 JaroWinkler 距离，并且将是跨站点猜测攻击的最佳候选者。 处理： 已知规则：在之前的研究中，确定了几种最常见的修改模式，包括子字符串、公共子字符串、大写、Leet 和顺序键（Wang 等人，2018 年；Walia 等人，2020 年）。我们根据这五种模式标记每个密码对，以创建一个标记数据集。最后，不符合任何这些规则的对将被丢弃。带标签的数据集包含 3,006,871 个唯一密码对。 工作集：唯一密码对中 长度在 5 - 17 的字符。 模型预测过程 目标：预测流程：对上面处理好的每一个数据：识别标签，分类，生成预测。 第一步，定义常见的修改模式，分析每个密码对并标记相应的类别。 在第二步中，我们使用神经网络模型将每个原始密码分配到单个修改类别中。这个过程称为单标签预测问题。 在第三步中，我们构建第二个模型来了解每个类别中可能的修改。该模型对测试数据的准确率达到 90%（），可以理解并生成每个类别的所有可能的修改。 然后组合两个模型，并在最后一步组装生成的管道。我们的方法可以仅将一个原始密码作为输入，将其分类为修改类别，并生成密码。 标记：同上面的处理过程。 分类：使用 Keras Python 库构建了一个 4 层分类器，用于自动密码修改类别预测。 输入层采用与数据集中最长密码相同长度的单个字符序列，即 17 个字符长。 One-Hot 编码器将每个密码处理为字符序列，并将这些序列转换为 One-Hot 数值数组。 编码被传递到 LSTM 单元。我们使用字符级双向 LSTM (BiLSTM) 层，它是传统 LSTM 的扩展，可以提高模型在序列分类问题上的性能。 BiLSTM 层在两个方向上运行输入，一个从过去到未来，另一个从未来到过去。与单向 LSTM 不同，BiLSTM 使用两个隐藏状态，可以保留过去和未来任意时间点的信息。由于这些特性，BiLSTM 可以更好地理解序列中每个字符周围的上下文（Xu 等人，2019）。 BiLSTM 单元的输出被馈送到密集的激活层。 激活层包含一个激活函数，它定义如何将输入的加权和转换为输出。为了确保模型学习特征并且不会过早收敛，我们使用具有较小学习率的 Adam 优化器（Kingma &amp; Ba，2014）。该优化器用于在每次训练迭代期间更新网络权重。 生成：训练字符级 BiLSTM 模型来生成每个修改类别内的密码。 输入层 1 将先前使用的字符序列形式的密码作为输入，而输入层 2 将密码修改类别作为输入。第一个流包括输入层 1、One-Hot 编码器和 BiLSTM 层。这三层的作用与第 3.2.2 节中描述的类似。第二个输入流包括输入层 #2 和重复向量层，这两个层都用于在单个类别内生成每个预测时向模型添加修改模式列表。最后，串联层组合两个流的输出，并将组合的输出馈送到激活层。激活层的作用与第 3.2.2 节中描述的相同。生成的模型可以为每个修改类别生成高度准确的密码猜测。我们最终采用了这种架构，因为它在超参数调整过程中产生了最佳性能。然而，添加更多 LSTM 层可能会导致模型过度拟合。 此外，单独实现了一个直接密码预测，在我们的模型中，它将确定输出修改后的密码。由于模型可以生成具有不同置信度的多个预测，因此我们需要一种算法来选择前十个最可能的输出。我们使用 Beam 搜索算法，这是序列到序列机器翻译问题中最广泛使用的算法之一（Yoo 等人，2020），来帮助我们识别最可能的预测。直接密码预测是最有前途的方法，因为它消除了不断的规则推导和分布分析的需要，并简化了数据集预处理。由此产生的模型，即直接预测机制 (DPM)，是独立于规则的，并提供高预测率。 实验结果在 Google Colab Pro（一个基于云的 Jupyter 笔记本环境）上运行了该项目的大部分内容。 托管运行时环境使用 Tesla P100-PCIE-16GB GPU、Intel(R) Xeon(R) CPU @ 2.20GHz 处理器、25GB RAM 和 109GB 磁盘空间。实验中对硬件要求最高的部分是数据集预处理、模型训练和直接密码预测。直接密码预测的计算成本最高。根据 Google Colab 的测量，在 40 万条记录上训练模型需要 13GB RAM，在 50 万条记录上训练模型需要 20GB。 短语猜测——针对性离线攻击第一种方法采用基于单词级注意力的 LSTM 模型，这与我们在第 3 节中开发的没有注意力机制的字符级 LSTM 模型不同。第二种方法利用 OpenAI 提供的生成式预训练 Transformer (GPT-2)，并且可供公众使用 Radfordetal。 （2019）。 数据集单词集语料库中的短语集（单词数量2 3 4）+ 频率生成的数据集包含 574,531 个短语，其中 99,953 个是不同的。 攻击向量 输入是一个短语，用于预测后续短语; 通过使用马尔可夫链模型建立基线来开始我们的实验。该模型将二元语法或三元语法的第一个单词作为输入，并使用马尔可夫算法生成短语的其余部分。 然后，该模型根据两个单词同时出现的统计概率构建一个转移矩阵。 结果，模型生成预测，我们将其与目标短语进行比较，看看它们是否匹配。 第一种——LSTM![[Pasted image 20240409191656.png]] 由编码器和解码器组成 编码器：获取输入序列并且将信息汇总为上下文向量。 输入层：第一个单词作为预测的起点。 嵌入层：使用 GloVe 模型（2014）进行词嵌入； BiLSTM层：压缩输入，降低向量维度。 上下文向量：所有编码器单元的输出作为输入来计算每个单字解码器想要生成的概率分布。它帮助解码器捕获有关输入的整体信息。 解码器：生成预测 输入层：上一步的向量； 注意力机制：多个注意力层 使用函数而不是使用单个隐藏状态来编码和解码 上下文向量将所有编码器单元的输出作为输入来计算每个单字解码器想要生成的概率分布 基于LSTM层。 使用 Adam 优化器和稀疏分类交叉熵作为损失函数。使用稀疏分类交叉熵的一个优点是更好的内存和计算资源利用率，因为它为每个类使用单个整数而不是整个向量。 第二种——GPT-2Transformer 长期以来一直用于短语自动完成（Vaswani 等人，2017）。我们使用 GPT-2，一种开源语言模型（Radford 等人，2019）。该模型的架构与纯解码器变压器非常相似。 由堆叠在一起的多个解码器块组成。 每个解码器块都有一个前馈神经网络和一个屏蔽自注意力层。 屏蔽自注意力层基于模型对相关和关联单词的理解以及处理该单词之前所需的上下文。该层分配定义片段中每个单词的相关性的分数，并将它们的向量表示相加（Vaswani 等人，2017）。 矢量表示被传递到完全连接的神经网络层进行处理。实验过程： 使用具有 124M 参数的 GPT-2 Small 模型版本。 使用的 4 元数据集样本中提取的单词列表上重新训练它。 还利用模型作者提供的一些超参数 前缀参数用于为模型提供短语的开头。 温度参数是控制随机性的浮点数。较低的温度使预测更加重复。 长度控制生成的短语将包含多少个单词。 参数 top_p 有助于缩小预测范围以选择最佳候选者。 我们生成的短语数量由 N_samples 参数控制，设置为 50。 结果不幸的是，大多数模型需要进行大量尝试才能正确猜测密码。请注意，现有工作的大多数作者没有透露他们实验的每个细节和参数。此外，在现有模型和我们的模型之间进行公平的一对一比较非常困难，因为它们经常使用并不总是提供的不同数据集。 LSTM 模型随时间变化的预测率。该模型在大约 1000 次尝试中破解了 24% 的密码。经过大约 5000 次尝试，它最终达到了 40% 的最大预测率。然后预测率开始趋于平缓。 除了我们开发的 LSTM 和 GPT-2 模型之外，我们还实现了基于马尔可夫的模型作为基线 和 我们开发和配置的模型之间的详细比较。我们对基于马尔可夫的模型的实现并未针对有效的 GPU 和资源利用进行优化，并且在大约 20 次尝试后停止收敛。那时，我们无法记录预测率。如果我们将尝试次数设置为高于 20，模型就会开始超时。 对于 LSTM，我们发现尝试次数越多，资源就会成比例增加；然而，总体利用率仍然较低。 GPT-2 模型在预测较长短语方面表现最佳，但需要更多输入才能更快地提供准确的猜测。 即使对于不太频繁出现的短语（通常更难破解），LSTM 也表现出了出色的预测率。 该实验断言，对于基于马尔可夫的模型和 LSTM，较长的短语和具有三个或更多单词的短语更难预测。我们所有的模型都显着增加了正确预测的数量，同时减少了尝试的次数。它表明我们的模型在破解密码方面非常有效。 未来展望密码的见解原始密码进行轻微修改在跨站点攻击中几乎没有提供额外的安全性。姐妹密码之间的相似度越低，用户数据就越安全。 因此构建一个主动密码检查器 防止用户选择易于猜测的后续密码，特别是当它与使用的密码相似时。服务提供商应考虑单独使用密码，转而使用双因素身份验证、生物识别、行为身份验证和其他替代手段 密码短语的见解 易于在企业级别进行实施和维护，对用户程序友好。 更长，更容易记住。 本质上是不安全的。构成密码的短语通常来自于主流文化和 容易被人工智能方式攻击。 未来展望 更多密码泄漏 原始密码生成后续密码，可以用新数据训练旧模型。 总结在本研究中，我们首先研究了有针对性的在线密码猜测问题。,我们使用 RNN 构建了一个密码预测管道来自动进行密码分类和生成。,预测结果优于传统的分类和猜测算法。,当我们将猜测尝试限制为五次时，性能提升尤其显着。,我们结合了对基于规则的预测算法的理解和 LSTM 神经网络的强大功能，解决了同一用户创建的密码的跨站点预测问题。,这是一种相对较新的方法，也许是使用 RNN 来完成此特定任务的首次尝试之一。,我们可以量化后续密码的相似性、修改模式和可预测性之间的相关性。,此外，我们还展示了最常见的修改策略的易于预测性和高精度，例如在原始密码或大写中添加头或尾符号。,我们展示了由于 RNN 模型的低复杂性和浅层性质，可以通过负担得起的硬件或在线计算资源（例如 Google Colab）来促进这种预测过程。,此外，预测效率使其可以在帐户被锁定之前允许尝试五次或更少的平台上运行。,我们还讨论了在线服务应采取的具体步骤，以提高身份验证过程的安全性。,然后我们探讨了使用 RNN 进行有针对性的离线密码猜测的问题。,我们构建了一个基于注意力的 LSTM 模型和一个微调的 GPT-2 模型来预测常用密码。,我们分析了结果，并将其与最常用的方法进行了比较，例如基于字典、基于规则和基于马尔可夫链的预测算法。,我们实现了明显更好的预测率，特别是考虑到使用的尝试次数、部署时间、资源利用率和操作简便性。,尽管这些方法并不是全新的，但它们在密码短语预测领域是新颖的，并且产生有竞争力的预测率。,资金 没有收到任何资金来协助准备本手稿。"},{"date":"2024-04-22T13:14:48.841Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/PassGPT/","categories":[["undefined",""]],"content":"摘要 LLM 无需显式监督： PassGPT 引导密码生成：利用 PassGPT 来生成匹配任意约束的密码。 引言贡献： 密码猜测 密码强度估计PassGPT 多猜测 20% 的未见过的密码，并且对新的泄露展现了良好的泛化能力。此外，我们通过矢量量化增强了 PassGPT [55]。由此产生的架构是 PassVQT，它可以增加生成密码的复杂性。 与之前整体生成密码的深度生成模型不同，PassGPT 对每个字符进行顺序采样，从而引入了引导密码生成的新颖任务。此方法确保对搜索空间进行更细粒度（字符级）的引导探索，其中根据任意约束对生成的密码进行采样。 最后，与 GAN 相比，PassGPT 提供了密码概率分布的明确表示。我们证明密码概率与最先进的密码强度估计器一致：PassGPT 为更强的密码分配较低的概率。 总结如下：– 我们引入了 PassGPT，一种自回归转换器，它在密码生成和对未见过的数据集的泛化方面获得了最先进的结果。– 我们展示 PassGPT 如何在任意约束下启用一种新颖的密码生成方法：引导密码生成。– 我们检查 PassGPT 下的密码概率以及它们如何与强度保持一致。我们讨论如何使用该指标来改进当前强度估计器。 – 我们提出了 PassVQT，这是一种通过矢量量化增强的类似架构，以增加生成的复杂性。 实验设置数据集![[Pasted image 20240319110425.png]] 最主要的是 RockYou 和 LinkedIn 对于 RockYou，我们分别获取最多 10 个和 16 个字符的所有密码列表。 我们将此列表中的 80% 作为训练数据。 在剩下的 20% 中，我们将训练分割中未包含的所有密码保留为测试数据，仅保留频率较低的密码。 测试集中最常用的密码仅出现了7 次 平均频率为 1.03 有助于测试生成低概率密码的能力。 对于 LinkedIn LinkedIn 泄露事件没有提供有关密码频率的信息，因此我们将 80% 作为训练数据，其余 20% 用于评估。同样保证没有相同的数据 同时 出现在两组中 评估集中删除 RockYou 训练密码来定义交叉评估测试集，反之亦然，以评估对未见分布的泛化。 最后，我们还将 MySpace、phpBB 和 Hotmail [52] 泄漏视为评估集。我们执行类似的交叉评估程序，从所有数据中删除 RockYou 和 LinkedIn 训练数据。 OST 10 OST。 模型自回归生成模型。PassGPT 和PassVQT 对密码中某个字符的出现概率进行建模，给定前面的值，对密码中某个字符的概率进行建模，给定前面的值，从分布中顺序采样可以生成可能的密码。 模型在词汇表 Σ 上运行，包含 256 个 UTF-8 字符。 tokenizer 定义为将词汇表中的每个字符 σ 映射为整数的函数，分词器：Σ 7→ [0, |Σ| − 1]。 然后，在 tokenizer 函数下使用其图像的 one-hot 编码为每个 token σ 创建向量表示。这会产生一个维度向量 |Σ|，在标记器 (σ) 位置处所有条目均等于 0，单个条目等于 1。 PassGPT![[Pasted image 20240319150308.png]]GPT 模型利用 Transformer 的解码器组件，并经过训练以自回归方式预测序列中的下一个标记。 为了预测密码中的特定字符 xi，转换器解码器仅考虑之前的字符 x0,…。 。 。 ，xi−1 作为输入， 并输出一个维度为 d 的潜在向量（在我们的工作中 d &#x3D; 768）。 然后这个潜在向量被映射到维度 |Σ| 的实数向量通过线性层并使用 softmax 函数进一步转换为词汇表上的概率分布。 词汇表上的输出分布表示 p(xi|x&lt;i; θ)。使用相对于在该位置找到的真实字符的独热编码表示的交叉熵损失来优化该分布。一旦网络经过训练，它就会为我们提供基于先前标记的词汇表的参数化分布，即 p(xi|x&lt;i; θ)。 我们可以从密码开始标记 &lt;\\s&gt; 开始，并找到 p(x1|x0 &#x3D; &lt;\\s&gt;)。这为我们词汇表中的每个字符分配了成为密码中第一个标记的概率。如果我们从这个分布中采样，我们可以固定第一个字符并通过计算 p(x2|x0, x1) 重复该过程来找到第二个字符。当在任何给定步骤从分布中采样到密码结束标记 &lt;/s&gt; 时，密码采样过程即告完成。 与训练不同，此过程是连续的。我们的 PassGPT 实现使用 HuggingFace 库 [53]，并具有以下规范：12 个注意力头、8 个解码器层和 GeLU 激活 [24]。此外，我们使用 AdamW 优化器对所有模型进行 1 轮训练，起始学习率为 5e-5，训练期间线性衰减。 PassVQT使用潜在空间的矢量量化增强了转换器架构。在计算机视觉领域，这已被证明可以提高样本质量[55]。 PassVQT 遵循 Yu 等人设计的架构。 [55]。 在对与 PassGPT 相同的条件分布进行建模时，我们的目标是评估量化是否可以提供任何额外的好处。 在图 2 所示的架构中，变压器编码器将每个输入标记映射到维度为 768 的潜在表示。 然后使用线性层将该潜在表示映射到 10 维，并使用 k 均值和具有 N 的码本进行量化条目。量化的 10 维向量通过线性层映射回 768 维，并作为变压器自回归解码器的输入。 该解码器经过训练，可以仅使用先前标记的量化表示来逐个字符地重建输入密码。 我们通过最小化 RockYou 泄漏训练分割的重建损失来进行超参数搜索。 我们的研究结果表明，更深的编码器和解码器结构可以提供更好的结果，码本大小为 300 可以提供最佳性能。 PassVQT 采用变压器编码器和 GPT-2 解码器，分别具有 12 个注意力头和 8 个层。它是使用 HuggingFace 库 [53] 实现的，并使用 AdamW 优化器进行端到端训练，起始学习率为 5e-5，具有线性衰减。一旦编码器-解码器网络收敛，该模型就可以从压缩的量化潜在表示重建输入密码。如果我们对潜在代码的分布进行建模，我们可以从中采样以生成可能的代码序列，然后解码器可以将其转换为可能的密码。为此，我们在训练数据集的量化表示上训练自回归代码模型。在推理过程中，我们通过从代码模型中采样代码序列并使用原始解码器将它们转换为密码来创建新密码。不再需要编码器。 4.3 引导式生成我们提出了一种新颖的密码生成方法：引导式密码生成。与之前生成整体密码的深度生成方法不同，PassGPT 分别对每个令牌进行建模，从而授予对每个字符的完全控制权。这使得生成过程能够满足特定的约束。这些限制的一些示例包括：密码长度、固定字符（例如，第一个位置的“a”）和模板（例如，四个小写字母和两个数字）。这可以通过限制采样分布 p(xi|x0, · · ·, xi−1) 以仅考虑分配给感兴趣子集 Σ′ ⊂ Σ 的概率质量来实现；例如，限制 Σ′。 实验结果将 PassGPT 和 PassVQT 和最新的深度生成模型，并演示了它们对不同数据集的泛化，而无需进一步训练。检查了 PassGPT 下密码的概率和熵，以深入了解其功能和建模分布。 训练集的两种变体：（1）唯一密码和（2）所有出现的密码。 在使用唯一密码进行训练时，PassGPT 表现出卓越的泛化能力， 相反，在使用唯一条目进行训练时，PassVQT 会遇到生成分发内密码的困难，但在合并其绝对出现次数后会显着改进。 从 PassGPT（针对唯一密码进行训练）和 PassVQT（针对所有密码进行训练）中对越来越大的密码猜测池进行采样，对越来越大的密码猜测池进行采样，并计算它们恢复的 RockYou 测试分割的百分比。 结果表明 PassGPT 优于所有其他模型。它在 109 个猜测中恢复了测试集的 41.9%，而最先进的 GAN 模型的匹配率为 23.33%。 密码生成评估的另一个重要因素是生成新颖且独特样本的能力。 在 109 次猜测中，PassGPT 保留唯一密码的比例最高 (60%)，而 PassVQT 则下降至 20%。由于 PassVQT 是针对所有出现的密码进行训练的，因此在其发行版下 测试集的恢复比例![[Pasted image 20240327105738.png]] 长密码效率：PassGPT 在对独特样本进行训练时表现最佳，PassVQT 现在在使用唯一密码进行训练时获得了更好的性能。在对这个新分布进行训练后，模型保持了相似的准确性。从 108 个猜测中，PassGPT 和 PassVQT 分别恢复了测试集的 15.5% 和 8.57%，而在 10 个字符设置中则为 19.37% 和 10.30%。 模型对为见过密码分布的泛化能力：。分别用 RockYou 和 LinkedIn 的数据集进行训练，然后都用 LinkedIn 的测试集测试。![[Pasted image 20240327142047.png]] 用 Rockyou 训练的模型 泛化效果更好![[Pasted image 20240327111524.png]]"},{"date":"2024-04-22T13:14:48.840Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/PassGAN/","categories":[["undefined",""]],"content":"关键点 第一个使用 GANs 生成密码 摘要已有工作 HashCat 和 John the Ripper 很成熟，但是还是需要扩展来进一步建模密码；PassGAN 使用的是 GAN；PassGAN 可以提取相当数量的密码属性，而其他工具没有； 引言密码常见；用户使用的密码简单；导致密码猜测很有效；密码猜测的效率由匹配大量可能密码和特定密码哈希的速度有关；（可能的密码不是穷尽所有组合，而是有一定的候选密码）最先进的工具在密码猜测时使用一些启发式算法可以构造许多新的候选密码。但是，这些启发式的变换规则基于直觉，而不是基于大型密码数据集的分析。 因此，生成的候选密码覆盖范围小并且这些启发式的变换规则的开发和测试是很耗时的事情；总结：当前方法的伸缩较差。 我们的方法核心：训练一个模型自动决定密码的特征和结构，并利用这些知识生成新的候选密码；同时神经网络的训练必须要任何先验知识和属性的假定。比较： 马尔可夫模型：隐含地假设所有相关的密码特征都可以用n元语法定义； 基于规则：只能猜测与可用规则匹配的密码；结果： 神经网络生成的密码覆盖范围很广GANs：在高维空间中进行密度估计 通过训练一个深度神经网络架构来执行隐式生成建模，该架构会输入一个简单的随机分布（如高斯分布或均匀分布），并生成遵循可用数据分布的样本。 在某种程度上，他们用深度神经网络隐含地模拟了累积分布的逆模型，即 x &#x3D; F-1 θ (s)，其中 s 是一个均匀分布的随机变量。 训练过程：GAN 使用了一种猫捉老鼠的游戏 其中深度生成网络（G）试图模仿样本的底层分布， 而判别型深度神经网络（D）则试图区分原始训练样本（即 “真样本”）和由 G 生成的样本（即 “假样本”）。 这种对抗性程序迫使 D 泄露训练数据的相关信息，这些信息有助于 G 充分再现原始数据的分布 贡献 预测率高 输出质量高于规则 PassGAN 可以输出几乎没有限制的密码猜测数 实验中 PassGAN猜出来的密码最多，但是生成的候选密码也最多 与基于深度神经网络的密码猜测算法相比有竞争力。 PassGAN可以用于有效增强密码生成规则 缺点： 分析：有缺点 优势(表现力、通用性和从样本中自主学习的能力) 和 输出大小方面的成本 之间存在权衡。 基于规则的密码猜测工具可以在极少的尝试次数内生成大量的匹配结果，而 PassGAN 则必须输出更多的密码才能达到同样的效果。 分析：不是问题 密码猜测工具可以很容易地组合在一起，先 hashcat 不行再 PassGAN 存储不是问题，可以离线生成。 结论：用匹配个数作为主要指标是有意义的，而不是匹配生成的速度 背景知识 GAN 当前工具，hashcat | JTR 基于字典 基于规则 基于马尔可夫模型： HC解释JTR解释 神经网络的使用（注意点在密码强度估计） 使用 模型：使用改进型 Wasserstein GANs（IWGAN）训练。 优化器：ADAM 优化器 参数： 批次大小： 迭代次数：在每次迭代中，GAN 会运行一次生成器迭代和一次或多次判别器迭代。 每次生成器迭代的判别器迭代次数：表示判别器在每次 GAN 迭代中执行的迭代次数。每次生成器迭代的判别器迭代次数设为 10，这是 IWGAN 使用的默认值。 模型维度：表示每个卷积层的维数。我们尝试在生成器和鉴别器中使用 5 个残差层，两个深度神经网络中的每个层都有 128 个维度。 梯度惩罚系数 (λ)：它规定了应用于判别器相对于输入的梯度准则的惩罚。增加该参数可使 GAN 的训练更加稳定。在实验中，我们将梯度惩罚值设为 10。 输出序列长度，表示生成器 (G) 生成的字符串的最大长度。我们将 GAN 生成的序列长度从 32 个字符（IWGAN 的默认长度）修改为 10 个字符，以符合训练过程中使用的密码最大长度。 输入噪声矢量（种子）的大小：它决定了正态分布中的随机数有多少个作为 G 的输入，以生成样本。我们将这一大小设置为 128 个浮点数。 最大示例数：表示要加载的训练项（PassGAN 中为密码）的最大数量。GAN 加载的最大示例数设置为整个训练数据集的大小。 ADAM 优化器的超参数： 学习率，即调整模型权重的速度 系数 β1，表示梯度运行平均值的衰减率。 系数 β2，表示梯度平方运行平均值的衰减速度。 ADAM 优化器的系数 β1 和 β2 分别设置为 0.5 和 0.9，学习率为$$ 10^{-4}$$这些参数是 Gulrajani 等人使用的默认值。 整体结构： 残差块结构 ![[Pasted image 20240222172953.png]] 生成器和判别器各有五个残差块 ![[Pasted image 20240222173025.png]] 运行环境 软件 IWGAN 的 TensorFlow 版本【代码】 TensorFlow 1.2.1 Python 2.7.12 Ubuntu 16.04.2 LTS 硬件： 64GB 内存 12 核 2.0 GHz 英特尔至强 CPU 英伟达™（NVIDIA®）GeForce GTX 1080 Ti GPU（11GB 全局内存）。 IWGAN 寻常在训练深度神经网络时，最初的训练误差会随着层数的增加而减小。然而，在达到一定层数后，训练误差又开始增加。 但是 ResNet 包含层与层之间的 “快捷连接”。这可以看作是对这些层的封装，并以标识函数（图 1 中表示为残差块）的形式实现。通过使用多个连续的残差块，ResNet 可以随着层数的增加不断减少训练误差。 PassGAN 中的残差区块由两个一维卷积层组成，并通过整流线性单元（ReLU）激活函数相互连接，如图 1 所示。区块的输入是标识函数，并与 0.3 个卷积层的输出相加，产生区块的输出。 训练数据 RockYou 数据集：选择长度为10，并且选择 80% 作为训练集，20% 作为测试集 LinkedIn 数据集：数据是散列形式的，基于规则的系统具有潜在的优势。 测试方向 相同的密码分布上训练和测试时，PassGAN 的预测效果如果 PassGAN 在不同数据集的预测效果 比较 用 PassGAN 的训练集作为HashCat Best64、HashCat gen2、JTR Spiderlab 规则、马尔可夫模型、PCFG 和 FLA 的输入数据集 按频率降序排序的密码实例化了 HashCat 和 JTR 的规则 HashCat Best64 生成了 754,315,842 个密码，其中 361,728,683 个密码是唯一的，长度不超过 10 个字符。请注意，这是 Best64 规则集在给定输入集（即 RockYou 训练集）上生成的最大样本数。 对于 HashCat gen2 和 JTR SpiderLab，我们从它们的输出中统一抽取了一个大小为 10的9 的随机子集。该子集由长度不超过 10 个字符的密码组成。 对于 FLA，我们根据中提供的说明设置了 [44] 中的代码。我们训练了一个包含 2 个隐藏层和 1 个大小为 512 的密集层的模型（全部参数列表见附录 A 表 6）。为了与其他工具保持一致，我们没有对训练集进行任何转换（例如删除符号或将所有字符转换为小写）。训练完成后，FLA 会枚举其输出空间的一个子集，该子集由概率阈值 p 定义：当且仅当一个密码的估计概率至少为 p 时，该密码才属于 FLA 的输出。这样，长度为 10 个字符或以下的密码总数达到了 747,542,984 个。在评估中使用这些密码之前，我们按照概率从大到小进行了排序。 我们使用 3-gram Markov 模型生成了 494,369,794 个长度不超过 10 的唯一密码。我们使用该模型的标准配置运行了该模型[18]。 我们使用韦尔等人[91]的 PCFG 实现生成了 109 个长度在 10 或以下的唯一密码 评估 首先评估了 PassGAN 的输出所生成的匹配数 然后将其与 FLA、马尔可夫模型的一种流行的 3-gram 实现[18]、PCFGs [91]以及 JTR 的密码生成规则（SpiderLab 混淆规则[82]）和 HashCat 的密码生成规则（Best64 和 gen2 规则[29]）进行了比较，（这些工具都在本文的训练数据集上做了多年的优化）。 结合 HashCat Best64 对 PassGAN 进行评估的实验结果。最后，我们从概率密度和密码分布的角度对 PassGAN 和 FLA 进行了比较。 如何避免过拟合： 比较不同迭代次数时的生成匹配个数，个数开始下降说明即将过拟合 如何 PassGAN 生成密码的质量 唯一个数 生成的密码和其他数据集的匹配个数 结果： 可以生成不少于其他工具的匹配个数的密码 且总的密码和其他工具相差在一个数量级之内 猜测不同于训练数据集的密码时，比基于规则的密码生成更有优势。 结合 PassGAN 和 HashCat 首先删除训练集中所有匹配HashCat Best64 的密码 实验结果表明结合后可以匹配更多密码， Hashcat 的下新版本的 “slow candidates” 支持了这重结合 比较 PassGAN 和 FLA FLA 是一个概率估计模型，本质是一个参数估计模型，会输出密码以及其概率估计 FLA 受马尔可夫过程限制，变异性与 n-gram 范围有关，而 PassGAN 的变异性更高 次数较少时，FLA 的概率模拟不准确，次数较高，两者差不多 结论 PassGAN 很有效，能从一个数据集中猜测另一个数据集的密码 基于规则的限制很大。 FLA 和 PassGAN 也差不多 总结 方法是革命性的，无需用户干预就可以生成密码，也无需用户分析密码库 PassGAN 可以输出相同的匹配数目，但是需要输出更多密码，这个成本可以忽略。 将 PassGAN 的生成模型换位条件 GAN 可以生成特定字符字段的密码 专业名词 名词 含义 leet speak 黑客文 "},{"date":"2024-04-22T13:14:48.839Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/PassDiff/","categories":[["undefined",""]],"content":"摘要 本文使用去噪扩散模型； 考虑到密码空间和文本空间的相似性，在输入阶段使用了字节级分词器，并通过修改源代码来优化采样过程。 进行特殊字符编码使得 PassDiff 可以处理可变长度的输入并获得可变长度输出，无需手动截断。 比 PassGAN 训练稳定，破解比例显著提高 去噪步骤增加，能进一步提高破解比例。 引言 传统的密码破解有三种：基于字典、基于概率统计模型、基于深度学习 基于字典：依赖字典和规则 基于概率统计：如马尔可夫和概率上下文无关文法（PCFG） 概率上下文无关文法：用标识+个数形式表示密码的结构，然后进行响应的填充。 基于深度学习：不需要预知识，也不会被限制范围 贡献 使用了扩散模型 使用了字节级别的标记器编码输入向量 通过调整代码优化了降噪过程， 背景知识以及相关工作GAN 生成模型 辨别模型 DDPM 构成：由两个马尔科夫链构成 前向链：扩散过程，添加噪声。 目的：经常用于将原始分布转为先验分布 反向链：降噪过程，转换噪声恢复数据。 目的：使用从深度神经网络学习的参数化转换内核来反转此过程 去噪过程分两步不断生成新的数据点：首先从先验分布中采样随机数据，然后执行原始采样。 数学过程 假设 x0 服从一个分布。 有一个向前传播转换链， 即 转换核心：q(xt|xt-1)，这个通常可以被自定义 计算得到了 x1，x2, x3,…. 的一个分布 DDPM 一般会在向前传播时把分布转换为一个初始分布。 扩散过程不断用噪音破坏输入数据，直到丢失所有特征。 新建数据点：从先验分布中收养一个随机噪音点，构造一个规则逆向马尔可夫链从而逐渐排除噪声。 从 xT 得到 x0 PassDiff 标准扩散模型主要用于图像生成 Diffusion-LM 改进 用 transform 学习噪音分布 添加文本嵌入步骤 添加舍入步骤 文本作为输入，将文本嵌入为一个以为整形向量—使用单词级别的 令牌嵌入 扩散过程中， 这个整形向量被 转为 一维 浮点向量， 去噪过程的最后：舍入步骤会将这个 一维 浮点向量 转为 一维 整形 向量 只能输出固定长度文本 单词级别的令牌嵌入器 将本文分割为单词， 嵌入频率高能捕获语义信息，但是容易导致嵌入范围过大 字节级别的令牌嵌入将文本识别为一系列的字节，连续嵌入单字节和多字节 单字节的语义信息很少 多字节是 单字节和 单词级别嵌入之间的妥协。 密码空间和 文本空间相似，语义信息 在密码的字符之间比在语料库中弱， 参考 Diffusion-LM 我们使用字符级别的嵌入器 我们发现降噪过程的生成密码长度一致，引入随机因子生成各种长度。 多字节嵌入没有比单字节有明显优势，为了简化，使用单字节。即 一个特殊字节和 95 个特殊字节。 特殊字节表示为0. 其余可见字节 按 ascill 顺序表 从 1 到 95 . 特殊字符的编码可以 处理可变长度的输入并且获取可变的输出，无序手动截断。 Passdiff使用最少的去噪步骤也能产生高质量的输出，建议 5-50，可以将采样速度提高十倍！ ![[Pasted image 20240229164854.png]] 实验配置 实验环境： DELL Precision 5820工作站，128G内存，10个CPU核心 NVIDIA GeForce RTX 3090显 Win10和Ubuntu 22.04下进行的 Win10上进行了PassGAN相关实验，使用rnnPassGAN的源代码[8,17]。与扩散模型相关的实验是 在Ubuntu 22.04上进行，源代码来自minimal-text-diffusion[16]。 数据集 12306 CSDN RockYou 去除密码长度超过15，或者有不可见字符的密码， 8：2 训练集和测试集 超参数 包括：训练轮次：批次大小：扩散步骤：去噪步骤，输入长度，输出长度，学习率 实验结果 CSDN和RockYou的破解率总体高于12306：重复率都高于12306 CSDN和RockYou数据集的破解率逐渐稳定在40个去噪步骤左右，超过了12306所需的去噪步骤：：训练不足和数据量较大。 增加数量 训练轮数和批量大小都可以提高破解率。 随着去噪步数的增加，破解率首先逐渐上升到最大值，然后趋于最佳. 去噪步骤和训练轮次增加，重复率增加 重复率越高，生成独一无二的相同长度密码的概率越低 比较 我们的PassDiff模型比PassGAN具有显着的优势。当生成108个密码时，我们的模型在12306、CSDN和RockYou数据集上的破解率分别比PassGAN高3.17%、6.33%和13.22%。 另外，这并不是我们在破解率方面最好的表现 但是，我们的模型与传统的密码猜测模型（例如PCFG）之间仍然存在一些差距。 总结本文通过优化扩散模型参数和增加训练轮数，提出了比 PassGAN 训练时间更短、训练过程更稳定的 PassDiff 模型。 但由于扩散模型采样速度较慢，导致本文采用的模型很难生成十亿级字典。扩散模型在构造密码猜测模型方面具有巨大潜力，理论上也能取得很大突破在未来的工作中，我们将进一步优化参数并调试其他参数，例如扩散步长和学习率。我们计划使用其他神经网络从噪声中学习以提高破解率。我们还将尝试其他编码方法，例如编码两个以上的字符并尝试将它们与PCFG结合。 专业名词"},{"date":"2024-04-22T13:14:48.838Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/PassBERT/","categories":[["undefined",""]],"content":"摘要sformer 的密码猜测框架。2. 使用预训练&#x2F;微调范例。3. 设计了三个特定场景下的密码猜测任务的微调方式： 1. 条件性口令猜测，在给定部分口令的情况下回复完整口令。 2. 针对、定向性口令猜测，利用特定用户的个人信息猜测其口令。 3. 基于规则的自适应密码猜测，其为单词(即，基本密码)选择自适应调整规则以生成经规则转换的候选密码。4. 最后我们提出了一种混合口令强度计量器来降低这三种攻击的风险。 引言 数据驱动模型(例如，马尔科夫[28，32])和基于规则的工具(例如，Hashcat[17])来有效地离线破解密码 现实世界猜测攻击：使用先验信息， 定向密码猜测 TPG。 条件密码猜测 CPG 基于自适应规则密码猜测 ARPG 双向转换器在自然语言处理领域受到了极大的关注[12，27，51]。由于能够捕获双向上下文信息和高度可转移性[51]，转换器在多语言任务(例如，文本分类[59]、语法校正[34])中是有效的。 有的密码猜测攻击，无论是一般的还是基于现实世界的，都可以概念化为近似密码(即文本)的概率分布的努力， 这表明它自然适合基于双向转换器的猜测框架。 现有有一些琐碎的工作（看了一下只是 神经网网络，不是特指 Transformer）效果不佳。 成功使用 Transformer 需要一些特定的设计，比如这里使用了序列标签，参考之前的序列到序列机制。 本文设计了一个基于字符级别的双向变化猜测框架。设计了特定方式的微调。取得了一定的效果。 本文测试了预训练的效果。预先训练的密码模型比预先训练的自然语言模型能够产生更好的猜测性能，这表明了预先训练在特定密码语料库上的有效性。 本文引入了混合密码强度计（HPSM）来降低三种攻击的风险。HPSM可以与密码泄漏检查[23，47]相结合来检测输入是否公开泄漏，一旦泄漏，输入就会遭受ARPG攻击。 相关工作一般密码猜测 基于数据驱动模型，基于密码数据对模型进行训练 基于规则猜测，由专家知识定制初始配置，转换初始密码到候选密码。 基于额外信息攻击 CPG：Pasquini等人[38，39]提出了一种基于Wasserstein自动编码器的最先进的CPG方法[48]。他们将最终的模型称为CWAE(上下文瓦瑟斯坦自动编码器)，它由将部分密码嵌入到潜在表示中的编码器和将部分密码的潜在表示转换为密码的解码器组成。 TPG：其中，Pal等人提出的Pass2Path。[35]2019年，是最新、最有效的定向竞猜模式。他们提出了凭据调整，以破解用户历史密码的变体(调整) ARPG：021年，Pasquini et al.[37]提出了第一个基于规则的自适应猜测框架ADAMS(Adaptive Dynamic Mgling Rules Attack)，该框架构建了一个卷积规则(CNN神经网络)建模，为每个单词选择自适应规则。 基准攻击模型我们使用最先进的CWAE、Pass2Path和ADAMS模型分别作为CPG、TPG和ARPG攻击的基线，因为这些模型与其他模型相比具有最高的猜测性能。例如，CWAE已被证明比基于PCFG、马尔可夫和神经网络的模型更好。因此，我们不会检查与其他型号的比较。 双向 Transformer基于自我注意机制(即将给定的标记与所有文本环境连接起来)提出的，由于能够捕获深层文本特征而在自然语言处理领域获得了广泛的应用。BERT(来自Transformers的双向编码器表示)[12，27]是一种流行的基于变压器的架构，已经在11个单独的NLP任务上取得了最先进的结果。BERT针对 MLM(掩蔽语言建模) 和 NSP(下一句预测) 两个目标进行预训练，以建立基于大量未标记Web语料库的预训练语言模型。 对于 MLM 目标，BERT 训练模型，使其能够预测屏蔽位置的正确标记。由于自然语言的特性，BERT很大程度上是为了预测屏蔽词而设计的。 NSP的目标是取一个句子对A和B，并预测B是否是A之后的实际下一个句子。Bert[12]提出了两种次级训练方法：微调和基于特征。 在微调方法中，所有参数都在下行任务期间更新。 而在基于特征的方法中，通过冻结一些普通的预训练层来从预训练的参数中提取固定的特征。 通常，微调方法会随着训练时间的增加而产生更好的结果[12]。在本文中，我们选择了所有参数都是可学习的微调方法。 预任务工作流攻击模型的工作流程是根据监督信号训练监督模型。我们将它们的监管信号总结在表1中。 具体来说，CPG的监管信号是部分密码及其完整密码。 CPG 旨在训练模型，使其在给定部分密码的情况下预测正确的密码。 CPG 的输出直接作为密码候选。 TPG的监督信号是通过动态规划算法计算出的最短编辑路径的密码（在[35]中实现）。编辑路径是一系列原子编辑操作（在我们的攻击设计中预定义）， ARPG的监督信号是具有命中规则（即规则集的子集）的单词，例如根据两个假设数据集之间的命中信息删除最后三个字符。 ARPG模型输出自适应规则，然后将其应用于单词以获得候选密码。自适应规则通常与单词更兼容，从而可以尽早产生命中。 ARPG 中使用的修改规则在 Hashcat 中定义。一般情况下，大多数重整规则可以是常见原子规则的组合（例如，“删除最后三个字符”是“删除最后一个字符”的三个原子规则的组合），并且可以自然地模拟应用多个规则的场景按顺序到基本词。 威胁模型假设攻击者可以选择预先训练的自然语言和密码特定参数（作为先验知识）或随机变量来初始化他们的攻击模型。 密码泄漏数据集数据集选择在实验中选择了先前作品[16、21、37、38、43、54、60]中使用的几个数据集。用于定向猜测的数据集是电子邮件，而用于非定向猜测攻击的数据集是明文密码。 密码预训练、CPG和ARPG使用由明文密码组成的数据集，Rockyou-2009、000Webhost、Neopets、Cit0day、Rockyou-2021： 对于TPG攻击，我们选择以下两个包含电子邮件的数据集，并总结表2中的基本信息：BreachCompilation (4iQ)Collection#1 数据处理账户加入：为了找到属于同一用户的密码列表，我们根据相同的电子邮件地址合并帐户（用户）[35] 数据集清理：我们采用常用的清理策略[16,35,37,60]来过滤掉原始数据集中的哈希密码、非ASCII密码和超过32个字符的异常长密码。 密码二向性顺序性：字符通常与其相邻字符更相关，这也可以是单向性的体现。聚合：相关字符的内部序列（例如“password123”中的“p@ssw0rd”和“123”；“mike199730”中的“199730”）有更多的连接线。捕获双向表示可以提供更好的候选密码，从而提高密码猜测效率!!!! ！ PassBERT预训练预训练模型主要捕获输入密码的上下文嵌入，这是密码中每个字符的具有上下文信息的高维表示。上下文嵌入是预训练层（即最后一个变压器块）的输出。 嵌入 密码标记为字符序列， 带有表示开头 ([CLS]) 和结尾 ([SEP]) 的附加符号，因为密码通常比句子短， 对比：与 BERT 不同，BERT 通常在 token 级别对文本进行 token 化，其 token 主要是单词，并将每个句子剪辑为 512 个 token。 我们考虑最大密码长度为32个字符，并考虑总共99个有效字符，包括95个ASCII字符（表示为Σ）和4个附加符号开始、结束、占位符和未知字符。 嵌入层： 其字符嵌入和位置嵌入 求和。将标记化输入转换为其输入嵌入。 删除了 BERT 中的句子嵌入 ![[Pasted image 20240314160458.png]] 数据处理 为了使用 MLM 目标预训练密码模型（即预测屏蔽位置后面的屏蔽字符），我们将训练集中的每个密码 (Dtraining) 预处理为部分密码的形式（表示为pivot）：关联完整的密码（表示为 pwd）。我们遵循 BERT [12] 中相同的掩码比例：我们随机选择密码中 15% 的字符，然后分别以 80%、10% 和 10% 的概率用掩码符号、随机字符和未更改字符替换所选字符 。随机且未改变的字符可以防止模型记住被屏蔽的字符。一个密码可以预处理到很多个pivot，本文设置了20个pivot。我们设置预训练任务，找到参数 θ 来最大化以下似然：![[Pasted image 20240314161946.png]] 数据集选择我们使用 Rockyou-2021 作为我们的预训练数据集，该数据集非常大。为了在训练时间和模型性能之间取得平衡，我们从 Rockyou-2021 中随机抽取了 6000 万个密码。 计算性能我们的工作是在一台配备 Nvidia GeForce RTX 2080 Ti 的 Ubuntu 20.04 机器上执行的，大约需要 2 天才能完成预训练。存储预训练模型需要 8.9 MB。 密码微调根据特定的攻击场景定制预训练模型。微调方法通常包括架构修改和模型重新训练。 架构修改期间， 通常修改预训练模型架构的任务特定层（表9中的全连接层和输出层），并保留预训练层，包括顶部输入层、嵌入层和几个 Transformer 块。 然后，我们使用特定的监督信号重新训练下游模型，以学习特定于任务的功能（例如密码规则兼容性）。（注意，重新训练过程中所有的参数都会改变：包括上面的所有层）。 用于现实世界攻击模型的 PassBERT介绍三种 微调。 三种预训练模型： 预训练的密码模型：PassBERT 预训练的 BERT 自然语言模型：Vanilla BERT Vanilla BERT 可以在字级、子字级和字符级对文本（即密码）进行标记，而我们仅根据词汇表将密码标记为字符序列。由于我们的 Vanilla BERT 的词汇表只有小写字母，因此我们扩展词汇表以涵盖 Σ 中的所有有效字符（通过替换未使用的标记） 随机变量模型：PassBERT 条件密码猜测任务：pivot（例如，“p ＊ ＊ ＊w0rd ＊ ＊ ＊ ”） ————&gt;恢复密码。建模：掩码语言模型任务，预测缺失字符的 pivot 的条件概率数据构建：使用与 CWAE [38, 39] 相同的策略创建： 我们以一定比例（即 50%）随机用表示缺失字符的屏蔽符号替换每个字符。 然后，我们只保留那些包含至少四个可观察字符和至少五个屏蔽符号的生成主元。 微调攻击设计：根据 Pivot 的创建机制调整 屏蔽机制。 增加掩码比例：只用掩码符号， （默认的屏蔽机制结果不佳）模型再训练：从中提取具有正确密码的有效枢轴以进行模型重新训练。评估：CPG 与 CWAE 具有相同的 评估 pivots 。 对一个 pivot 生成相同数量的候选密码。然后，比较匹配的个数。 按照匹配的个数分类这些 pivot 。区分不同频率的 pivot 的猜测性能评估集： Neopets 和 Cit0day 空间较大： WAE 仅为每个主元类别生成 30 个评估主元，可能会引起偏差。 我们从评估集中为每个类别总共提取了 120 个评估基准评估指标： 每个 pivot 的平均破解率。结果：很好预训练效果：用密码预训练的效果更好。改进原则：模型通过以递减的概率耗尽屏蔽位置中的字符来生成候选密码，而 CWAE 通过解码器将潜在表示（即高维空间中的点）转换为候选密码。很优计算性能：训练我们的 CPG 模型大约需要 2 天（8.9 MB），训练 CWAE 大约需要一天半的时间（4.4 MB） 定向的密码猜测目标：用历史密码生成其变体目标：对给定的密码训练模型输出编辑路径。 微调：攻击设计： Pass2path 基于 RNN 模型的序列机制将密码转换为编辑路径。Transformer 不支持，修改：使用序列标记机制来预测密码中每个字符的一个编辑操作。其中一系列编辑操作构成编辑路径。在序列标注机制中，每个字符位置只能输出一次编辑操作。 为了适应序列标记机制，我们预先定义了新的编辑操作如下：保留（keep）、删除（del）和替换（rep1，rep2）这里，替换涉及用一个（表示为rep1）或两个字符替换（表示为rep2）。 缺陷：无法捕获一些转换，比如：插入三个字符模型结构：![[Pasted image 20240314203049.png]] 修改特定于任务的层以学习密码中每个字符的编辑操作（例如，(op, str)）的概率。模型再训练：遵循 Pass2path [35]，我们在 BreachCompilation 中从同一用户中随机抽取 80% 的密码对，并在最小编辑距离不超过 4 时选择合格的密码对，最终得到 85,269,455 个密码对。 评估：评估设置： 和 Pass2pass 保证相同数据集训练对评估指标。我们选择特定用户泄露的密码之一作为输入，然后根据 TPG 模型推断其变体（限制为 1,000 次猜测）计算了 Ncracked Naccounts 给出不同猜测（即 10、100 和 1,000）的评估用途&#x2F;帐户之间的破解率实验结果：三个模型都高于 Pass2path 预训练效果：密码和自然语言模型在 TPG 中表现出边际收益。发现预训练的改进空间仍然很小，这表明预训练的作用较弱。这种现象是可以理解的，因为有针对性的攻击集中在个性化密码转换上，这与预训练模型中的全局密码分布关系不大。两个预训练模型生成的上下文嵌入在帮助理解密码转换方面几乎没有效果。 TPG 往往依赖于任务，主要根据特定攻击的数据集形成其模型参数。 改进原则：更关注本地角色信息。准确地说，我们的编辑操作二元组 (op, str) 可以比 Pass2path 中使用的三元组操作 (op, str,pos) 减少一维搜索空间，其中 pos 指密码中的位置。 看法：随着编辑距离的增加，破解率下降。 计算性能比较。训练 PassBERT (82 MB) 和 Pass2path (166 MB) 分别需要大约 13.6 和 42 小时。我们根据经验计算得出，两种模型都实现了相似的推理速度。 PassBERT 和 Pass2path 每秒可以推断 4.38 和 4.63 个密码对（pairs&#x2F;s）。 基于自适应规则的密码猜测目标：每个单词仅与选定的自适应修改规则关联以生成猜测。 [37]尝试使用默认变压器，但与 ADaM 相比没有取得实质性改进。本质：找出单词适应的规则。效果：输入单词和规则输出破解的可能性方式：构建分类模型， 微调捕获整个单词和密码级别的修饰规则之间的自适应关系，因为修饰规则最终适用于单词。监督信号：是在两个假设数据集上带有标记规则（即命中规则）的单词。模型架构： 修改了特定任务层，推断重整规则和具有焦点损失函数的基本单词之间的自适应概率。 输出层没有序列长度。模型再训练： 针对两个规则集训练了两个 APRG 模型 使用 2 亿个密码作为目标空间 使用 1000 万条数据作为初始空间 训练：Ri 是个向量，每个元素对应一个规则的匹配可能性 注意：0和1的比例极其不平衡，即超过95%的标签都是0。因此，我们应用与ADAM相同参数策略的焦点损失[25,37]来关注硬标签。 设置一个 阈值 来表示规则使用的条件。 评估评估设置：比较了静态和动态模型。专注于单词和规则之间的关系建模，即我们的目标是为单词选择更具适应性的规则，以在相同的猜测下获得更高的猜测效率。评估指标：动态策略模型的最终破解率作为评估指标。试验结果：动态模型和静态模型都高于 ADaMs Dynamic PassBERT 在这些模型中实现了最高的破解率，并且在四个实验中比 ADaM 平均提高了 4.86%。 静态 ARPG 的显着提前停止是因为每个单词仅与自适应规则关联，其大小小于标准基于规则的攻击中的所有规则。预训练效果： 密码预训练使 Transformer 的性能优于 ADaM 用默认 Transformer 基本没改进。所以这些个模型的改进没有效果。改进原则：密码预训练对于 ARPG 的提升有着重要的作用。见解：发现了容易收到攻击的破坏规则。计算性能比较： 我们两个小时： ADam 需要 十个小时 推理速度相似。 密码强度估计讨论密码双向性： 双向 Transformer 在文本特征提取方面比自动编码器 (CWAE)、RNN (Pass2path) 或 CNN (ADaMs) 等领先方法效果更好， 工作[22]已经设置了单向变压器，并显示了双向训练机制在一般猜测任务中的优越性。 密码具有双向性，对字符进行不同的权重。 预训练对于提高猜测效率也能起到重要作用。深度学习模型通常由通用预训练层和特定于任务的层组成，其中预训练层的上下文嵌入层可以为任务相关层提供更多上下文信息。预训练的合适场景猜测：密码训练和自然语言训练：只能为有针对性的攻击提供效益 "},{"date":"2024-04-22T13:14:48.837Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Pass%20improved%20Transformer/","categories":[["undefined",""]],"content":"摘要 分析了个人信息和密码之间的关联性。 实现了一个基于改进 Transformer 的密码猜测模型 数据预训练时引入信息权重。 使用改进的波束搜索算法来快速搜索排名考前的输出结果。 模型和使用模型结构与原始Transformer模型不同的是，我们在词嵌入层之后添加了信息权重层。信息权重层涉及一个权重矩阵，权重矩阵表示每个字符携带的用户信息，该矩阵的维度与词嵌入矩阵相同。信息权重矩阵中，同一输入位置对应的参数值相同。参数值是通过模型的训练来确定的。信息权重矩阵将添加到输入嵌入中。那么后续步骤就和原来的Transformer一样了。![[Pasted image 20240327152521.png]] 模型训练 输入数据处理：模型的输入是一批代表用户个人信息的数值向量。首先，这些输入通过嵌入层（embedding layer）被转换成字符嵌入（character embeddings），即将数值向量转换为能够代表这些信息的嵌入向量。 信息权重处理：接着，通过信息权重层（information weight layer），这些字符嵌入会根据用户信息的权重进行调整，以更准确地反映每部分用户信息的重要性。 位置编码：之后，这些调整后的字符嵌入通过位置编码层（positional encoding layer）来获得位置向量，这一步是为了让模型能够理解字符在序列中的位置关系。 自注意力机制：调整后的字符嵌入随后进入编码器（encoder）的自注意力层（self-attention layer）。在这一层中，模型计算字符之间的依赖关系，而这种计算忽略了字符之间的距离，即无论字符相隔多远，都能捕捉到它们之间的联系。 向前传播和解码：编码器的输出接着被用作前向神经网络（forward neural network）的输入。在编码器的最终层输出的基础上，将这些输出转换成一组键值对（Key, Value），然后这组键值对被传递给解码器（decoder），以帮助解码器获取输入序列的信息。 解码器的输入是一批代表用户密码的数字向量。解码器嵌入输入并添加每个字符的位置嵌入。与编码器不同，解码器中的自注意力层仅关注当前已确认的输出字符。解码器的输出是浮点数的向量列表。接下来，通过线性层将向量列表转换为向量。它称为对数向量。对数向量的每个元素对应一个字符，其值代表该字符的得分。最后，对数向量通过softmax层转换为每个字符出现的概率。 模型预测使用改进的束波搜索来快速找到排名考前的预测结果。 字符匹配算法优化存储每个生成密码的 key 和 value，使用hash 表来存储，key 值计算过程。Key &#x3D; (char2int(a)*M + char2int(b)*M )%p 评估数据集 收集了 一亿条密码。 筛选了19种个人信息的 31 个网站。 统计了密码格式：大多数是 单词+数字，少部分是 单词 和 数字 分析了密码和数字的关联性 邮箱 + 后缀 生日：年月日组合 姓名：拼音 KMP算法 进行 字字符串比配 结果：关联很大。经过统计，我们得出的结论是，用户密码与邮箱、生日、昵称、电话号码、姓氏、名字、登录网站的域名高度相关。其中，电子邮件相关密码占比最高，占比12.551%，手机号码相关密码占比5%，用户名​​相关密码占比2.617%，生日相关密码占比0.863%，密码相关密码占比最高。与网站域名相关的密码占0.173%，与名字相关的密码占0.794%，与姓氏相关的密码占0.002%。 选择了与密码相关性较大的七个个人信息来构成输入序列，分别是生日、电话号码、网站域名、电子邮件地址、昵称、姓氏和名字。 研究序列的长度：主要是长度为8 的密码，用 包含8 的长度训练，用包含8的密码测试，生成长度为8 的密码 .实验环境改进的 Transformer 是用 TensorFlow 实现的。我们使用 TensorFlow 版本 1.12.0 和 Python 版本 3.5.2。我们在 GPU 服务器上进行实验，该服务器具有 2 核 2.2GHz CPU、NVIDIA Tesla V100 GPU 和 16GB 全局内存。 对比实验三个序列到序列模型：1. LSTM 2. encoder-decoder 3. Transformer在相同的数据集中进行实验：x 轴表示测试集大小 ；y轴表示破解的数量，当生成的密码数量超过 10,000 个时，Transformer 的准确率将高于其他两个模型。我们的实验结果表明，Transformer 在基于用户信息的密码破解研究中表现最好。 实验结果时间优化匹配算法的优化（相同时间内匹配的次数更多） 模型优化信息权重和无信息权重（），束波搜索和随机搜索 密码长度五个不同密码长度： 8 9 10 11 12密码长度越长，相同的猜测次数下越难猜出密码。 总结 分析了用户个人信息和密码之间的关联性，邮箱关联度很高 提出了一种基于改经的 Transformer 模型，数据预处理中引入了信息权重 波束搜索算法， "},{"date":"2024-04-22T13:14:48.835Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GuessFuse/","categories":[["undefined",""]],"content":"多视图混合密码猜测 摘要 集合多种模型进行密码猜测（也称为混合密码猜测），从而更好地捕获真实破解者的破解能力 问题提出：为什么整合模型会有效果，如何整合模型还没有研究， 本文从多视图学习的概念中汲取灵感： 将各种密码猜测模型生成的猜测列表视为数据的多个视图。 通过对这些密码猜测列表的分析发现了混合密码猜测模型能增强破解能力的原因： 集成更多元的视图可以覆盖更广泛的异构密码特征。 并提供更详细的有效密码信息分布。 提出了一种新的密码猜测框架：GuessFuse 采用多视图子集提取模块和分段分割选择模块，从多个猜测列表中准确提取和重组有效密码。 在 六个大型数据集中证明了 GuessFuse 的有效性。 通过组合两个（或五个）猜测列表，GuessFuse 在 107 个猜测中比其最重要的同行平均高出 11.00% ∼ 59.62%（或 4.70% ∼ 17.66%） 引言 密码使用频繁 密码破解器的性能是密码强度估计器的一个设计标准。 密码猜测分类：数据驱动和基于规则。当前主要是数据驱动方法效果更好。 PCFG [6] 对具有简单结构的密码（例如，passsword123）产生更好的猜测性能 马尔可夫模型 [22] 对于具有上下文相关特征的密码（例如，1qaz2wsx）更有效。 但是现实世界的破解专家能力大于单一的模型能力。 混合密码猜测的研究在初级阶段。 [16]通过并行应用多个密码猜测模型，设计了一个 Minauto 指标作为混合密码猜测的上限。 一些研究[29]-[33]试图在结构层面整合特定模型的优点（例如，分别在密码结构层面和字符串层面应用PCFG模型和马尔可夫模型进行密码猜测[29]） 。 另一方面，一些研究[32]、[34]实验性地组合来自不同密码猜测模型的多个猜测列表，以实现更准确的密码猜测。 然而，上述方法要么集成了两种特定的密码猜测模型，要么简单地组合猜测列表，而不是从机制上解决这个问题。未能有效利用多种异构密码猜测模型的优势。 更重要的是，据我们所知，为什么集成多个密码猜测模型可以增强破解能力尚未得到满意的答案。 我们将各种密码猜测模型生成的猜测列表视为数据的多个视图。 通过对多个猜测列表组合的深入分析，我们揭示了混合密码猜测能够增强破解能力的关键原因。 同时我们还发现多个视图之间的子集中的密码也遵循Zipf定律。 基于这些关键发现，我们提出了一种基于多视图学习的混合密码猜测框架，名为 GuessFuse。 GuessFuse 可根据以下工作流程从巨大的密码空间中实现准确的密码猜测结果。 GuessFuse 首先使用单独的密码猜测模型（例如 PCFG 和马尔可夫模型）生成多个密码猜测列表。 然后，使用多视图子集提取方法提取多个猜测列表之间的相交和互补子集。 接下来，GuessFuse 根据幂律将子集分割成密码段。 大量的实验表明，GuessFuse 的性能优于同类产品，并且能够有效地整合多种密码猜测模型的优点。 我们还将 GuessFuse 应用于 PSM 进行更广泛的比较分析。 我们发现 Minauto指标低估了某些密码的强度，从而降低了密码设置的可用性。 使用 GuessFuse 可以解决这个问题。 准备工作相关研究 密码破解方式概述 密码强度指标 minauto ， 但Minauto指标只是理论上的上限，并行使用多个密码猜测模型比使用单个模型需要多数倍的猜测次数，与实际猜测场景的要求不一致。 混合密码猜测模型提出但是处于起步阶段。 根据集成对象的划分，现有的混合密码猜测方法可以分为模型架构级别的集成和猜测列表级别的集成。 模型架构级别： 2018 年，Zhang 等人。 [29]提出了一种称为 SPSR 的混合密码猜测模型。 SPSR将PCFG模型[6]应用于密码结构层，将马尔可夫链模型[22]应用于密码字符串层。 同年，他们还提出了SPRNN模型[30]，该模型结合了结构划分和双向长短期记忆递归神经网络（BiLSTM）。 与 SPRNN 相反，Xia 等人。 [31]提出了另一种混合密码猜测模型，称为 PL。 PL采用PCFG模型[6]进行密码字符串层解构，并采用LSTM模型[15]进行结构层建模。 这些双重方法有效地利用了两种密码猜测模型在不同密码分析粒度层的优势。 然而，这些模型架构级别的集成需要有针对性的定制，限制了它们集成更广泛的密码猜测模型的能力。 猜测列表级别： 2021 年，王等人。 [32]平均组合多个密码猜测列表并对输出进行重复数据删除。 他们发现，将密码猜测模型与明显不同的密码生成策略（例如RNN模型[39]和PCFG模型[6]）相结合可以有效提高破解成功率。 2022 年，帕里什等人。 [34]对训练集中的密码进行去重，并按密码频率降序排序，生成猜测列表。 他们将其定义为“身份猜测者”。 他们发现，将模型生成的多个猜测列表与 Identity Guesser 相结合，可以显着提高破解成功率。 不过，这两项研究仅在猜测清单层面进行了初步尝试。 同等地组合猜测列表并不能准确地从每个猜测模型中提取有效部分。 2022 年，Han 等人。 [33]介绍了一种称为 hyPassGu 的混合猜测框架。 hyPassGu 通过限制每个模型生成目标类型的密码并分别确定猜测次数，利用 PCFG 和马尔可夫模型的优势。 尽管声称 hyPassGu 可以应用于 PCFG 和马尔可夫之外的其他模型，但它需要先了解模型的架构及其目标密码类型。 因此，它不能直接应用于其他模型。 另外，hyPassGu根据密码的结构特征将密码粗略地分为两类，导致有效密码大量流失。 因此，hyPassGu的破解成功率低于单个模型，仅略优于限制特定类型密码生成的模型。 关键问题仍然没有令人满意的答案： （1）什么有助于提高混合密码猜测的破解能力？ （2）如何有效发挥多种不同模型的优势？ 本文重点解决这些问题。 密码猜测场景数据驱动的密码猜测模型 + 拖网猜测 + 混合密码猜测 攻击者首先根据泄露的数据集构建密码猜测模型； 然后使用模型生成猜测； 最后尝试使用猜测破解所有目标密码。 虽然基于规则的密码猜测工具[20]、[21]也可用于混合密码猜测，但这些工具仍然依赖于目标数据集分布的先验知识来增强密码猜测的有效性。 注意：拖网场景可以进一步分为站内场景和跨站场景。 虽然跨站密码猜测场景较为真实，但攻击者一般以掌握目标数据的部分分布信息为前提来猜测密码。因此，为了更好地描述混合密码猜测的概念，我们将站点内拖网场景中的密码猜测任务形式化。形式化：一些符号表示，某个猜测密码成功破解了目标系统的多少个密码 多视角学习多视图学习的概念与混合密码猜测的概念非常吻合。 多视图学习引入了一个函数来对特定视图进行建模，并联合优化所有函数以利用相同输入数据的冗余视图，从而提高学习性能[40]。在更高层次上，多视图学习构建多个单一视图并评估它们的表现，然后设计功能来组合这些视图以改善学习成果。同样，混合密码猜测旨在通过利用从不同角度分析数据的多个模型来提高密码猜测的有效性。 因此，多视图学习可以用于解决在混合密码猜测中集成各种密码猜测模型的优势的挑战。 徐等人。 [41]提出多视图学习基于两个关键原则有效地利用多种视图： 共识和互补。 共识原则旨在最大限度地达成不同观点之间的一致。 互补原则指出，每种观点都可能包含其他观点所不存在的独特知识。 通过有效利用多种观点的共识和互补性，可以实现对数据的全面、准确的描述。在本文中，我们基于这些原理探索混合密码猜测。 数据集使用包含 5400 万个纯文本密码的 6 个数据集（参见表 I）。其中，三个数据集来自英文网站，三个数据集来自中文网站，涵盖六种不同的服务类型。数据源的多样性有助于减少我们分析中的偏差。 由于我们所有的数据集都可以在互联网上公开获取，因此这项工作的结果是可重复的。![[Pasted image 20240418173807.png]] 数据清洗原始数据集包含异常情况，例如未解密的哈希字符串、不可能由用户生成的过长密码以及包含不符合标准密码策略的不可打印字符的密码。 因此，我们首先对数据集进行数据清洗。 我们删除包含 95 个可打印 ASCII 字符之外的字符的密码，并删除长度 &gt; 30 的密码。这种数据清理策略在现有的密码猜测文献中也很流行 [24]、[26]、[42]。 道德考虑尽管我们使用的数据集是公开的，并广泛用于密码猜测研究 [8]、[27]、[32]–[34]，但这些数据集包含敏感的个人信息。 因此，我们仅分析密码数据的分布特征，并报告汇总的统计信息。 我们不会将任何数据用于学术研究以外的目的，因此不会增加受影响个人的风险。 密码猜测模型采用了四种领先的密码猜测模型（例如 Markov、PCFG、FLA 和 RFGuess）。 为了减轻其他因素对分析结果的影响，我们重点关注站内密码猜测场景。 这种情况被认为是理想的，因为训练集和测试集具有相同的数据分布，确保了我们分析的一致性。 我们从数据集中随机抽取大小为 $10^6$ 的子集作为测试集，而其余数据用作训练集。 我们发现结果对于基于此测试集大小的分析来说足够稳定。 这一现象与论文[9]、[43]、[44]一致。 用于生成猜测列表的每个模型的设置如下： 1）PCFG。 我们使用了 PCFG 模型 [6] 的 4.0 版，该模型可以在 GitHub 网站上找到。 该模型的语法包括六个段类别：字母A、数字D、其他字符O、键盘D、特殊字符串X和年份Y。 2）Markov。 我们选择 4 阶马尔可夫模型，并采用[26]中使用的拉普拉斯平滑和结束符号正则化来生成猜测。 3）FLA。 我们利用了 GitHub 网站上提供的 FLA 开源代码2，并遵循[15]中指定的推荐参数。 我们训练了一个由三个 LSTM 层组成的模型，每层有 128 个单元和两个密集层，总共 20 个 epoch。 4）RFGuess。 参考[24]，我们训练了一个有 30 棵决策树的随机森林。 其叶子节点的最小数量为10，特征的最大比例为80%，其余为scikit-learn框架的默认值[45]。 分析 本节通过多视角分析，依次解决以下问题： （1）混合密码猜测能够提高破解能力的关键原因是什么？ （2）使用猜测列表进​​行混合密码猜测可以获得什么好处？ 混合密码猜测的本质数据集比较我们通过比较每个模型生成的猜测列表来展示密码猜测模型之间的差异![[Pasted image 20240418204958.png]] PCFG 模型生成的猜测列表包括诸如 “dearbook123456” 和 “DEARBOOK” 之类的密码，这表明偏向于生成包含常见短语和简单结构的密码。 Markov 模型生成的猜测列表包含 “woshili” 和 “1989123” 等密码，反映了基于密码内字符相关性进行猜测的偏见。 RFGuess 模型的猜测列表包括诸如 “1234567” 和 “1234567}” 之类的密码，表明了将常见短语与各种特征相结合的趋势。 FLA 模型与马尔可夫模型类似，会生成一个包含 “45665456” 和 “qwqwqwqww” 等密码的猜测列表，显示出其对密码中顺序上下文一致性的偏见。 然而，FLA 模型生成的猜测列表也与马尔可夫模型不同。 中心区域还有“dearbook”、“12345678”等密码。 它表明，尽管每个模型都有独特的偏见，但他们已经就某些密码达成了共识。 这些密码可能很常用，并且很容易被各种模型预测到。 为什么需要混合猜测列表中的相似点和差异强调，即使基于相同的数据集，现有的密码猜测模型也可以生成包含不同密码特征的猜测列表。 本质上，密码猜测模型是根据用户设置易受攻击密码的行为而设计的。通过准确识别和匹配这些弱密码的特征，模型就可以生成破戒律很高的猜测列表。然而，由于用户密码设置行为的异构多样性[42]。即使像 PCFG 这样的已经涵盖了各种弱密码特征的模型（参见第II-E节），也无法在有限的猜测次数内完全覆盖所有特征 。 利用多种具有不同偏差的密码猜测模型的组合，可以更全面地识别和匹配密码特征，从而提高密码猜测的性能。 怎么混合 在模型的结构层面上利用多个异构密码猜测模型是相当具有挑战性的，并且不适用于新提出的密码猜测模型（例如RFGuess），这背离了使用多个异构模型的最初想法。 密码猜测模型最终会生成猜测列表。 鉴于多视图学习对于混合密码猜测的天然适用性（参见第 II-C 节），我们将密码猜测模型生成的猜测列表视为数据的多个视图。 在多视图学习领域，直观的是，集成多个不同视图来分析和处理数据可以产生更全面的信息，从而提高学习模型的性能。 这个概念也适用于混合密码猜测。 我们用一个示意性的解释来描述这一点。 ![[Pasted image 20240418211929.png]] 如图3所示，不同的密码猜测模型生成的猜测列表{G1，G2，G3，····，Gl}可以覆盖测试集T的不同部分。假设混合密码猜测方法H可以有效地提取出测试集T的不同部分。 从{G1,G2,G3,····,Gl}中覆盖T的密码，生成优化猜测列表eG。 组合的猜测列表越多，测试集的覆盖范围就越大，从而产生更好的破解能力。 请注意，图 3 中描绘的示意图可能并不通用，因为猜测列表中的密码通常存在重叠。 然而，组合更多猜测列表可以覆盖更多测试集的因果关系是普遍存在的。 我们将通过下面的分析来证明这一点。 多视图集成的优点依据： 大量实验已经证实，我们所描述的结果在六个数据集中得到一致观察，从而证实了它们的普遍性 以 CSDN 数据集生成的 top $10^6$ 密码猜测列表为例。 实验：定量特征分析：对组合猜测列表的统计特征进行定量比较，以证明多视图集成带来的猜测有效性。 将每个模型生成的猜测列表以相同的大小组合起来，并对它们进行重复数据删除，然后将结果与单个猜测列表进​​行比较。 例如，在比较大小为 $10^3$ 的猜测列表时，我们结合 PCFG 和马尔可夫模型生成的前 $10^3$ 个猜测，删除任何重复的密码。 输出被定义为两个视图集成。 请注意，这种组合策略与 Minauto 指标 [16] 的计算相同，并不反映现实场景 ( Minauto 指标是一个理论上的上限，代表了每个密码被所有密码猜测模型破解所需的最小猜测次数。然而，在实际的密码猜测场景中，使用多个密码猜测模型并行进行猜测需要的次数是单个模型的数倍，这与实际需求不一致)。 我们的目的只是证明通过跨多个视图的特征量化来增强密码破解能力的可行性。 ![[Pasted image 20240419151949.png]] 密码结构 Struct：基于 PCFG 算法来分析猜测列表中的密码结构的数量。 例如，如果猜测列表包含 “dearbook134”、“dearbook309” 和 “123dearbook” 等密码，则它包含两种类型的密码结构： “A8D3” 和 “D3A8”。 有效密码比例 Effect：有效密码&#x2F;猜测列表大小。猜测列表中的密码如果能够与测试集中的密码匹配，则认为该密码有效。 破解成功率 $Min_{auto}$ 唯一密码比例 Uniq：唯一密码&#x2F;单个单个猜测列表 结果： Effect 和 $Min_{auto}$ 说明了多视图集成所带来的猜测有效性的提高。 Uniq 和 struct 说明通过集成多个视图实现的多样性的增强。整合更多元的观点可以增加猜测的多样性，覆盖更多的密码特征，从而提高密码猜测的有效性 有效的密码分析：问题：组合多个猜测列表会产生许多独特的密码，其大小远远超过单个猜测列表的大小。 如果全部用于最后的破解，会导致破解次数消耗过多。![[Pasted image 20240419155303.png]]幸运的是，图4（a）表明，当猜测数量增长到大约 $4×10^4$ 时，组合多个猜测列表的有效密码数量开始低于单个猜测列表的大小（an注：从一个数据集中选择 100 比 四个数据集中每个选择 25 个效率高）。在这种情况下，通过选择这些有效密码中的一部分，即使优化后的猜测列表大小小于单个猜测列表的大小，猜测的成功率仍然可以达到多视图集成的 Minauto 指标。（an注：此时单猜测列表 和 多猜测列表分母增加相同大小时， 单猜测列表的分子增加的多，论文描述的意思是减少了 多猜测列表分母增加的个数，有点偷换概念的意思） 在了解测试集的前提下，我们假设了一种最优的混合密码猜测方法，该方法可以有效地整合多个猜测列表以产生优化的猜测列表。 此混合密码猜测方法生成的优化猜测列表包含每个猜测列表中最有效的密码。 如果猜测的密码在测试集中破解频率最高的密码，则认为该密码具有最高的效率。 例如，在整合大小为103的猜测列表时，我们首先对猜测的密码进行去重，并将其与测试集进行比较，计算每个猜测的密码可以破解的测试密码的数量。 然后，我们按照破解次数降序选择猜测的密码，并将其添加到最佳猜测列表中，直到最佳猜测列表的大小也达到103。我们将最佳猜测列表的破解成功率与最佳猜测列表的破解成功率进行比较。 由单一密码猜测模型生成的猜测列表（见图4（b））。 如图 4（b）所示，当猜测数 &lt; 4 × 104 时，与 PCFG 模型相比，整合两视图的最优猜测列表的破解成功率平均提高了 1.3%，整合三视图的破解成功率平均提高了 1.3%。 2.6%，整合四视图增长了2.8%。 尽管有效密码的数量超过了单个猜测列表的大小，但选择效率最高的部分同等大小的猜测密码仍然可以增强破解能力。 上述结果表明，从多个视图中准确提取有效密码是进行有效混合密码猜测的关键。 多视图子集分析：目的：为了找到从多个视图中提取有效密码的有效方法，我们对多个视图之间的子集进行了分析。手段：对不同模型生成的相同大小的猜测列表进​​行集合运算，以获得相交和互补的子集。![[Pasted image 20240419165944.png]] 我们分析了集成不同数量的视图时多视图子集的变化（见图 5）。 如图 5 所示，集成两个视图生成三个子集，三个视图集成生成七个子集，四个视图集成生成 15 个子集。 此外，随着集成视图数量的增加，每个子集中有效密码的分布变得更加集中。 相交子集中有效密码的比例（中间红色数值表示）随着集成视图的增加而增加。 在基于CSDN数据集的实验中，融合两个视图时，相交子集中有效密码的比例为51.51%。 当集成三视图时，这一比例增加了 21.0%，而当集成四视图时，这一比例又增加了 3.9%。 这些结果表明，集成多个视图可以提供有关有效密码分发的增量信息。 随着更多视图的整合，增量信息增多，有效密码的分布更加集中。 ![[Pasted image 20240419170332.png]] 我们继续分析有效密码在各个子集中的分布。 目前，当我们仅通过集合运算提取猜测列表中的相交和互补子集时，子集中的密码不包括顺序。 因此，我们通过分析不同大小的猜测列表中子集的平均破解成功率来评估子集中有效密码的分布（见图7）。 为了便于表示，我们用图6来表示每个平均破解成功率所代表的子集。 具体来说，我们使用 4 位二进制代码来命名每个子集，其中每个位代表密码在特定模型生成的猜测列表中的出现，从左到右分别为：PCFG、FLA、Markov 和 RFGuess。 例如，“1000”表示该子集中的密码仅出现在PCFG猜测列表中。 ![[Pasted image 20240419170602.png]] 如图7所示，相交子集“1111”的整体破解效率明显优于其他子集。 然而，相交子集中的密码的后半部分并不优于其他子集中的密码的前半部分。 例如，相交子集“1111”在101次猜测中的平均破解成功率为3.57×10−6，而互补子集“0010”在101次猜测中的平均破解成功率为2.61×10−5。 有趣的是，子集的平均破解成功率与单个猜测列表的大小呈现近似线性关系。 当我们将 15 条曲线作为一个整体来考虑时，这一点变得更加明显。 这可能符合密码中的 Zipf 定律 [46]。 我们使用PDFZipf模型[46]拟合数据：logfr &#x3D; logC − s · logr，其中fr和r分别对应于平均破解成功率和单个猜测列表的大小。 如表III所示，C ε [-3.55，-0.85]和s ε [-0.92，-0.63]是常数。 拟合的 RMSE 在 [0.08, 0.24] 范围内。 这种现象有助于在混合密码猜测中利用多种密码猜测模型的优势。 尽管密码分布中的齐普夫定律通常用于解释用户设置密码的行为，但也有研究 [33]、[47] 通过分析密码猜测模型中齐普夫定律的存在来改进密码猜测模型。 在我们的例子中，齐普夫定律表明，猜测密码的效率随着密码在子集中的排名降序而下降。 换句话说，子集中排名较高的密码被认为具有较高的破解效率。 密码结构密码结构（Struct.）的数量是通过对猜测列表中的密码进行分析和统计得出的。具体来说，密码结构是指密码中的字符排列方式，例如”A8D3”或”D3A8”。在文中，作者使用PCFG算法对密码结构进行分析，通过猜测列表中的密码来确定不同类型的密码结构的数量。通过对密码结构的数量进行统计分析，可以评估多视图整合对密码破解能力的提升效果。这些统计特征表明，整合更多不同的视图可以增加猜测的多样性，覆盖更多的密码特征，从而提高口令猜测的有效性。 Minauto类似于一个破解成功率。 多视图提取模块的工作原理是什么？多视图提取模块的工作原理是从多个猜测列表中提取交集和补集密码，以生成多视图子集。该模块首先根据输入的猜测列表数量生成多个子集，并为每个子集分配逻辑标签。然后，根据排名，模块按照密码的逻辑规则将密码分别添加到交集子集或补集子集中。最终，该模块输出维护的多视图子集，其中包含了有效密码的信息。这一过程有助于准确地从多个视图中提取和重新组织有效密码。 分割选择模块的作用是什么？分割选择模块的作用是进一步细化每个子集，根据幂律间隔将其分割成多个密码片段。该模块利用幂律分布间隔序列来将输入的子集分割成更细粒度的密码片段。然后，该模块利用验证集来评估这些密码片段的破解效率，具体来说，该模块计算这些密码片段的平均破解成功率。最终，该模块重新组织有效的密码片段，生成最终的优化猜测列表。"},{"date":"2024-04-22T13:14:48.834Z","url":"/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/AReview/","categories":[["undefined",""]],"content":"摘要 2016-2023 发表了 30多种密码猜测方法。 引入了一种分类方法，将现有的方法分为拖网猜测和定向猜测。 其次，我们提供了一个广泛的基准数据集，可以帮助研究人员和从业者在后续工作中。 文献计量学分析，以呈现这一领域的趋势和综述论文之间的交叉引用。 从不同的应用场景、猜测ﬁ的效率以及传统学习方法和深度学习方法的结合等方面讨论了密码猜测的开放挑战。 本文提出了密码猜测的未来研究方向，以指导密码猜测的后续研究和发展。 引言 互联网和安全发展，认证方式 基于密码认证的原理 密码猜测为什么有效 身份认证的分类： 用户知道的：密码，个人识别码 用户拥有的：U盾，加密卡 用户本身的： 生理特征：指纹，虹膜 行为特征：手写 为什么密码安全有压力： 互联网发展。需要很多加密 记忆能力有限。弱密码，相同密码 密码泄露 密码猜测的角度： 启发式搜索 概率模型 深度学习 相关的综述综述1A Large-Scale Analysis of the Semantic Password Model and Linguistic Patterns in Passwords2021从样本量、 和语言信息三个方面对语义密码文法进行了综合评价。 密码是一种特定的文本序列，具有长度短、语义丰富的特点。因此，充分利用口令的语义和语言模式有利于口令猜测。在调查[8]中，从样本量、概率平滑和语言信息三个方面对语义密码语法进行了综合评估，并在交叉验证环境中将其与最新的概率上下文无关文法(PCFG)和神经网络模型进行了比较。实验结果揭示了句法模式和语义模式对口令猜测的贡献，表明句法和语义模式对口令安全的影响更大。此外，本文还说明，与最新的神经网络同类模型相比，PCFG往往仍具有竞争力。在训练密码超过100万的PCFG时，性能略有提高。 综述2Deep Learning for Password Guessing and Password Strength Evaluation: A Survey2020调查了2019年前密码猜测和密码强度评估的最新深度学习方法。 在文[9]中，作者综述了用于密码猜测和密码强度评估的最新深度学习方法，包括密码模式提取、候选密码生成和密码强度测量。然而，由于发表于2020年，该论文在2019年之前只是一篇与统计相关的论文。近年来，各种人工智能技术迅速迭代，范式从模型转变为机器学习理论。因此，迫切需要更新相关内容。 综述3A Preliminary Analysis of Password Guessing Algorithm [10] 采用覆盖率指标来量化个人信息在创建个人密码过程中的参与程度，并使用蒙特卡洛和Zxcvbn方法来评估密码强度。 在文献[10]中，简要回顾了现有的各种典型密码猜测算法，包括假设、识别信息和理论模型。多个指标也被用来理解和评估这些算法的性能。通过对实验结果的分析，总结了不同密码猜测算法的特点。证明了当猜测次数相同时，两种算法比一种算法猜测更多的口令。此外，作者还提出了一种混合密码猜测算法PaMLGuess，该算法具有很强的可解释性和泛化能力，并利用概率映射来解决不同密码猜测算法给出的概率大小相差很大的问题。 综述4The AI-Based Cyber Threat Landscape: A Survey2020探索了通过将人工智能的“黑暗面”与攻击技术相结合而构成的网络攻击的研究实例，并介绍了对这些攻击进行建模的分析框架。 综述5 **Deep Learning vs. Traditional Probabilistic Models: Case Study on Short Inputs for Password Guessing [12]2019重点分析了深度学习算法与传统概率模型在短密码串上的对比分析。对深度学习算法和传统概率密码猜测模型进行了比较分析[12]。作者分析了泄漏数据集的密码模式，并对两种主流的概率模型进行了比较研究，即基于马尔可夫模型和基于PCFG的模型，以及基于深度学习的代表性方法PassGAN模型。 综述6 **Let’s Go in for a Closer Look: Observing Passwords in Their Natural Habitat [13]2017 调查方法检索和选择作品的方法 这篇关于密码猜测的综述遵循了一种系统的文献综述方法。我们按照[14]提出的程序，从现有文献中检索研究论文，从结果中挑选相关作品，然后进行总结。因此，系统的审查过程是可重复的，并减轻了对文献中作品的选择偏见。第2.1-2.3节概述了本综述的研究问题、搜索策略和研究选择。 专业术语密码猜测 神经网络密码破解 神经网络密码攻击 机器学习密码猜测 深度学习密码可预测性 VAE(分自编码器（Variational Autoencoder）)密码重用 变压器密码概率模型 PCFG拖网 GaN目标 马尔可夫 密码数据![[Pasted image 20240305093437.png]] 泄露的数据大多来自大国的知名公司，如美国和中国。因此，在泄露的密码数据集中，英语和中文是主要语言。其次，大多数密码数据都是在2010年左右泄露的。最后，一般来说，数据集的大小越大，被用作研究语料库的可能性就越高。例如，就数据量而言，RockYou是表中最大的英文数据集，也是我们评论作品中使用最频繁的语料库。 密码猜测攻击根据是否在线： 离线密码猜测 在线密码猜测根据是否利用用户的个人信息： 拖网猜测 定向猜测，利用用户个人信息 是否使用人工智能： 传统攻击 神经攻击 几乎所有现有的工作都是离线猜测，而在线猜测太难在100个猜测内猜出一个密码。 拖网猜测传统拖网猜测 启发式算法 这些算法没有严格的理论体系，严重依赖于零散的奇思妙想，例如，基于开源软件使用精心设计的猜测序列[21]构建独特的猜测词典[18-20]。正如[3]指出的那样，这些启发式方法很难重现，也很难相互公平地比较。因此，在这里，我们只介绍一些常用的启发式密码猜测工具。 Jtr是一个密码猜测工具，专注于破解UNIX&#x2F;Linux系统的弱密码。在JTR中有四种模式–简单破解模式、单词表模式、增量模式和外部模式。 Hashcat是世界上速度最快、最先进的密码恢复实用程序，支持针对300多种高度优化的哈希算法的五种独特攻击模式。Hashcat目前支持Linux、Windows和MacOS上的CPU、GPU和其他硬件加速器，并具有帮助启用分布式密码破解的设施。 彩虹攻击：一种基于预先计算和空间-时间折中的攻击方法。通过事先计算大量的密码和散列值之间的对应关系（彩虹表），以便在攻击时快速查找并破解散列值。彩虹表是一个预先计算的表格，其中包含了大量的密码和对应的散列值。通过在彩虹表中进行查找，攻击者可以找到与目标散列值匹配的原始密码。 概率上下文无关语法 该算法的核心假设是密码的字母段L、数字段D和特殊字符段S相互独立。该算法包括训练和猜测两个阶段。 在训练阶段，最关键的是根据泄露的密码数据集计算密码模式(结构)和字符成分(语义)的频率。 在猜测阶段，根据训练阶段得到的模式频率表和语义频率表生成具有频率猜测的集合，以模拟真实密码的概率分布。 粒度太细，无法获得字符之间的语义关系，改进： [24]基于词的内聚力和自由度提取密码中的语义片段，并对基于语义片段的PCFG算法进行改进 [25]中，作者将密码视为由几个块组成，其中块是频繁出现在一起以模拟密码的相关字符的序列。 25扩展了字节对编码(BPE)[26]算法，它使用块词汇表平均长度的可配置参数来替换合并操作的数量。 为了解决长密码猜测的困难，请参考。[27]提出了一种改进的基于PCFG的LONG 马尔可夫序列决策 该算法的核心假设是用户从前到后依次构造口令。它不像PCFG那样分割密码，而是训练整个密码，并通过从左到右链接字符来计算密码的概率。 传统的马尔可夫模型因其结构简单、推理速度快而被广泛应用于口令猜测工作中。但是，它也有一定的缺陷，如过度拟合、重复率高、基于随机抽样生成的密码覆盖率低等。 过拟合：[17]将拉普拉斯平滑和结束符号正则化技术应用于马尔可夫模型。平滑策略消除了数据集中的过拟合问题，正则化技术使攻击算法产生的猜测概率总是和为1。 重复率：[29]设计了一种基于随机抽样的动态分配机制。该机制允许动态调整密码的概率分布，并在猜测过程中严格收敛到均匀分布。基于上述动态分布机制，提出了一种动态马尔可夫模型。 对口令中的语义段进行建模：Ref.[30]提出了一个名为Word马尔可夫的模型，通过语义分段从密码中提取单词的凝聚力和自由度。 神经拖网猜测算法密码猜测任务视为文本生成问题，2006年，参考文献[32]引入浅层神经网络进行密码猜测。2016年，参考文献[33]提出了基于递归神经网络的口令分布估计算法FLA。该模型遵循马尔可夫模型的序贯决策过程，但放宽了n-马尔可夫假设。FLA可以通过树遍历算法枚举密码空间，从而产生新的猜测结果。从那时起，各种基于的产生式神经网络被反复观察到用于拖网密码猜测。 回归神经网络(RNN)：回归神经网络是一个递归神经网络，它以序列数据为输入，在序列进化方向上递归，所有节点以链的形式连接。 RNN 有许多变体：如： LSTM，GRU，BiLSTM 等。 步骤：基于RNN的密码猜测方法通常有以下两个步骤： ·将训练集中的密码序列输入到RNN模型中，以进行顺序文本生成的训练。 训练好的 RNN 猜测模型旨在根据已有的密码字符生成下一个密码字符，直到输出终止字符位置。 在整个密码生成过程中，RNN计算任何字符作为下一个密码令牌的概率。对于给定的阈值，概率高于阈值的密码将被放入密码猜测集中作为有效密码。 一些变体： [38]对密码单词进行切分，在词段的基础上使用BiLSTM生成密码。 [39]中，作者提出了一种层次语义模型HSM，该模型将LSTM和语义分析相结合，用于挖掘用于密码猜测的词之间潜在的概率关系。 [40]提出了一种基于汉字音节的密码猜测方法。该方法将汉语音节作为整体元素对密码进行解析和处理。然后，在LSTM神经网络中对处理后的密码进行训练以生成密码。 [41]中，作者使用深度学习，特别是具有注意力机制的RNN来组合和内插同一组用户的信息，以定义比他们的密码分布更稳健和准确的优先级。推理时使用辅助数据来适应目标密码的分布。通过这种方式，开发了一种利用辅助信息并实例化上下文感知口令计量和猜测攻击的全自动方法，而不需要来自目标口令分布的任何明文样本。 生成式对抗网络（GAN）： 基于GAN的密码猜测模型由密码生成器和鉴别器组成。这两个网络相互对峙，不断地调整参数。最终目的是使鉴别器网络无法判断生成网络的输出是否真实，从而达到口令生成的效果。 问题：离散密码数据的不可微性可能导致梯度反向传播失败，基于GAN的密码猜测模型训练难以收敛，由GAN模型生成的密码重复率较高。 离散数据不可微问题解决： Works[44，45]使用Gumbel-Softmax[46]松弛技术来训练基于GAN的密码猜测模型。此外，在工作[44]中提供了另一种解决方案，其使用通过附加的自动编码器获得的真实口令的平滑表示。 [47]提出了一种基于SeqGAN的密码猜测方法RLPassGAN，该方法使用策略梯度来确保模型参数的持续优化。 收敛困难的问题解决：[49]设计了一种基于双向生成对抗网络的猜测算法，提高了算法的收敛速度。与传统的GaN相比，它可以在更短的时间内产生相同数量的样本 重复率高问题解决：[47]认为根本问题在于生成器中间层的输出是一个不完整的密码序列，在到达输出层之前不能由鉴别器直接评估，从而导致许多冗余的合成密码。为了解决这个问题，作者提出了一种改进的方法，该方法使用蒙特卡罗搜索[50]来评估中间层输出处的不完整密码序列。不同的是，裁判。[49]使用鉴别器和控制器构建额外的控制器网络，以分别学习生成的密码分布与真实密码分布和均匀分布之间的度量。然后利用这两个度量对生成器进行训练，从而降低了口令生成的重复率。 RLPass[51]还创新性地将表征学习用于密码猜测。具体地，将密码投影到隐藏空间，并且使用隐藏空间中的密码表示之间的距离来定义密码的相似性。基于口令的强局部性和弱局部性现象，提出了一种动态口令猜测和条件口令算法。为了解决基于GAN的模型生成的长密码质量较低的问题，请参阅[52]设计了一种基于DenseNet的[53]GAN密码猜测结构DenseGAN，并提出了两种新的密码猜测DenseGAN模型，这两种模型都能生成高质量的密码猜测。 自动编码器（AE）：自适应学习是一种无监督的学习模型。它基于反向传播算法和优化方法，使用输入数据本身作为监督来指导神经网络学习映射关系，以获得重建的输出。AE 包括编码器和解码器两部分。根据学习范式，AE 可分为欠完备自动编码器、正则化自动编码器和变分自动编码器，其中前两种是判别性模型，后者是生成性模型。在拖网密码猜测的研究中，通常使用变分自动编码器[54]来生成密码猜测。如图5所示，将密码样本(Yu123)输入到VAE的编码器以获得表示，然后使用解码器基于该表示重建样本。基于输入x和生成的x之间的重构损失来训练密码生成器。 [47，51，55，56]使用VAE模型来生成密码猜测，但每种方法都有稍微不同的侧重点。在[55]中，作者使用经典的VAE框架来猜测密码，而不需要任何更改。不同的是，裁判。[47]将VAE技术与GaN技术相结合，用VAE代替GaN生成器，旨在解决离散口令数据的反向传播问题。针对模型的轻量级问题，在[56]中，用门控卷积神经网络(GCNN)[57]代替了复杂的RNN生成单元，大大降低了模型的复杂性。 Transformer. ![[Pasted image 20240305201324.png]] Transformer 模型采用编解码器体系结构，并使用注意[59]替换 Seq2Seq 模型中的递归结构，以实现序列建模的并行化。这种并行化结构给自然语言处理 (NLP) 领域带来了巨大的冲击。随着研究的深入，相关技术逐渐从自然语言处理向计算机视觉 (CV)、语音、生物、化学等领域发展。同样，在拖网密码猜测的研究中也出现了一些基于Transformer的方法[60，61]。 [60]旨在研究常见密码规则中密码破解的概率，为密码设置提供参考。作者收集了大量用户的个人信息和密码，并分析了个人信息和密码的相关性。在此基础上，实现了一种基于改进 Transformer 的口令猜测模型。该工作将消息权重引入到数据预处理中，并在模型中使用改进的波束搜索算法来快速搜索排名靠前的密码猜测。 在[61]中，作者提出了一个基于双向 Transformer 的猜测框架 PassBERT，首次将预训练&#x2F;微调的范式应用于密码破解。具体地说，首先，作者设计了包含一般密码分布知识的通用密码预训练模型。然后，提出了三种特定于攻击的微调方法来定制预先训练的口令模型以适应以下真实攻击场景：条件口令猜测、目标口令猜测和基于自适应规则的口令猜测。最后，他们进一步提出了一种混合密码强度计来降低这三种攻击的风险。 Reinforcement Learning (RL) 强化学习 RL是机器学习的范例和方法之一，用于描述和解决智能代理在与环境交互过程中最大化回报或实现特定目标的学习策略问题。密码生成器是代理，每个生成的密码序列代表一个完整的轨迹，生成过程中的每个字符都被视为一个操作。例如，由时间戳t生成的字符是action at。根据由所生成的不完整序列确定的关于当前状态ST的随机策略来生成每个动作At。生成器根据一组随机策略从任意字符生成密码，直到满足预定长度。 在[47]中，作者提出了一种基于强化学习和遗传算法的拖网密码猜测模型RLPassGAN。具体地说，该工作遵循SeqGAN[48]，将密码猜测视为一个连续决策，并使用策略梯度 来确保参数可以连续优化。此外，通过蒙特卡罗搜索[50]来评估输出的不完整密码序列。蒙特卡罗是一种使用大量随机样本来了解特定系统的计算方法。它非常强大和灵活，但易于理解和实现。它是在1940年的曼哈顿计划中提出的。这个名字来自赌城蒙特卡洛，象征着可能性。除了上述应用，蒙特卡罗还被应用于估计给定口令的猜测次数[62，63] Flow.在上面介绍了生成模型GaN和VAE；它们都没有显式地学习真实数据p(X)的概率密度函数。对于具有潜在变量的生成模型，几乎不可能计算它们p(X)&#x3D;p(x|z)p(Z)，因为很难遍历隐藏变量z的所有可能值。 这一挑战被生成性模型解决了–归一化流动[64]，这是一个强大的密度估计统计工具。流动模型的一个非常独特的特征是，它的转变通常是可逆的。如图8所示，流模型不仅找到了从分布Z转移到分布X的网络路径，而且该路径还允许X改变为Z。简而言之，流模型找到了分布Z和X之间的双向路径。当然，这种可逆性是以Z和X的数据维度必须相同为代价的。 [68]中，朱利奥·帕尼奥塔等人。提出了一种基于产生流模型的口令猜测方法PassFlow。基于流的口令猜测模型使用精确的对数似然计算和优化，使潜在变量的推断更加准确。此外，还给出了潜在空间的一种有意义的表示，这使得探索潜在空间的特定子空间和内插等操作成为可能。作者论证了产生流模型在密码猜测中的适用性。实验结果表明，PassFlow能够在比以前方法小几个数量级的训练集的情况下，在密码猜测方面优于现有的基于GAN的方法。 传统定向密码猜测攻击者使用与目标人员相关的个人信息来增强猜测2015年，裁判[69]首次提出了一种基于拖网马尔可夫攻击模型的定向攻击猜测方法[70]。基本思想是，关于使用某些个人信息的人口的百分比，攻击目标也将具有相同的使用该个人信息的可能性百分比。为了实现这一想法，文献[71]首先将PII划分为几种类型，如用户名-A、电子邮件前缀-E和名称-N，并根据所需的粒度进一步细分每种广泛的类型。然后，将训练集的每个密码中的所有PII替换为对应的PII类型。训练阶段的其余步骤与行走马尔可夫模型的步骤相同[71]。猜测集生成阶段分为两个步骤。在第一步中，运行行走马尔可夫模型[71]以生成中间猜测集合，该中间猜测集合包含直接可用的猜测，例如123456，以及具有PII类型的基本字符的中间猜测(例如，N1、N2123)。第二步用相应的PII信息替换中间猜测中的基本PII类型字符。个人-PCFG。2016年，参考[5]提出了一种基于PCFG的定向攻击猜测方法–Personal-PCFG。它遵循拖网PCFG攻击模型[16]。其基本思想与PCFG攻击模型相同：根据字符类型和长度对密码进行切片。为了实现这一思想，文献[5] (即用户名-A、电子邮件前缀-E、姓名-N、生日-B、电话号码-P和ID-G)，并将这六种PI字符类型等同于拖网PCFG模型中的L、D和S，从而在个人PCFG中有九种类型的字符。然后，在训练过程中，与拖网PCFG攻击模型[16]中一样，训练集中的每个密码根据相应的字符类型及其长度进行分段。TarGuess Wang等人。[4]提出了一个框架，它用四个数学概率模型系统地刻画了典型的定向猜测场景。第一个场景TarGuess-I旨在利用用户的PII创建在线目标密码猜测。为了在密码中表示PII令牌，除了PCFG[16]模型中的L、D和S标签外，作者还定义了基于个人信息类型的28个PII标签(例如，N1−N7和B1−B10)。对于每个PII标签，其下标编号表示该类型的PII使用的细分，而不是指示相应长度的下标编号，例如L、D和S标签。例如，N表示名称信息，而N1表示全名，而n2表示全名的缩写第二个是Targuess-II，目的是根据用户在其他网站(例如Dodonew)中泄露的密码来猜测目标网站(例如CSDN)中的用户密码。具体地，作者提出了6种结构级和2种字符级助记转换来描述密码重用，并基于上述重用规则使用马尔可夫模型来刻画上下文无关的转换文法。第三个是TarGuessIII，目的是使用姐妹密码和一些PII信息来猜测用户的密码。TarGuess-III将PII信息引入到TarGuess-II模型中，允许在结构级密码重用中嵌入PII信息。与TarGuess-III相比，TarGuess-IV场景中的攻击者知道难以量化的额外PII(例如，性别、教育)。为了解决某些PII难以直接体现在口令中的问题，在这种难以量化的PII的基础上，巧妙地引入贝叶斯理论来计算口令的重用概率。 RFGuess-PII.在第4.1.1节介绍的RFGuess的基础上，Wang et al.[31]提出了一种新的目标口令猜测模型RFGuess-PII。密码训练和生成过程类似于拖网猜测场景。不同的是，通过新的PII匹配，密码中的PII字符串被替换为相应的数字标签。这种新的PII匹配旨在最小化信息熵，并试图准确地提取整个用户组的PII使用行为。PII匹配算法的第一步是细分PII的各种可能的变换，并使用数字标签来表示它们。第二步是为训练集中的每个密码列出具有PII标签的所有可能的表示。然后，按频率从高到低对表征进行排序。 RFGuess-重复使用。除了基于PII的针对性密码破解研究外，在[31]中，作者还专注于对用户的密码重复使用行为进行建模。他们还考虑了结构级和段级转换，如TarGuess-II[4]。具体地，它们通过计算训练集中每个密码对的编辑矩阵来计算结构级转换，并训练基于随机森林的段级转换(即，相同类型的字符串内的转换，例如字母段中的密码→密码)模型。 TG-SPSR图谱。在[72]中，作者将马尔可夫模型和PCFG模型转化为目标攻击，提出了一种基于结构划分和字符串重组的系统目标攻击模型，称为TG-SPSR。在结构划分阶段，除了将密码划分为类似于PCFG[16]的基本结构外，还在基本语法模式中定义了基于轨迹的键盘，并引入了索引位来准确描述特殊字符的位置。此外，基于定义的9条修改规则，构造了一个BiLSTM分类器来重用和修改密码的行为。 神经定向密码猜测随着自然语言处理技术的发展，一些复杂的神经网络被应用到目标密码猜测领域。在[73]中，作者提出了一个由指针生成网络组成的有针对性的口令猜测模型PG-PASS。该工作创新性地将目标口令猜测作为一项摘要任务，并将智能摘要领域中常用的指针网络技术应用于该任务。需要注意的是，除了用户的人口统计相关信息(姓名、生日等)外，攻击者还可以利用用户在其他网站上泄露的密码进行有针对性的攻击。可以预期，这种利用用户密码重复使用这一易受攻击行为的定向攻击可能比基于人口统计相关信息的攻击更具危害性。基于用户总是渴望通过重用或微调旧密码来生成新密码的事实，参考文献。[74]提出了一种基于密码转换器的重用模型，并对证书篡改攻击进行了模拟。在IEEE S&amp;P‘19会议上，Pal等人。[75]引入深度学习技术来表征用户的密码重复使用行为。更具体地说，他们训练了序列到序列(Seq2seq)模型，以预测将现有密码转换为其姊妹密码所需的修改，并在大规模数据集(即4iQ数据集[76])上进行了验证。此外，Wang et al.[77]提出了一种有针对性的密码猜测算法PASS2EDIT，用于模拟日益严重的凭据篡改攻击，在该攻击中，攻击者利用受害者泄露的密码来提高她在其他站点猜测受害者密码的成功率。特别是，他们提出了一种多步决策训练机制，并建立了一个分类神经网络来学习一步编辑操作对现有密码的反应。由于每项工作使用的数据集不同，因此很难进行横断面比较。以12306数据集为例，当猜测集为100时，现有的定向密码猜测方法的猜测准确率约为41.07%[73]。在重复使用猜测场景中，当受害者在站点A的密码(即PWA)已知时，在100次猜测中，Sota方法[77]在猜测其在站点B的密码(PWB&#x3D;PWA)时的破解成功率分别为24.2%(对于普通用户)和11.7%(对于安全敏感用户)。 数据统计从时间上看，2016年至2018年零星出版数量较少，2021年增至9种。这一数量的增加表明关于密码猜测的研究总体上呈上升趋势。在分布式定位技术方面，基于深度神经网络的猜测方法主要集中在图形的左上角，而基于传统方法的猜测方法主要集中在右下角。这一现象表明，神经密码猜测的研究是近年来的热点。这也反映了密码猜测技术的重点正在从传统方法向深度神经网络方法转移。 从数字的角度来看，在许多研究中，神经密码猜测方法所使用的技术更加多样化。其中，GAN、RNN和CNN是使用最多的技术。在传统的密码猜测方法中，PCFG和马尔可夫是使用最广泛的两种密码猜测技术。 从品类上看，现有研究主要集中在拖网密码猜测技术多达37种，针对定向密码猜测研究的著作仅有6部。一方面，正如前面提到的，目前对密码猜测攻击缺乏系统的研究，而且大多集中在个别攻击场景，如离线拖网猜测。此外，越来越现实的有针对性的在线猜测很少得到解决。另一方面，尽管少数研究侧重于有针对性的密码猜测，但大多停留在用户行为的简单统计和对启发式突发事件的依赖程度上，缺乏理论和原则性的研究。一些根本性的问题需要解决。 交叉引用分析FLA[33]和PassGan[43]是被其他人引用最多的作品。这间接说明了这两部作品的影响。由于FLA和PassGAN是第一个分别使用RNN[34]和GAN[42]来模拟口令可猜测性的方法，它可以确定为什么RNN和GAN使用频率很高。我们对第一作者所在机构的国籍进行了统计分析。如图11所示，我们用出版物的数量给世界地图上的国家&#x2F;地区上色。一眼就可以看出，中国地区的色彩最深。这意味着中国是密码猜测领域发表论文最多的国家。美国队紧随中国之后，位居第二。每个国家的影响因子计算如下：IF&#x3D;引文出版物。经过计算，中国的IF为0.28，美国的IF为11。这个值表明，虽然中国的出版物数量很多，但其在这一领域的影响力远远小于出版数量较少的美国。 挑战和未来趋势多样应用前景主要集中在拖网竞猜场景上，但在实际应用场景中，往往存在定向竞猜、少发竞猜、资源占用少的要求。对于这种场景，现有的方法很难解决镜头少的问题。虽然可以使用数据增强等技术来缓解样本不足的问题，但要从根本上突破两难境地仍然是棘手的。同样，各种深度学习算法也严重依赖于样本的大小和质量，密码等敏感信息的收集是困难的。因此，根据有限的密码样本猜测目标密码是一个复杂的问题。 密码猜测的效率然而，在实际应用中，时间和计算资源往往是有限的。传统和神经密码猜测方法都需要巨大的计算能力来支持。随着神经网络模型变得越来越复杂，存储需求也大大增加。因此，在低资源的情况下快速执行密码猜测也是具有挑战性的。在综述的论文中，只有一篇文章讨论了一个轻量级密码猜测模型[56]。因此，这方面的大量工作仍亟待推进。 传统学习方法与深度学习方法相结合然而，纵观现有的研究，新兴的生成模型，如流[64]和扩散模型[78-80]，很少应用于密码猜测。此外，目前的神经密码猜测方法都有一个前提，即密码字符串是一个字符序列，使用序列编码对密码进行处理。事实上，我们可以做更多的尝试，比如根据密码的语义依赖将密码组织成图，并使用最新的图神经网络(GNN)相关技术[81-84]进行采样和生成。此外，预训练的语言模型[85-88]近年来在自然语言处理领域如火如荼地进行着。然而，只有一部作品[61]讨论了关于密码猜测的预训练&#x2F;精调范式。因此，将预先训练的语言模型强大的通用语言建模功能与现有的密码猜测工作结合起来也可能是一个好主意"},{"date":"2024-04-22T13:14:48.794Z","url":"/2024/04/22/%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/6G%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E6%8A%A5%E5%91%8A/","categories":[["undefined",""]],"content":"随着5G技术的全球部署和应用，6G技术的研究和探索已经悄然启动。预计6G将在2030年前后商用，带来比5G更高的速度、更低的延迟和更广泛的连接能力。6G不仅仅是速度的飞跃，它还将引入新的技术和架构，以支持更加智能、互联的世界。第6代移动通信技术（6G）是继5G之后的下一代通信技术，其研究和应用正处于探索阶段。 国内外研究与应用现状总体来看，6G技术的研究和应用展现出了蓬勃的发展态势，不同国家和地区在推动6G技术进步方面各有侧重，共同构建了一个多元化、互联互通的6G研发生态。随着技术的不断突破和国际合作的深入，6G技术的未来将更加光明，预计将为人类社会带来更加广泛和深远的影响。 随着全球对6G技术研究的深入，不同国家和地区在推进6G技术方面展现出了各自的特色和优势。在全球覆盖、频谱效率提升、能量效率优化、智能控制水平增强以及安全性方面，各研究机构和企业正通过前沿技术探索，共同推动6G技术的进步。 国内研究与应用中国在6G的研究和开发方面显著领先，不仅在全球专利申请量上占据重要地位，而且在关键技术研究上取得了突破。中国的研究机构和企业正致力于解决6G技术中的核心问题，如通过新型材料和设计提升信号在高频段的传播效率，开发低功耗设备和算法以提高能量效率，以及利用人工智能技术增强网络的自主管理能力和安全性。此外，中国政府也大力支持6G技术的研发，发布了多项政策和计划，旨在建立完善的6G研发和测试环境，加速6G技术的商业化进程。 国际研究与应用在国际上，芬兰的6G旗舰项目是6G研究的典范，它汇集了学术界和工业界的力量，共同探索6G的未来。该项目不仅关注技术层面的创新，如太赫兹通信、量子通信技术等，还着眼于6G可能带来的社会变革，比如智能交通、智慧城市和数字健康等应用场景。美国、欧盟、日本和韩国等也都在积极推进6G研究，这些地区的研究计划通常强调开放性和国际合作，旨在建立全球统一的6G标准和生态系统。 专利和标准化工作在6G的研究和开发过程中，专利和标准化工作是不可忽视的方面。全球专利申请量的增加反映了国际社会对6G技术的高度重视和活跃的研发态势。中国在专利申请方面的领先地位显示了其在6G技术研发方面的强大动力和潜力。同时，国际电信联盟（ITU）、3GPP等国际标准化组织已开始讨论6G的标准化问题，旨在促进全球6G技术的统一和互操作性，确保6G技术的健康发展和广泛应用。 采用的技术手段6G将使用太赫兹通信技术，这是其最大的优点之一，因为太赫兹频段具有大带宽和高传输速率的特点，非常适合未来高带宽业务场景。此外，6G还将利用卫星通信、无人机通信、地面通信和海上通信，实现”太空-空中-地面-海洋”一体化网络。这些技术手段的结合，将为6G提供更广泛的覆盖范围和更低的成本。 发展方向和挑战6G的发展方向包括智慧连接、深度连接、全息连接和泛在连接。这些方向共同构成了6G总体愿景，即”一念天地，万物随心”。然而，实现这一愿景面临诸多挑战，包括峰值吞吐量、更高能效、随时随地的连接等技术需求。此外，6G极致体验需求挑战以及技术进步，共同驱动了对核心技术研究和国际标准制定的需求。 技术需求挑战 峰值吞吐量：为了支持全息通信等高带宽应用，6G网络需要实现Tbps级别的数据传输速度，这要求开发更高效的信号处理和数据传输技术。 更高能效：随着网络速度的提升和连接设备数量的增加，如何在保证性能的同时降低能耗成为一大挑战，这需要研发新的能效优化技术和低功耗硬件。 随时随地的连接：实现深度连接和泛在连接的愿景，需要克服复杂环境下的信号传播问题，开发新的通信技术和设备。 极致体验需求挑战为了满足用户对于极致体验的需求，6G网络必须提供几乎零延迟的实时响应和高度可靠的连接，这对网络架构和技术实现提出了更高要求。 核心技术研究和国际标准制定实现6G愿景还需要在核心技术上取得突破，包括但不限于太赫兹通信、量子通信、人工智能、分布式网络架构等。同时，为了确保6G技术的全球兼容和互操作性，开展国际标准的制定工作至关重要，需要国际社会的共同努力和协作。 基于5G的应用场景设计基于5G&#x2F;6G技术的卫星互联网一体化安全研究提出了未来5G&#x2F;6G卫星互联网发展及业务应用场景的设计思路。例如，可以设计一个基于5G&#x2F;6G技术的智能交通系统，该系统通过集成卫星通信、无人机通信等技术手段，实现全球范围内的实时交通监控和管理。在这个场景中，5G&#x2F;6G技术能够提供高速的数据传输速率和低时延，确保交通管理系统能够快速响应各种紧急情况，从而提高交通安全性和效率。 6G技术在提升频谱效率和能量效率方面的最新研究进展6G技术在提升频谱效率和能量效率方面的最新研究进展主要体现在以下几个方面： 太赫兹通信技术：太赫兹波段的利用是6G技术发展的一个重要方向。中国航天科工二院二十五所完成了国内首次太赫兹轨道角动量的实时无线传输通信实验，这为6G通信提供了新的技术支持，有助于缓解太赫兹波段频谱资源不足的问题。此外，太赫兹量子级联激光器和单行载流子光电探测器等关键技术的发展，也为6G技术的应用提供了可能。 智能超表面技术：智能超表面技术被用于通信系统中，以增强覆盖并显著提升网络传输速率、信号覆盖以及能量效率。通过对无线传播环境的主动定制，可以根据所需无线功能进行调整，如减小电磁污染等。 超大规模多输入多输出（MIMO）技术：6G研发在超大规模MIMO技术方面取得了重要进展，这一技术能够有效提升频谱效率和网络容量。 通感一体技术和内生AI通信：6G技术的发展还包括了感知和通信的原生集成，无需额外的频谱和网络投资，每个移动终端、每个基站都能实现更高效的通信。同时，内生AI通信技术的应用也是6G研究的一个重点，这些技术的进步有助于提高通信效率和降低能耗。 6G技术在提升频谱效率和能量效率方面的最新研究进展主要包括太赫兹通信技术的应用、智能超表面技术的开发、超大规模MIMO技术的突破以及通感一体技术和内生AI通信技术的发展。这些技术的进步不仅能够提高通信的速度和质量，还能够降低能耗，对于未来的通信网络具有重要意义。 太赫兹通信技术在6G中的具体应用案例太赫兹通信技术在6G中的具体应用案例主要包括以下几个方面： 无人机网络：太赫兹频段被设想用于无人机网络，作为6G非地面网络的预期用例之一。这包括监测和监视等应用场景。 全息通信、微小尺寸通信、超大容量数据回传、短距超高速传输：由于太赫兹频段具有超大带宽的频谱资源，支持超大速率的无线通信，因此在这些场景中有望得到应用。 安全检测：太赫兹波对很多非极性物质有很强的穿透能力，可以进行远距离探测和高分辨率的成像。这表明太赫兹技术在安全检测方面也有潜在的应用价值。 小型化、3D立体成像、太赫兹质谱检测、太赫兹组网、移动性研究：华为6G研究团队将持续推进对太赫兹通感一体化技术的研究和验证，这些研究方向包括小型化、3D立体成像、太赫兹质谱检测、太赫兹组网以及移动性等方面。 无通线超宽带通信场景：太赫兹通信技术将与其他低频段网络融合组网，广泛应用于地面的各种无通线超宽带通信场景，并将搭载卫星、无人机、飞艇等平台，作为替代方案。 太赫兹通信技术在6G中的应用案例涵盖了无人机网络、全息通信、微小尺寸通信、安全检测、小型化研究、3D立体成像、太赫兹质谱检测、太赫兹组网、移动性研究以及无通线超宽带通信等多个领域。 目前全球范围内6G技术研发的主要国家或组织以及主要贡献目前全球范围内6G技术研发的主要国家或组织包括中国、美国、日本等。他们的主要贡献如下： 中国：中国在6G技术研发方面具有全球领先的战略布局和推进组织。自2019年起，中国建立了完善的6G推进组织，由工信部牵头成立中国IMT-2030（6G）推进组，旨在加强国际合作交流和技术研发。此外，江苏紫金山实验室宣布在太赫兹频段取得了全球最快的数据传输速度，标志着中国在6G技术上的重大进展。 美国：美国希望通过建立6G战略路线图、推动相关政策及预算、以及全球推广6G技术和服务来确立其在6G时代的领导地位。美国还通过了三项通信法案，以促进未来网络的发展。 日本：日本是全球率先出台6G国家战略的国家之一。2020年4月和6月，日本相继发布了全球首个以6G作为国家发展目标和倡议的综合战略计划纲要和路线图，提出了具体的实施目标，包括在2025年实现6G关键技术突破、2030年正式启用6G网络等。 这些国家或组织在6G技术研发方面的贡献主要体现在战略规划、技术研发、国际合作以及政策支持等方面。中国通过建立推进组织和取得技术突破展现了其在全球6G技术研发中的领先地位；美国则通过制定通信法案和推动政策预算来争取在6G时代的领导地位；而日本则通过出台国家战略和设定具体目标来推动本国6G技术的发展。 6G技术面临的最大技术挑战以及如何应对6G技术面临的最大技术挑战主要包括网络架构重建、关键应用挖潜、降低功耗等方面。此外，还有毫米波&#x2F;太赫兹通信、空天地融合移动通信技术等前沿原创性技术的加强需求。为了应对这些挑战，目前的研究方向主要集中在加速6G无线、网络、安全等核心技术的产业化，以实现6G网络能力的跨越式发展来服务未来业务需求。同时，注重创新保障、产业支撑、扩大内需、人才保障，支持创新生态构建，形成6G发展的良好环境。此外，6G的研究还着重于探讨更高速率、更低时延、更大规模连接的技术目标，以及可能对6G产生重要影响的关键技术，如太赫兹通信、人工智能等。通过这些研究方向的努力，旨在推动6G技术的发展，以满足未来市场发展趋势和网络与业务需求。 基于5G&#x2F;6G技术的智能交通系统设计思路及其潜在应用场景基于5G&#x2F;6G技术的智能交通系统设计思路主要围绕利用先进的通信技术来提升交通系统的智能化水平，包括但不限于车路协同、自动驾驶、实时数据处理和分析等方面。具体来说，5G技术通过其高速率、低时延和大连接的特点，为智能交通系统提供了强大的支持。例如，5G可以实现智能网联车路协同系统，通过融合人、车、路、站台信息，打造新型智慧交通。此外，5G还能够支持应急联动，确保运行服务的安全。 对于6G技术，虽然目前尚未广泛应用，但其潜在的应用场景包括全功能自动驾驶、高精度定位和高分辨率感知等。6G技术的研究重点在于提供更高速率、更低时延和更大容量的通信服务，这对于实现更高级别的自动驾驶和车联网系统至关重要。例如，6G赋能下的智能车联网系统将覆盖全域感知，这将极大促进智能交通系统的智能化发展。 基于5G&#x2F;6G技术的智能交通系统设计思路主要包括利用5G的高速率、低时延和大连接特性来支持智能网联车路协同系统，以及探索6G技术在全功能自动驾驶、高精度定位和高分辨率感知等方面的应用潜力。这些设计思路不仅能够提升交通系统的效率和安全性，还能够为未来的智能交通系统发展奠定基础。 参考来源1. [PDF] 2023年中国6G产业研究报告 [2023-05-08] 2. 目前 6g 技术在国内外的研究进展如何？ - 知乎 3. 6G：典型应用、关键技术与面临挑战 4. 中兴通讯：6g移动通信网络愿景、挑战与关键技术 - 知乎 5. 基于5G&#x2F;6G技术的卫星互联网一体化安全研究 [2023-07-04] 6. 长篇综述 | 6G：天地无疆，万物无界_澎湃号·湃客_澎湃新闻-The Paper 7. [PDF] 6G 全球进展与发展展望白皮书 - 电子与信息学报 8. 畅想6g时代!业内专家长篇综述：6g最新进展及未来发展趋势 - 知乎 9. 中国信科陈山枝谈6G的三大挑战与三大突破 [2023-07-03] 10. 权威专家解读：5G融合应用走深向实为6G发展夯实应用基础 - 数智 [2023-12-11]"},{"date":"2024-04-22T13:14:48.764Z","url":"/2024/04/22/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E9%85%8D%E7%BD%AE%EF%BC%9AWindows/","categories":[["undefined",""]],"content":"最新最全！吊打付费！这13款才是真正的电脑必装软件！_哔哩哔哩_bilibili vscode 配置 官方文档 win11 配置 node jsNode.js安装及环境配置之Windows 11篇_nodejs安装及环境配置 win11-CSDN博客 C++十六、windows11下VSCode配置C&#x2F;C++编译环境_vscode c语言编译环境-CSDN博客 c_cpp_properties.json launch.json tasks.json C++ 终端闪退的问题C&#x2F;C++ Compile Run 并且选中 run in external terminal 命令行"},{"date":"2024-04-22T13:14:48.764Z","url":"/2024/04/22/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E9%85%8D%E7%BD%AE%EF%BC%9Apython%20%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3/","categories":[["undefined",""]],"content":"vscode 终端不显示 环境  相关工具Anaconda 命令行界面使用 防止不是最新的，更新： conda update -n base -c defaults conda 查看版本： conda --version 管理虚拟环境 active【环境名】：引导用户进入独立的环境。在不加参数的情况下，默认进入base环境。这里不加参数 conda create -n 【环境名】 python&#x3D;【版本号】：创建一个虚拟环境。 conda create -n test_env python=3 (python版本为3(这里conda会自动找python3中最新的版本下载)) conda env list：列出所有环境。验证。 activate 【环境名】：进入环境 activate test_env conda activate 【环境名】：进入（激活）环境 安装 conda install 【包名】 Conda常见命令使用 conda包管理工具需要进入anaconda prompt conda –version，输出anaconda的版本 安装 python Jupyter 版介绍Jupyter Notebook是基于网页的用于交互计算的应用程序。其可被应用于全过程计算：开发、文档编写、运行代码和展示结果。——Jupyter Notebook官方介绍 Jupyter Notebook是以网页的形式打开，可以在网页页面中直接编写代码和运行代码，代码的运行结果也会直接在代码块下显示的程序。如在编程过程中需要编写说明文档，可在同一个页面中直接编写，便于作及时的说明和解释。 安装 python参考：参考： 神器 VS Code，讲讲超详细的 Python 配置使用指南_用vs code开发python需要准备什么-CSDN博客 Anaconda搭建python环境 - 知乎 (zhihu.com) 补充：conda、anaconda概念的差别 conda可以理解为一个工具，也是一个可执行命令，其核心功能是包管理与环境管理。包管理与pip的使用类似，环境管理则允许用户方便地安装不同版本的python并快速切换。 Anaconda是一个打包的集合，里面预装好了conda、某个版本的python、众多packages、科学计算工具等，也称为Python的一种发行版。 Windows安装一、Python安装依照教程，Python 可以从 Python 官网下载安装，也可以使用 Anaconda 来安装。本教程选择了 用 Anaconda。 下载地址 下载完成之后直接按照默认设置一步步安装即可。（挺大的，5G左右，换个合适的目录） 二、Vscode 安装这里好多教程，最好直接官网上下一个。 三、环境配置进行到这一步之后，只是工具的准备完成，但是： win 的 cmd 命令行 不能找到 python 指令 VScode 的 终端都不能识别到 python 指令 打开 Anaconda 界面，在界面中打开 VScode，可以识别 python 指令 环境变量配置 保存 anaconda 路径到系统 “用户变量” 的 path 变量中。保存。此时 Win cmd 中就可以 识别到 conda 指令。 四、VScode插件安装 扩展插件：Python 代码补齐：Pylance JupyterNotebook 支持：Jupyter 高亮缩进：indent-rainbow 括号高亮：Bracket pair Colorizer 注释高亮：Better Comments settings.json Linux Conda 安装Ubuntu安装anaconda,tensorflow等 - 知乎 (zhihu.com) 一、Conda 安装 二、Conda 启动环境变量 三、Conda 修改国内源 四、Conda 进行环境管理环境创建 利用依赖包安装环境 生成依赖包配置 Linux 离线安装 卸载 pythonLinux 使用 python 库离线包安装 打包 使用 安装 CUDA Log in | NVIDIA Developer 安装 tensorflow&#x2F;torch PyTorch: pip install torch TensorFlow: pip install tensorflow Flax: pip install flax 来更新已安装的库： PyTorch: pip install --upgrade torch TensorFlow: pip install --upgrade tensorflow Flax: pip install --upgrade flax 安装 参考 安装 runfile  选择runfile，按照给出来的指令执行： 第一个ACCEPT接受 第二个选择安装的东西 带X的是要安装的，有驱动就把驱动关掉，只安装cuda tool kit即可 然后移动到最下面install 点回车 等待片刻安装好了 环境变量 安装结束 nvcc -V检查安装是否成功，显示版本则成功。 此时就可以执行 安装 CUDNN 深度学习库比较 scikit-learn、PyTorch 和 Keras 的信息： scikit-learn（sklearn）: 是一个流行的 Python 机器学习库，提供了各种经典的机器学习算法和工具。 包含用于分类、回归、聚类、降维、特征选择等任务的算法。 提供了数据预处理、特征工程、模型评估和模型选择的功能。 具有简单且一致的API，易于使用和上手。 适合处理传统机器学习任务，特别是在数据量较小或特征工程较为重要的情况下。 PyTorch: 是一个用于构建和训练神经网络的深度学习库。 提供了动态计算图的能力，使得模型构建和调整更加灵活。 具有丰富的深度学习函数和模块，用于处理张量操作、构建神经网络层、定义损失函数等。 支持自动求导，方便进行反向传播和梯度更新。 提供了灵活的调试和自定义选项，适合深入研究和开发深度学习模型。 Keras: 是一个高级的神经网络 API，用于构建和训练深度学习模型。 提供了简洁、一致的接口，使得模型的原型设计和实验更加快速和便捷。 可以作为 TensorFlow、Theano 和 Microsoft CNTK 等深度学习框架的后端。 支持多种网络结构，包括顺序模型、函数式 API 和子类化 API。 提供了丰富的预处理工具、模型评估和可视化工具。 "},{"date":"2024-04-22T13:14:48.762Z","url":"/2024/04/22/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E9%85%8D%E7%BD%AE%EF%BC%9AVMWare/","categories":[["undefined",""]],"content":"网络配置三种网络模式的区别桥接模式![[Pasted image 20240329091944.png]]（图中的网卡虚拟交换机，实际上是物理主机的）指将虚拟机直接连接到物理网络中，使得虚拟机可以像局域网中的独立主机一样进行通信。 在选择桥接模式时，如果需要使用一个分配的网络账号才能上网（比如公司或学校的网络），虚拟机将无法直接上网。原因在于，虚拟机此时被当作局域网中的另一台电脑，是一台独立主机，而宿主机的网卡则被虚拟化成了一个交换机。因此，虚拟机无法通过桥接模式直接使用分配的网络账号进行上网。 NAT 模式![[Pasted image 20240329092608.png]]NAT（Network Address Translation）模式：用于共享主机的IP地址。 NAT模式能够解决账号上网的问题。在NAT模式下，物理主机充当了代理服务器的角色。当虚拟机需要访问外部网络资源时，首先会向物理主机发送请求，然后物理主机会代替虚拟机去外部网络拉去请求，并将响应转发给虚拟机。 在NAT模式下，虚拟机之间和虚拟机与物理主机之间的通信是受到限制的。因为NAT会将虚拟机的IP地址转换为物理主机的IP地址，所以虚拟机之间和虚拟机与物理主机之间的网络流量都必须经过NAT设备进行转发。 当虚拟机使用NAT模式时，虚拟机会被分配一个私有IP地址，这个地址只在虚拟网络内部有效。而物理主机与外界通信时使用的是公网IP地址。为了将虚拟机与外界联通，需要进行网络地址转换（Network Address Translation, NAT），将虚拟机的私有IP地址转换成物理主机的公网IP地址。当虚拟机与外界通信时，网络流量从虚拟机发出，经过虚拟网卡，进入到NAT设备中。NAT设备会检查目标IP地址，如果是外部网络地址则会直接发送到物理主机接口；如果是内部网络地址，则会将流量重新封装，使用主机的公网IP地址作为源地址，再发送给路由器或互联网上的其他设备。同样的，当虚拟机之间或者虚拟机与物理主机之间通信时，网络流量也需要经过NAT设备。例如，当虚拟机A向虚拟机B发送数据时，数据先会发到NAT设备，被NAT设备重新封装后再发送给虚拟机B。因此，无论是虚拟机之间的通信还是虚拟机与物理主机之间的通信，都需要经过NAT设备进行转发。 另外，虚拟机可以访问外部网络，但外部网络无法直接访问虚拟机，这也是NAT模式的特点之一。 当虚拟机使用NAT模式时，它可以通过物理主机访问外部网络。这是因为虚拟网络地址被转换为物理网络地址，并且所有的网络通信都通过物理主机进行转发。虚拟机发送的网络请求首先到达物理主机，然后物理主机将这些请求发送到外部网络。外部网络接收到请求后，将响应返回给物理主机，再由物理主机转发给相应的虚拟机。然而，外部网络无法直接访问虚拟机。这是因为虚拟机的网络地址是由NAT模式进行转换的，它与物理主机存在不同的网络地址空间。外部网络只能识别并访问物理主机的网络地址，而无法直接访问虚拟机的网络地址。这种限制提供了一层安全性，防止外部网络直接访问到虚拟机。虚拟机被保护在一个相对独立的网络环境中，只能通过物理主机来与外部网络进行通信。这有助于保护虚拟机免受潜在的网络攻击和威胁。 仅主机模式![[Pasted image 20240329102239.png]] 当使用虚拟机进行测试、实验或隔离时，可以选择使用主机模式,这种模式适用于需要隔离、安全性要求较高或仅在单个主机上运行虚拟机的场景。在主机模式下，虚拟机与主机共享一个专用网络，而与外部网络相隔离。 在主机模式下，虚拟机使用的是主机的物理网络接口，而不是虚拟化软件提供的虚拟网络接口。这意味着虚拟机可以通过主机来与外部网络通信，但外部网络无法直接访问虚拟机。 为了实现虚拟机之间的通信，在物理主机上会虚拟出一个虚拟路由器。这个虚拟路由器会创建一个虚拟网卡（例如VMnet1），用于连接所有虚拟机。虚拟网卡负责控制虚拟机之间的信息传递，类似于一个路由器。你可以通过配置虚拟路由器的DHCP地址池，让新建立的虚拟机动态获取到同一网段的IP地址，从而实现虚拟机之间的互相通信。 需要注意的是，虚拟网卡和主机的物理网卡是独立的，它们之间不能直接通信。虚拟网卡只用于虚拟机之间的通信，而主机的物理网卡用于虚拟机与外部网络的通信。 主机模式提供了一种隔离虚拟机和外部网络的方式，可用于进行测试、安全实验等需要隔离环境的场景。但需要注意的是，在主机模式下，虚拟机无法直接与外部网络通信，这也是主机模式在某些场景下的限制之一。 在仅主机模式下，虚拟机可以共享主机的计算资源，例如处理器、内存和存储空间。这允许您在单个物理主机上运行多个虚拟机，并高效地利用计算资源。 虚拟机静态 ip 配置 查看宿主机 ip ![[Pasted image 20240329141220.png]] 打开 Vmware 虚拟网络设置，依次点击”编辑“ – ”虚拟网络编辑器“ ![[Pasted image 20240329142244.png]] 这里选择 NAT 模式 为了能够使用静态IP，这里不要勾选”使用本地DHCP服务将IP分配给虚拟机“这个选项。 然后是配置子网 ip，子网 IP 与宿主机的 ip 一定不能处在同一地址范围里，否则就算虚拟机能上网，网络既慢，还不稳定。我主机的 ip 段是113.54.xxx.xxx，所以我配了113.54.10.xxx 来避开主机的 ip 段，反正 ip 的第三个数字在 0 到 254 并且不是 115 就行。 点”NAT设置”，查看虚拟机的网关。 ![[Pasted image 20240329142326.png]] 配置网关 IP 以匹配子网 IP，同时添加端口转发规则。 进入虚拟机配置网络，vi &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-name 重启网卡服务，执行 service network restart 多网卡路由需求配置如图网络：![[Pasted image 20240329152009.png]]此时，虚拟机的 192 网段之间可以互相 ping 通，但是虚拟机 A 的192 无法 ping 通虚拟机 B 的 113 网段。 解决：配置路由安装工具 添加路由：将虚拟主机 B 的私有地址作为公网地址的网关。 "},{"date":"2024-04-22T13:14:48.761Z","url":"/2024/04/22/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E9%85%8D%E7%BD%AE%EF%BC%9ALinux%20%E7%B3%BB%E7%BB%9F/","categories":[["undefined",""]],"content":"Linux 安装盘安装准备工作 准备一个空 U 盘，作为后续安装盘。 下载 rufus U 盘 格式化软件，分区类型：选择GPT，目标系统类型UEFI（非SM） 下载地址 下载 Ubuntu iso 镜像 Ubuntu22.04.4 使用 rufus 对安装盘进行制作，配置如下 ![[Pasted image 20240406101639.png]] 磁盘分区方式：MBR和GPT，MBR的主分区只能划分最多4个，GPT的分区数量没有限制，这里选择 GPT。 开始安装 开启服务器主机，进入系统 BIOS（Del键、F2键、F10键或者ESC键，依据主板型号，或者在开机界面一般会显示具体按键） 选择 UEFI U 盘作为启动盘。保存并重启系统。 Secure Boot设置：Security —&gt; Secure Boot —&gt; Disabled Boot Option设置：Boot —&gt; Boot Option #1 —&gt; 将 UEFI USB Key: **** 选择至第一位 保存BIOS设置：Exit—&gt;Save Changes and Reset 大部分默认设置。 网络 IP 使用默认？ 安装源： 磁盘分区：参考 SWAP 分区：Linux 的虚拟内存，建议将交换分区的大小设置为物理内存的1.5倍到2倍。 &#x2F;boot 分区：引导分区，建议 300-500M ； 【关于是否要分&#x2F;boot分区的问题：如果你的主板bios里设置的是UEFI+GPT分区表模式，那么给ubuntu分区的时候不用设置这个&#x2F;boot分区，设置下面第3步的efi系统分区即可； 但如果你用的是legacy+MBR分区表那就正常设置&#x2F;boot分区，这个很重要，特别是装双系统或多系统时，避免破坏到其他系统的引导文件】 EFI引导分区，类型为逻辑分区，，默认ext4。 推荐分 512 ~ 1024M，注意：放在空间起始位置。 &#x2F;var 分区（可选）：log 的日志文件存放，如果不分则默认在 &#x2F; 下。如果 linux 用于服务器和经常做日志分析，建议划分。最少 300-500M，一般 2-3 G。 根分区（&#x2F;）：存储操作系统文件、应用程序和用户数据。建议较大，15G+； home 分区：存放个人的文件，建议最大；（&#x2F; 和 &#x2F;home 之间类似 C 和 D 盘的关系） Linux 系统配置root 账户设置密码当前普通用户界面下输入命令，然后按提示两次输入密码即可。 新增用户并且增加用户权限 useradd：用于创建新用户账号。 其中，username 是要创建的新用户的用户名。该命令会创建一个新的用户账号，并分配一个默认的用户ID和组ID。 passwd：用于设置用户的密码。 其中，username 是要设置密码的用户的用户名。该命令会提示你输入新的密码，并要求确认密码。 usermod：用于修改用户的属性，包括用户权限。 其中，groupname 是要将用户添加到的用户组的组名，username 是要修改权限的用户的用户名。该命令会将指定用户添加到指定用户组中，从而增加用户的权限。 注意：在执行 usermod 命令时，使用 -aG 选项是为了将用户添加到用户组而不覆盖用户原有的用户组。 重新登录：在完成上述步骤后，建议重新登录用户账号，以使用户权限的更改生效。 源配置1. 首先备份官方自带的软件源cp /etc/apt/sources.list /etc/apt/sources.list.bak 2.然后编辑修改为清华源sudo gedit sources.list ssh配置 PermitRootLogin 是一个用于配置 SSH 服务器的选项。这个选项决定了是否允许 root 用户通过 SSH 直接登录到服务器。通常情况下，为了提高安全性，最好禁止 root 用户通过 SSH 直接登录，而是使用一个普通用户登录后再通过 su 或者 sudo 切换到 root 用户来执行需要特权的操作。这样可以降低系统受到攻击的风险。常见的 PermitRootLogin 选项取值包括： yes：允许 root 用户通过 SSH 直接登录。 no：禁止 root 用户通过 SSH 直接登录。 without-password：允许 root 用户通过 SSH 密钥登录，但不允许使用密码登录。 按i进入编辑模式，找到#PermitRootLogin prohibit-password，默认是注释掉的。把 PermitRootLogin without-password 改为 PermitRootLogin yes，注意PermitRootLogin without-password被注释掉了，要去掉注释。如果没有找到PermitRootLogin without-password，直接文件末尾添加PermitRootLogin yes即可。然后按esc，输入:wq保存并退出。重启sshd服务 驱动更新 换源 网络配置 打开终端，使用root权限登录或者使用sudo命令获取root权限。 使用文本编辑器打开网络配置文件/etc/netplan/00-installer-config.yaml。 在文件中，找到network部分，然后根据你的网络设置进行编辑。以下是一个示例配置： enp0s3是网络接口的名称，你需要根据你的实际网络接口名称进行替换。 addresses是你的服务器的静态IP地址和子网掩码。 gateway4是你的网关IP地址。 nameservers是DNS服务器的IP地址。 保存文件并关闭文本编辑器。 在终端中执行以下命令以应用配置更改： 重新启动网络服务以使更改生效： 现在，你的 Ubuntu Server 已经配置了静态网络。你可以通过 ping 命令或打开浏览器测试网络连接。 禁止自动休眠 执行如下命令查看休眠模式的情况，如果 sleep 状态是loaded，也就是处于自动休眠开启状态 接下来，执行如下命令关闭系统的自动休眠开关： 再次执行查看命令，可以看到 sleep 的状态已经变成了 masked，也就是关闭了。 编辑/etc/systemd/logind.conf文件： 在文件中找到以下行： 将这两行的注释符号#移除，并设置IdleAction为ignore，IdleActionSec为0，如下所示： 保存文件并关闭编辑器。 使用以下命令重新加载systemd配置： 完成！现在 Ubuntu Server 将不会因为无操作而自动休眠。 设置关盖&#x2F;合盖不挂起&#x2F;不睡眠 通过更改登录配置文件logind.conf设置 里面影响关盖操作的变量包括HandleLidSwitch、HandleLidSwitchExternalPower和HandleLidSwitchDocked。 logind.conf 里面各项的含义如下： HandleLidSwitch: 定义笔记本电脑关闭盖子时的行为。可以设置的值有： ignore：忽略关闭盖子的事件。 poweroff：关闭电源。 reboot：重新启动。 halt：停止系统。 kexec：通过 kexec 进行快速启动。 suspend：挂起系统。 hibernate：休眠系统。 HandleLidSwitchExternalPower: 定义当笔记本电脑连接外部电源时关闭盖子的行为。可以设置的值与 HandleLidSwitch 相同。 HandleLidSwitchDocked: 定义当笔记本电脑连接到底座时关闭盖子的行为。可以设置的值与 HandleLidSwitch 相同。 HandlePowerKey: 定义电源按钮的行为。可以设置的值与 HandleLidSwitch 相同。 IdleAction: 定义系统空闲时的行为。可以设置的值有： ignore：忽略空闲事件。 poweroff：关闭电源。 reboot：重新启动。 halt：停止系统。 kexec：通过 kexec 进行快速启动。 suspend：挂起系统。 hibernate：休眠系统。 IdleActionSec: 定义系统空闲多少秒后触发 IdleAction。 UserTasksMax: 定义每个用户可以同时运行的任务数的最大值。 KillUserProcesses: 定义当用户注销时是否终止用户的进程。可以设置的值有： yes：终止用户的进程。 no：不终止用户的进程。 定时自动关闭显示器 想要定时关闭显示器，可以编辑/etc/default/grub这个文件 通过修改GRUB_CMDLINE_LINUX_DEFAULT变量，你可以添加或修改传递给内核的参数，以便在启动时对系统进行自定义配置。 修改完了运行sudo update-grub，然后重启，即可设置系统5分钟没有活动自动关闭显示器。 网络自动认证 将软件放在 &#x2F;root&#x2F;path&#x2F;uestc-network-login&#x2F;uestc_wifi_helper_cli-x86_64-linux 将脚本放在 &#x2F;root&#x2F;boot&#x2F;auto-login-uestc-network.sh 部署服务在 &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;auto-network-login.service 启动服务。 外接 U 盘挂载 确认 U 盘被正确地检测到：在终端中输入以下命令来查看系统识别的存储设备列表： 这将显示所有已连接的存储设备，包括 U 盘。通常，U 盘的设备名称类似于 /dev/sdb 或 /dev/sdc。 挂载 U 盘：在 Linux 中，需要将 U 盘挂载到文件系统才能访问其中的文件。首先，创建一个用于挂载的目录。在终端中输入以下命令： 这将创建一个名为 /mnt/usb 的目录作为挂载点。 挂载 U 盘：使用 mount 命令将 U 盘挂载到刚刚创建的目录。假设 U 盘的设备名称为 /dev/sdb1，在终端中输入以下命令： 如果 U 盘有多个分区，将 /dev/sdb1 替换为相应的分区设备名称。 访问 U 盘中的文件：现在，可以通过访问挂载点 /mnt/usb 来查看和操作 U 盘中的文件。你可以使用文件管理器（如 Nautilus、Dolphin 或 Thunar）浏览 U 盘，也可以在终端中使用命令行来执行相关操作。 卸载 U 盘：在完成对 U 盘的操作后，应该将其卸载以安全地移除。在终端中输入以下命令来卸载 U 盘： 确保在操作完成后再拔出 U 盘。 请注意，U 盘的设备名称和挂载点可能会根据系统和具体情况有所不同。因此，在使用上述命令时，请根据你的实际情况进行相应的调整。 域名配置在 Ubuntu 22.04 TLS 上进行动态 DNS 部署的步骤：0. 注册 DNS 服务商账号 ZoneEdit - Control Panel: Sign Up 安装 ddclient：打开终端并执行以下命令来安装 ddclient： 配置 ddclient：ddclient 的配置文件位于 /etc/ddclient.conf。使用你喜欢的文本编辑器（如 nano）打开该文件： 在配置文件中，你需要提供动态 DNS 服务提供商的相关信息，例如域名、用户名、密码等。具体配置取决于你选择的动态 DNS 服务提供商。以下是一个示例配置文件的部分内容： 替换示例配置中的 protocol、server、login、password 和 your.domain.com 为你的实际设置。请参考你选择的动态 DNS 服务提供商的文档，获取正确的配置参数。 保存并关闭配置文件。 启用并启动 ddclient 服务：执行以下命令启用 ddclient 服务： 然后，执行以下命令启动服务： ddclient 将会读取配置文件并在启动时或 IP 地址变化时更新动态 DNS 记录。 验证配置：执行以下命令来检查 ddclient 服务的状态： 如果状态显示为 “active”，则表示 ddclient 服务已成功启动并正在运行。 查看详细信息 注意域名的更新在域名服务器之间可能需要一个小时左右，需要等待。内网穿透：多种方式解决：如何映射我的私人IP动态变化到我的VPS IP？ - adsl - 码客 (oomake.com) 网络高级配置双网卡跨网卡 iptables 完成数据转发目标：将主机的一个 ip 的发送包，转换为主机另一个 ip 的发送包。 配置网卡IP地址， 配置静态IP地址， &#x2F;etc&#x2F;sysconfig&#x2F;network-stripts&#x2F;ifcfg-eth0 配置网卡一， 外网卡， 可以联网 &#x2F;etc&#x2F;sysconfig&#x2F;network-stripts&#x2F;ifcfg-eth1 配置网卡二， 内网卡， 连接内网 配置网卡配置文件 DEVICE&#x3D;eth1 IPADDR&#x3D;192.168.145.122&#x2F;&#x2F; IP地址 NETMASK&#x3D;255.255.255.0&#x2F;&#x2F; IP掩码 gateway&#x3D;192.168.142.2&#x2F;&#x2F; 外网网关地址 ONBOOT&#x3D;yes 或者配置&#x2F;etc&#x2F;sysconfig&#x2F;network 添加网关地址 gateway&#x3D;192.168.142.2 &#x2F;&#x2F; 外网网关地址 打开包转发功能： echo “1” &gt; &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_forward 修改&#x2F;etc&#x2F;sysctl.conf文件， 让包转发功能在系统启动时自动生效： net.ipv4.ip_forward &#x3D; 1 添加iptables nat路由规则： &#x2F;&#x2F; 清楚其他不需要的iptables规则 &#x2F;sbin&#x2F;iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE&#x2F;&#x2F; 其中网卡eth0 为外网卡。 保存规则到&#x2F;etc&#x2F;sysconfig&#x2F;iptables 配置文件中 service iptables save 重新启动iptables service iptables restart 1、首先确保转发开关打开 2、核对双网卡配置 3、执行 iptables 并保存 局域网自动认证 TODO对于学校或者公司，每次主机重启可能需要进行账号的认证，才可上网，这里实现一个脚本进行自动登录 curl -d “username&#x3D;202321081216&amp;password&#x3D;feng0408”  程序自启动在 Ubuntu 中，你可以使用 Systemd 来配置在系统启动时自动后台执行可执行程序。 下面是一些基本的步骤： 创建一个用于 Systemd 配置的服务文件。在终端中使用以下命令创建一个新的服务文件： 在打开的编辑器中，输入以下内容： 请确保将 /path/to/test 替换为你的可执行程序的实际路径，并将 your_username 替换为你的用户名。 保存并关闭文件。使用 Ctrl + X，然后输入 Y 保存文件并退出编辑器。 重新加载 Systemd 配置，使其识别新的服务文件。在终端中使用以下命令： 启用服务，使其在系统启动时自动运行。在终端中使用以下命令： 最后，启动服务。在终端中使用以下命令： 现在，你的可执行程序 test 将在系统启动时自动以后台进程的形式运行。 你可以使用以下命令来检查服务的状态： 如果需要停止服务，可以使用以下命令： 后台运行程序 在执行命令时，可以使用 &amp; 符号将其放在命令的末尾。这样会将命令放入后台运行，并且不会阻塞当前终端。例如： 如果你希望程序在后台运行的同时不产生任何输出，你可以将其输出重定向到 /dev/null，这样输出将被丢弃。例如： 这样，可执行程序 ./test 将在后台运行，并与当前终端解耦。它不会阻塞当前终端，并且输出将被重定向到 /dev/null，不会显示在终端上。 请注意，使用这种方法运行程序后，你将无法看到程序的输出。如果你需要查看程序的输出或进行其他交互操作，可能需要使用其他工具或技术来实现。 离线安装包的使用Centos 打包 使用 查询指定域名的 IP 日志查看 查看显存运行情况"},{"date":"2024-04-22T13:14:48.760Z","url":"/2024/04/22/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E6%95%99%E7%A8%8B%EF%BC%9AWandb/","categories":[["undefined",""]],"content":"Wandb（全称为Weights and Biases）是一个专注于机器学习项目的工具，旨在帮助用户跟踪、可视化和协作。它提供了实时指标跟踪、超参数调整、模型可视化等功能，使机器学习工程师能够更好地管理和监控他们的实验，从而获得更多的见解和知识。Wandb支持与Jupyter、TensorFlow等工具的集成，能够自动记录模型训练过程中的超参数和输出指标，并进行可视化和比较，便于快速共享结果 使用Wandb的基本步骤包括：首先，在W&amp;B网站上创建一个免费账户 ；然后，安装Wandb库并登录 ；在代码中导入Wandb并初始化，定义要监视的变量，如超参数、loss和Metric ；接着，记录训练日志（包括图片和文字）、保存模型，并在训练过程中获取权重直方图等 。此外，Wandb还提供了丰富的用户管理和团队管理功能，方便团队协作 总结来说，Wandb是一个强大的机器学习项目管理工具，通过其提供的功能，可以帮助研究人员和开发者更有效地进行实验设计、数据可视化和结果分析。通过简单的注册、安装和初始化过程，即可开始使用Wandb来提升机器学习项目的效率和质量。0c95d54f6822155600329ac8870848a84191a0ca"},{"date":"2024-04-22T13:14:48.760Z","url":"/2024/04/22/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E6%95%99%E7%A8%8B%EF%BC%9Agit/","categories":[["undefined",""]],"content":"git 安装Win 下载 Linux git 配置 生成公钥 获取公钥 配置公钥复制公钥到 gitee｜githbu 网页个人设置中的公钥之中；保存。 终端环境配置 vscode 用一个远程库初始化一个本次仓库ctrl+shift+p initialize repository:远程链接源代码管理库-》远程-》远程仓库-》粘贴.git 链接-》ok git 使用远程仓库管理git remote -v:列出所有远程仓库的别名和地址 git remote show &lt;远程仓库别名&gt;:查看远程仓库的详细信息 git remote add &lt;别名&gt; &lt;仓库地址&gt;:添加一个新的远程仓库 git remote remove &lt;别名&gt;:删除一个远程仓库 git remote rename &lt;原别名&gt; &lt;新别名&gt;:重命名一个远程仓库 git remote set-url &lt;别名&gt; &lt;新地址&gt;:修改某个远程仓库的地址 git remote set-head &lt;别名&gt; &lt;分支名&gt;:修改远程仓库的默认分支 git remote -v update:更新所有远程仓库 git remote prune &lt;别名&gt;:移除远程上已删除的tracking分支 git remote get-url &lt;别名&gt;:查看远程仓库的原始克隆URL 查看跟踪远程分支git branch -vv (当前分支：hash：远程跟踪分支：注释) 修改跟踪的远程分支git branch –set-upstream-to&#x3D;origin&#x2F;remote-branch local-branch 这将为本地分支 local-branch 设置远程跟踪分支为 origin/remote-branch。 你也可以使用简化命令来设置远程跟踪分支： git branch -u origin&#x2F;remote-branch local-branch （这个命令用于现有本地分支，将其与远程分支相关联） 创建新分支git branch origin&#x2F;serverfix 切换本地分支git checkout master 这里的master应该就是本地的分支 此外： git checkout -b master origin&#x2F;master 表示切换的分支是新建的（本地只有main，没有master），分支名：master； 来源：orgin&#x2F;master; 确定要删除的分支，并运行以下命令来删除该分支（假设要删除的分支名为 branch-name）： git branch -d branch-name 如果分支上有未合并的更改，Git 会拒绝删除该分支，并显示一条警告消息。如果确实要强制删除分支，可以使用 -D 选项： git branch -D branch-name 拉取源代码管理-》拉取 或者 pull 就行 git pull [远程仓库名] [远程分支] 提交手动 添加消息-》提交-》同步 命令提交 暂存：git add * 提交：使用与当前分支关联的远程仓库和同名的远程分支： git pushgit push xinka master git push [&lt;远程仓库名&gt;] [&lt;本地分支名&gt;:]&lt;远程分支名&gt; 强制将当前分支 覆盖掉origin 的 master 分支：git push origin +current-branch:master （下面几个命令可以用来解决：refuse to merge unrelated histories） 获取分支最新提交hashgit log -1 main 从远程仓库获取最新的提交和分支信息，但不会自动将这些更改合并到你的本地分支中： git fetch &lt;远程仓库名&gt; 使用其他命令（如git merge或git rebase）将这些更改合并到你的本地分支中。 git mergegit merge –strategy&#x3D;ours 这将执行合并操作，但会忽略当前分支的更改，选择其他分支（&lt;branch-to-merge&gt;）的内容。冲突会被自动解决为其他分支的更改。 放弃本地分支，重置到master分支的最新提交 git fetch git reset –hard 或者 git reset –hard origin&#x2F;master 另一种解释：放弃本地更改： git reset –hard FETCH_HEAD，其中FETCH_HEAD表示上一次成功pull之后的commit点，然后git pull即可 强制推送当前分支到远程分支git push -f xinka master 这样就丢掉了 master 原来的历史。这样可以快速建立分支之间的关系，但是原有分支的历史提交会丢失。需要根据实际情况权衡是否使用这种破坏性的重置方式。上面可能会出现：error: src refspec localagent does not match any ，此时，试试下面的操作。 git fetchgit push -f orign HEAD:&lt;远程分支名&gt; 补充git pull 后 仓库 在本地的默认名称 就是 origin 查看：git remote -v 修改：git remote rename old new 比如： git push origin master 表示将本地 推到 master 永久修改默认名称：git config –global init.defaultBranch new_name; 强制覆盖本地分支 强制覆盖远程分支git push [&lt;远程仓库名&gt;] [&lt;本地分支名&gt;:]&lt;远程分支名&gt; 每次提交之后，必须点击：同步更改才能看到网页的变化。 问题解决connect reset 问题，直接通过命令行配置解决git config –global http.sslBackend “openssl” git config –global http.sslCAInfo “C:\\Program Files\\Git\\mingw64\\ssl\\cert.pem” git remotegit remote [-v | –verbose] 显示所有远程仓库的名称，以及它们的 URL。示例：git remote -vgit remote add [-t ] [-m ] [-f] [–tags | –no-tags] [–mirror&#x3D;&lt;fetch|push&gt;] 添加一个新的远程仓库。示例：git remote add origin  remote rename [–[no-]progress] 重命名一个远程仓库。示例：git remote rename origin upstreamgit remote remove 移除指定名称的远程仓库。示例：git remote remove origingit remote set-head (-a | –auto | -d | –delete | ) 设置指定远程仓库的 HEAD 引用。示例：git remote set-head origin mastergit remote show [-n] 显示指定远程仓库的信息，包括分支和跟踪情况。示例：git remote show origingit remote prune [-n | –dry-run] 移除没有对应本地分支的指定远程仓库分支的引用。示例：git remote prune origingit remote update [-p | –prune] [( | )…] 更新指定的远程仓库的引用。示例：git remote update origingit remote set-branches [–add] … 设置指定远程仓库应该跟踪的分支。示例：git remote set-branches origin maingit remote get-url [–push] [–all] 获取指定远程仓库的 URL。示例：git remote get-url origingit remote set-url [–push] [] 设置指定远程仓库的 URL。示例：git remote set-url origin  remote set-url –add 添加一个新的 URL 到指定远程仓库。示例：git remote set-url –add origin  remote set-url –delete 从指定远程仓库中删除指定的 URL。示例：git remote set-url –delete origin  创建一个新的分支来保留你所做的提交git switch -c :这将在当前提交处创建一个新的分支，并切换到该分支，让你可以在该分支上继续工作 如果放弃处于分离头指针状态下的更改，执行：git switch - ：将切换回进入分离头指针状态之前所在的分支 将一个本地仓库 提交到另一个本地仓库跟踪的远程分支确保你当前在本地分支 pv 上。可以使用以下命令检查当前所在的分支： git branch确保 pv 分支前面有 * 标记。 首先，将本地分支 pv 推送到远程的 private&#x2F;master 分支。执行以下命令： git push private pv:master这将把本地分支 pv 推送到远程仓库的 private&#x2F;master 分支。 接下来，切换到本地分支 pv 并合并远程的 public&#x2F;master 分支。执行以下命令： git checkout pvgit merge public&#x2F;master这将切换到分支 pv 并将远程的 public&#x2F;master 分支合并到本地分支。 最后，将合并后的更改推送到远程的 public&#x2F;master 分支。执行以下命令： git push public pv:master这将把本地分支 pv 推送到远程仓库的 public&#x2F;master 分支。"},{"date":"2024-04-22T13:14:48.758Z","url":"/2024/04/22/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E6%95%99%E7%A8%8B%EF%BC%9ARime%20%E8%BE%93%E5%85%A5%E6%B3%95/","categories":[["undefined",""]],"content":"_when_：有几种状态 composing、has_menu、paging _accept_：控制接受的按键 minus、equal,、period、comma、bracketleft、bracketright _send_：控制动作 Page_Up、Page_Down、Escape(清空输入码) # # 快捷键# - &#123; when: has_menu, accept: semicolon, send: 2 &#125; # &quot;:&quot; (分号)选择第 2 个候选词# - &#123; when: has_menu, accept: apostrophe, send: 3 &#125; # &quot;&#39;&quot; (引号)选择第 3 个候选词# - &#123; when: composing, accept: Shift+Tab, send: Shift+Left &#125; # &quot;Shift+Tab&quot; 键向左选拼音分词# - &#123; when: composing, accept: Control+a, send: Home &#125; # &quot;Control+a&quot; 光标移至首# - &#123; when: composing, accept: Control+e, send: End &#125; # &quot;Control+e&quot; 光标移至尾# - &#123; when: composing, accept: Control+g, send: Escape &#125; # &quot;Control+g&quot; 清码# - &#123; when: composing, accept: Return, send: Escape &#125; # &quot;Return&quot; 回车清码# - &#123; when: always, accept: Control+Shift+1, select: .next &#125; # 切换输入方案# - &#123; when: always, accept: Control+Shift+2, toggle: ascii_mode &#125; # 中/英文切换# - &#123; when: always, accept: Control+Shift+3, toggle: full_shape &#125; # 全角/半角切换# - &#123; when: always, accept: Control+Shift+4, toggle: simplification &#125; # 繁简体切换# - &#123; when: always, accept: Control+Shift+5, toggle: extended_charset &#125; # 通用/增广切换（显示生僻字）# - &#123; when: composing, accept: Control+b, send: Left &#125; # &quot;Control+b&quot; 移动光标# - &#123; when: composing, accept: Control+f, send: Right &#125; # &quot;Control+f&quot; 向右选择候选词# - &#123; when: composing, accept: Control+h, send: BackSpace &#125; # &quot;Control+h&quot; 删除输入码 "},{"date":"2024-04-22T13:14:48.756Z","url":"/2024/04/22/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E6%95%99%E7%A8%8B%EF%BC%9AObsidian/","categories":[["undefined",""]],"content":"This is your new vault. Make a note of something, , or try the Importer! When you’re ready, delete this note and make the vault your own. 使用方式设置和使用教程笔记结构数据库 数据模性 关系模型的数据结构关系模型的完整性约束 关系模型： B+的主要完成函数， ····代码 实现流程 说完了流程，这里不展现具体代码细节，举出例子加以理解"},{"date":"2024-04-22T13:14:48.756Z","url":"/2024/04/22/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E6%95%99%E7%A8%8B%EF%BC%9AMD%20%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E8%AF%AD%E6%B3%95/","categories":[["undefined",""]],"content":"Markdown数学公式语法 - 简书 (jianshu.com) 一、简单分类1. 行内公式将公式插入到本行内，符号：$公式内容$，如：$xyz$ 2. 行间公式将公式插入到新的一行内，并且居中，符号：$公式内容$，如：$$xyz$$ 二、上标、下标与组合1. 上标符号，符号：^，如：$x^4$ 2. 下标符号，符号：_，如：$x_1$ 3. 组合符号，符号：&#123;&#125;，如：默认情况下，上、下标符号仅仅对下一个组起作用。一个组即单个字符或者使用{}（大括号） 包裹起来的内容。如果使用$10^10$表示的是,而$10^&#123;10&#125;$ 才可以表示为。同时，大括号还能消除二义性，如：$x^5^6$ 将得到一个错误，必须使用大括号来界定^的结合性，如:$&#123;x^5&#125;^6$表示的：或者用$x^&#123;5^6&#125;$表示的。 三、括号1.小括号与方括号用原始的( ) ，[ ] 即可，如(2+3)[4+4]可表示：。使用\\left(或\\right)使符号大小与邻近的公式相适应（该语句适用于所有括号类型），如\\left(\\frac{x}{y}\\right)可表示 2. 大括号由于大括号{} 被用于分组，因此需要使用{和}表示大括号，也可以使用\\lbrace 和\\rbrace来表示。如{ab}或\\lbrace ab\\rbrace表示: 3. 尖括号区分于小于号和大于号，使用\\langle 和\\rangle 表示左尖括号和右尖括号。如\\langle x \\rangle表示： 三、取整1. 上取整使用\\lceil 和 \\rceil 表示。 如，\\lceil x \\rceil表示为： 2. 下取整使用\\lfloor 和 \\rfloor 表示。如，\\lfloor x \\rfloor表示为: 四、求和\\积分\\连乘1.求和\\sum 用来表示求和符号，其下标表示求和下限，上标表示上限。如:$\\sum_&#123;r=1&#125;^n$表示： 2. 积分\\int 用来表示积分符号，同样地，其上下标表示积分的上下限。如，$\\int_&#123;r=1&#125;^\\infty$表示:多重积分同样使用\\int ，通过 i 的数量表示积分导数：如：$\\iint$ 表示为：$\\iiint$ 表示为： 3. 连乘$\\prod &#123;a+b&#125;$ 表示：$\\prod_&#123;i=1&#125;^&#123;K&#125;$ 表示：$$\\prod_&#123;i=1&#125;^&#123;K&#125;$$表示（注意是行间公式）： 4. 其他与此类似的符号还有，$\\prod$ ：$\\bigcup$：$\\bigcap$ ：$arg\\,\\max_&#123;c_k&#125;$：$arg\\,\\min_&#123;c_k&#125;$：$\\mathop &#123;argmin&#125;_&#123;c_k&#125;$：$\\mathop &#123;argmax&#125;_&#123;c_k&#125;$：$\\max_&#123;c_k&#125;$：$\\min_&#123;c_k&#125;$： 五、分式与根式1. 分式第一种，使用\\frac ab，表示为: ，\\frac作用于其后的两个组a ，b ，结果为。如果你的分子或分母不是单个字符，请使用{…}来分组，比如$\\frac &#123;a+c+1&#125;&#123;b+c+2&#125;$表示:第二种，使用\\over来分隔一个组的前后两部分，如$&#123;a+1\\over b+1&#125;$： 2. 连分数书写连分数表达式时，请使用\\cfrac代替\\frac或者\\over两者效果对比如下：\\frac 表示如下： 显示如下：\\cfrac表示如下： 显示如下： 3.根式根式使用\\sqrt 来表示。如开4次方：$\\sqrt[4]&#123;\\frac xy&#125;$ 可表示：开平方：$\\sqrt &#123;a+b&#125;$可表示： 六、多行表达式1. 分类表达式定义函数的时候经常需要分情况给出表达式，使用\\begin{cases}…\\end{cases} 。其中：使用\\\\ 来分类，使用&amp;指示需要对齐的位置，使用\\ +space表示空格。如： 表示:以及: 表示: 如果想分类之间的垂直间隔变大，可以使用\\\\[2ex]代替\\\\来分隔不同的情况。(3ex,4ex 也可以用，1ex相当于原始距离）。如下所示： 表示： 2. 多行表达式有时候需要将一行公式分多行进行显示。 表示: 表示: 其中begin&#123;equation&#125; 表示开始方程，end&#123;equation&#125;表示方程结束；begin&#123;split&#125; 表示开始多行公式，end&#123;split&#125; 表示结束；公式中用\\\\ 表示回车到下一行，&amp; 表示对齐的位置。 七、方程组使用\\begin{array}…\\end{array} 与\\left \\与\\right 配合表示方程组,如: 表示： 注意：通常MathJax通过内部策略自己管理公式内部的空间，因此a…b 与a…….b（.表示空格）都会显示为ab。可以通过在ab间加入\\,增加些许间隙，\\;增加较宽的间隙，\\quad 与\\qquad 会增加更大的间隙。 八、特殊函数与符号1. 三角函数$\\sin x$ : $\\arctan x$ : 2.比较运算符小于(\\lt )：大于(\\gt )：小于等于(\\le )：大于等于(\\ge )：不等于(\\ne ) :可以在这些运算符前面加上\\not ，如\\not\\lt : 3.集合关系与运算并集(\\cup): 交集(\\cap): 差集(\\setminus):子集(\\subset): 子集(\\subseteq): 非子集(\\subsetneq): 父集(\\supset): 属于(\\in): 不属于(\\notin):![(\\notin)]((%5Cnotin)) 空集(\\emptyset): ![(\\emptyset)]((%5Cemptyset)) 空(\\varnothing)&#96;: 4. 排列\\binom&#123;n+1&#125;&#123;2k&#125; : &#123;n+1 \\choose 2k&#125; : 5. 箭头(\\to):(\\rightarrow): (\\leftarrow): (\\Rightarrow):![(\\Rightarrow)]((%5CRightarrow)) (\\Leftarrow): ![(\\Leftarrow)]((%5CLeftarrow)) (\\mapsto)&#96;: ) 6. 逻辑运算符(\\land): (\\lor): (\\lnot): (\\forall): (\\exists): (\\top): (\\bot): (\\vdash): (\\vDash): 7.操作符(\\star): ![(\\star)]((%5Cstar)) (\\ast): ![(\\ast)]((%5Cast)) (\\oplus): ![(\\oplus)]((%5Coplus)) (\\circ): ![(\\circ)]((%5Ccirc)) (\\bullet)&#96;: 8.等于(\\approx):(\\sim): (\\equiv): (\\prec): 9.范围(\\infty):(\\aleph_o):(\\nabla): (\\Im): (\\Re): 10. 模运算(\\pmod): ![&#96;(\\pmod)]((%5Cpmod))如a \\equiv b \\pmod n 表示为: 11. 点(\\ldots): (\\cdots): (\\cdot): 其区别是点的位置不同，\\ldots 位置稍低，\\cdots 位置居中。 表示(注意两部分点的位置)： 12.顶部符号对于单字符，\\hat x：多字符可以使用\\widehat &#123;xy&#125;：类似的还有\\overline x: 矢量\\vec x:向量\\overrightarrow &#123;xy&#125;: \\dot x : \\ddot x: \\dot &#123;\\dot x&#125;: 九、表格使用\\begin&#123;array&#125;&#123;列样式&#125;…\\end&#123;array&#125;这样的形式来创建表格，列样式可以是clr 表示居中，左，右对齐，还可以使用|表示一条竖线。表格中各行使用\\ 分隔，各列使用&amp; 分隔。使用\\hline 在本行前加入一条直线。 例如: 得到： 十、汉字、字体与格式 汉字形式，符号：\\mbox&#123;&#125;，如： 字体控制，符号：\\displaystyle，如： 下划线符号，符号：\\underline，如： 标签，符号\\tag&#123;数字&#125;，如： 上大括号，符号：\\overbrace&#123;算式&#125;，如： 下大括号，符号：\\underbrace&#123;算式&#125;，如： 上位符号，符号：\\stacrel&#123;上位符号&#125;&#123;基位符号&#125;，如： 十一、占位符 两个quad空格，符号：\\qquad，如： quad空格，符号：\\quad，如： 大空格，符号\\，如： 中空格，符号\\:，如： 小空格，符号\\,，如： 没有空格，符号&#96;&#96;，如： 紧贴，符号\\!，如： 十二、定界符与组合 括号，符号：（）\\big(\\big) \\Big(\\Big) \\bigg(\\bigg) \\Bigg(\\Bigg)，如： 中括号，符号：[]，如： 大括号，符号：\\&#123; \\&#125;，如： 自适应括号，符号：\\left \\right，如：$\\left(x\\right)$， 组合公式，符号：&#123;上位公式 \\choose 下位公式&#125;，如： 组合公式，符号：&#123;上位公式 \\atop 下位公式&#125;，如： 十三、四则运算 加法运算，符号：+，如： 减法运算，符号：-，如： 加减运算，符号：\\pm，如： 减甲运算，符号：\\mp，如： 乘法运算，符号：\\times，如： 点乘运算，符号：\\cdot，如： 星乘运算，符号：\\ast，如： 除法运算，符号：\\div，如： 斜法运算，符号：/，如： 分式表示，符号：\\frac&#123;分子&#125;&#123;分母&#125;，如： 分式表示，符号：&#123;分子&#125; \\voer &#123;分母&#125;，如： 绝对值表示，符号：||，如： 十四、高级运算 平均数运算，符号：\\overline&#123;算式&#125;，如： 开二次方运算，符号：\\sqrt，如： 开方运算，符号：\\sqrt[开方数]&#123;被开方数&#125;，如： 对数运算，符号：\\log，如： 极限运算，符号：\\lim，如： 极限运算，符号：\\displaystyle \\lim，如： 求和运算，符号：\\sum，如： 求和运算，符号：\\displaystyle \\sum，如： 积分运算，符号：\\int，如： 积分运算，符号：\\displaystyle \\int，如： 微分运算，符号：\\partial，如： 矩阵表示，符号：\\begin&#123;matrix&#125; \\end&#123;matrix&#125;，如： 十五、 逻辑运算 等于运算，符号：=，如： 大于运算，符号：&gt;，如： 小于运算，符号：&lt;，如： 大于等于运算，符号：\\geq，如： 小于等于运算，符号：\\leq，如： 不等于运算，符号：\\neq，如： 不大于等于运算，符号：\\ngeq，如： 不大于等于运算，符号：\\not\\geq，如： 不小于等于运算，符号：\\nleq，如： 不小于等于运算，符号：\\not\\leq，如： 约等于运算，符号：\\approx，如： 恒定等于运算，符号：\\equiv，如： 十六、集合运算 属于运算，符号：\\in，如： 不属于运算，符号：\\notin，如： 不属于运算，符号：\\not\\in，如： 子集运算，符号：\\subset，如： 子集运算，符号：\\supset，如： 真子集运算，符号：\\subseteq，如： 非真子集运算，符号：\\subsetneq，如： 真子集运算，符号：\\supseteq，如： 非真子集运算，符号：\\supsetneq，如： 非子集运算，符号：\\not\\subset，如： 非子集运算，符号：\\not\\supset，如： 并集运算，符号：\\cup，如： 交集运算，符号：\\cap，如： 差集运算，符号：\\setminus，如： 同或运算，符号：\\bigodot，如： 同与运算，符号：\\bigotimes，如： 实数集合，符号：\\mathbb&#123;R&#125;，如：\\mathbb&#123;R&#125; 自然数集合，符号：\\mathbb&#123;Z&#125;，如：\\mathbb&#123;Z&#125; 空集，符号：\\emptyset，如： 十七、数学符号 无穷，符号：\\infty，如： 虚数，符号：\\imath，如： 虚数，符号：\\jmath，如： 数学符号，符号\\hat&#123;a&#125;，如： 数学符号，符号\\check&#123;a&#125;，如： 数学符号，符号\\breve&#123;a&#125;，如： 数学符号，符号\\tilde&#123;a&#125;，如： 数学符号，符号\\bar&#123;a&#125;，如： 矢量符号，符号\\vec&#123;a&#125;，如： 数学符号，符号\\acute&#123;a&#125;，如： 数学符号，符号\\grave&#123;a&#125;，如： 数学符号，符号\\mathring&#123;a&#125;，如： 一阶导数符号，符号\\dot&#123;a&#125;，如： 二阶导数符号，符号\\ddot&#123;a&#125;，如： 上箭头，符号：\\uparrow，如： 上箭头，符号：\\Uparrow，如： 下箭头，符号：\\downarrow，如： 下箭头，符号：\\Downarrow，如： 左箭头，符号：\\leftarrow，如： 左箭头，符号：\\Leftarrow，如： 右箭头，符号：\\rightarrow，如： 右箭头，符号：\\Rightarrow，如： 底端对齐的省略号，符号：\\ldots，如： 中线对齐的省略号，符号：\\cdots，如： 竖直对齐的省略号，符号：\\vdots，如： 斜对齐的省略号，符号：\\ddots，如： 十八、矩阵使用\\begin&#123;matrix&#125;…\\end&#123;matrix&#125;这样的形式来表示矩阵，在\\begin 与\\end之间加入矩阵中的元素即可。矩阵的行之间使用\\\\分隔，列之间使用&amp;分隔，例如: 得到： 1. 括号如果要对矩阵加括号，可以像上文中提到的一样，使用\\left与\\right 配合表示括号符号。也可以使用特殊的matrix 。即替换\\begin&#123;matrix&#125;…\\end&#123;matrix&#125; 中matrix 为pmatrix ，bmatrix ，Bmatrix ，vmatrix , Vmatrix 。pmatrix$\\begin&#123;pmatrix&#125;1 &amp; 2 \\\\ 3 &amp; 4\\\\ \\end&#123;pmatrix&#125;$:pmatrixbmatrix$\\begin&#123;bmatrix&#125;1 &amp; 2 \\\\ 3 &amp; 4\\\\ \\end&#123;bmatrix&#125;$ :bmatrixBmatrix$\\begin&#123;Bmatrix&#125;1 &amp; 2 \\\\ 3 &amp; 4\\\\ \\end&#123;Bmatrix&#125;$ :Bmatrixvmatrix$\\begin&#123;vmatrix&#125;1 &amp; 2 \\\\ 3 &amp; 4\\\\ \\end&#123;vmatrix&#125;$ :vmatrixVmatrix$\\begin&#123;Vmatrix&#125;1 &amp; 2 \\\\ 3 &amp; 4\\\\ \\end&#123;Vmatrix&#125;$ :Vmatrix元素省略:可以使用\\cdots ：⋯，\\ddots：⋱ ，\\vdots：⋮ 来省略矩阵中的元素，如： 表示： 2. 增广矩阵增广矩阵需要使用前面的表格中使用到的\\begin&#123;array&#125; ... \\end&#123;array&#125;来实现。 显示为： 二十、公式标记与引用使用\\tag&#123;yourtag&#125;来标记公式，如$$a=x^2-y^3\\tag&#123;1&#125;$$显示为： 二十一、字体1.黑板粗体字此字体经常用来表示代表实数、整数、有理数、复数的大写字母。$\\mathbb ABCDEF$：$\\Bbb ABCDEF$： 3.黑体字$\\mathbf ABCDEFGHIJKLMNOPQRSTUVWXYZ$:$\\mathbf abcdefghijklmnopqrstuvwxyz$: 3.打印机字体$\\mathtt ABCDEFGHIJKLMNOPQRSTUVWXYZ$: 二十二、希腊字母 字母 实现 字母 实现 A A α \\alhpa B B β \\beta Γ \\Gamma γ \\gamma Δ \\Delta δ \\delta E E ϵ \\epsilon Z Z ζ \\zeta H H η \\eta Θ \\Theta θ \\theta I I ι \\iota K K κ \\kappa Λ \\Lambda λ \\lambda M M μ \\mu N N ν \\nu Ξ \\Xi ξ \\xi O O ο \\omicron Π \\Pi π \\pi P P ρ \\rho Σ \\Sigma σ \\sigma T T τ \\tau Υ \\Upsilon υ \\upsilon Φ \\Phi ϕ \\phi X X χ \\chi Ψ \\Psi ψ \\psi Ω \\v ω \\omega ε $\\varepsilon$ ϑ $\\vartheta$ ϖ $\\varpi$ ϱ $\\varrho$ ς $\\varsigma$ φ $\\varphi$  作者：无剑_君链接：来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"},{"date":"2024-04-22T13:14:48.755Z","url":"/2024/04/22/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E6%95%99%E7%A8%8B%EF%BC%9ALinux%20%E6%8C%87%E4%BB%A4/","categories":[["undefined",""]],"content":"Zip 将 &#x2F;root&#x2F;test 这个目录下所有文件和文件夹打包为当前目录下的 test.zip： 将 &#x2F;root&#x2F;test 这个目录下所有文件和文件夹加密打包为当前目录下的 test.zip： 将 test.zip 解压到指定目录： Tar 解压缩到指定文件夹： 上述命令将解压缩名为 “archive.tar” 的 tar 文件到指定的目标文件夹 “&#x2F;path&#x2F;to&#x2F;destination” 解压缩特定文件到指定位置： 上述命令将解压缩名为 “archive.tar” 的 tar 文件中的特定文件 “&#x2F;path&#x2F;to&#x2F;file” 到指定的目标文件夹 “&#x2F;path&#x2F;to&#x2F;destination”。 压缩为指定文件名： 上述命令将把指定文件夹 “&#x2F;path&#x2F;to&#x2F;source” 打包成名为 “new_archive.tar” 的 tar 文件。 Vim复制 yy 粘贴 p Bzip2 安装 解压 .bz2 文件： 如果你想同时保留原始的 .bz2 文件，可以使用 -k 选项： bzip2 命令也支持通过管道方式解压文件，例如： 上述命令将解压 .bz2 文件并将输出重定向到 outputfile。 GoLang安装 去 golang 的网站  解压至 &#x2F;usr&#x2F;local 修改终端文件 配置 DockerDocker 安装与卸载Centos安装 更新系统软件包列表： 安装所需的依赖软件包，以便使用 Docker： 添加 Docker 软件源： 安装 Docker 引擎： 启动 Docker 服务： （可选）将当前用户添加到 Docker 用户组，以便无需使用 sudo 运行 Docker 命令： 请注意，这将需要您重新登录才能生效。 验证 Docker 是否成功安装： 这将显示 Docker 引擎的版本信息。 卸载 停止 Docker 服务：运行以下命令停止正在运行的 Docker 服务： 删除 Docker 软件包：运行以下命令删除 Docker 软件包及其依赖项： 删除 Docker 数据和配置：运行以下命令删除 Docker 相关的数据和配置文件： 请注意，这将删除 Docker 所有镜像、容器和卷数据，以及配置文件。如果您希望保留这些数据，请备份相应的目录。 删除 Docker 用户组：运行以下命令删除 Docker 用户组： 这将删除名为 “docker” 的用户组。如果您在安装 Docker 时没有创建该用户组，可能会出现错误提示，可以忽略此步骤。 删除 Docker 相关的存储库文件：如果您添加了 Docker 的存储库文件，请运行以下命令删除这些文件。存储库文件通常位于 /etc/yum.repos.d/ 目录下，文件名可能类似于 docker-ce.repo 或 docker.repo。 请注意，这一步仅适用于您手动添加了 Docker 的存储库文件的情况。 删除 Docker 命令。 完成上述步骤后，Docker 将从 CentOS 系统中被完全卸载。 Docker 使用 创建一个容器，允许使用 GPU 产生基础镜像： Docker保存为离线文件： 将离线文件载入镜像： 修改镜像 bell&#x2F;backend:1.0作为基础镜像，对其进行修改产生新镜像。 Apach 使用 ApacheBench (ab) 工具进行性能测试 数据竞争检测 TODO使用 -race 标志可以在 Go 中进行数据竞争检测，以帮助发现并发程序中的潜在竞态条件。 构建服务器： 打开终端或命令提示符。 进入服务器的代码目录。 运行以下命令：go build -race。 如果构建成功，则会生成一个可执行文件，可以使用 ./&lt;可执行文件名&gt; 执行服务器。 运行服务器： 打开终端或命令提示符。 进入服务器的代码目录。 运行以下命令：go run -race main.go。 这将使用 Go 运行时环境直接运行 main.go 文件，并启动服务器。 使用 -race 标志构建或运行服务器后，Go 编译器&#x2F;运行时将会执行数据竞争检测。如果检测到潜在的竞态条件，它将在终端或命令提示符中显示相关的警告或错误信息。 注意，使用 -race 标志构建或运行服务器可能会增加程序的执行时间和资源消耗，因为数据竞争检测会引入一些额外的开销。因此，它通常用于开发和调试阶段，以帮助发现并解决并发问题，而不是在生产环境中使用。"},{"date":"2024-04-22T13:14:48.753Z","url":"/2024/04/22/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E6%95%99%E7%A8%8B%EF%BC%9AHexo/","categories":[["undefined",""]],"content":"Hexo 基本操作使用 Hexo 搭建个人博客并通过 GitHub部署到vercel - 知乎 (zhihu.com) 主题Themes | Hexo比较好看 最好看的Candinya&#x2F;Kratos-Rebirth: 一只可爱的hexo主题 m(&#x3D;•ェ•&#x3D;)m~🍭 (github.com) 考虑一个 obsidianHexo+obsidian+github完美建站教程 - 知乎 (zhihu.com)obsidian 加入 hexoHexo + Obsidian + Git 完美的博客部署与编辑方案 | EsunR-Blog"},{"date":"2024-04-22T13:14:48.752Z","url":"/2024/04/22/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E6%95%99%E7%A8%8B%EF%BC%9AGPU%20%E7%9B%B8%E5%85%B3/","categories":[["undefined",""]],"content":"在线差查看 gpu 的配置 查看GPU 名称 lshw -C display 在线查看 GPU 配置 GPU 配置信息 GPU 驱动安装 从NVIDIA官网下载最新的NVIDIA驱动程序。 从官方网址下进行下载（打开慢把.com换成.cn）Official Drivers | NVIDIA 使用root用户登录系统，然后运行以下命令： # sh NVIDIA-Linux-x86_64-xxx.run 按照提示完成安装。 -no-nouveau-check：安装驱动时禁用nouveau -no-opengl-files：只安装驱动文件，不安装OpenGL文件 安装完成后，您可以使用 nvidia-smi 命令查看 NVIDIA GPU 的状态。"},{"date":"2024-04-22T13:14:48.751Z","url":"/2024/04/22/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/","categories":[["undefined",""]],"content":"[[教程：Obsidian]][[教程：MD 数学公式语法]][[教程：Linux 指令]][[教程：Rime 输入法]][[配置：python 安装文档]][[配置：Linux 系统]][[配置：Windows]]"},{"date":"2024-04-22T13:14:48.685Z","url":"/2024/04/22/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python/python%20%E9%AB%98%E7%BA%A7/","categories":[["undefined",""]],"content":"使用python库 魔术命令 一组专门用于增强Jupyter Notebook交互性和功能的特殊命令。这些命令以%或%%开头，并且只能在Jupyter Notebook环境中使用。 注意：魔术命令不能在 python 命令行中执行。 行魔术命令（line magic commands）：以%开头的命令，作用于单行代码。例如： %run：运行外部Python脚本。 %pwd：显示当前工作目录。 %time：测量代码的执行时间。 单元魔术命令（cell magic commands）：以%%开头的命令，作用于整个代码单元格。例如： %%time：测量整个单元格的执行时间。 %%html：将单元格内容解释为HTML。 %%bash：在单元格中运行Bash命令。 帮助命令：以?结尾的命令，用于获取相关对象或函数的帮助信息。例如： len?：获取len函数的帮助信息。 obj?：获取对象obj的帮助信息。 魔术命令的参数和选项：魔术命令可以接受参数和选项，以进一步定制其行为。例如： %matplotlib inline：将Matplotlib图形嵌入到Notebook中。 %run -i script.py：以交互模式运行外部脚本。 装饰函数Python__ 的装饰器是一种重要的编程概念，它们允许开发者在不改变被装饰函数源码的情况下，为函数添加额外的职责或者行为。装饰器通常由一个函数组成，它可以接收另一个函数作为输入，并返回一个新的函数对象。这些新的函数对象包含了原函数的功能和一些额外的逻辑。 以下是关于 Python 装饰器的几个例子： @lru_cache 装饰器用于提高性能，特别是对于那些经常重复计算的函数。它会缓存函数的计算结果，以便在未来相同的参数调用下可以直接获取缓存中的结果，而无需重新计算。这种缓存机制特别适用于那些计算成本较高的场景。 @total_ordering 装饰器则是为了提供缺失的比较方法，特别是在没有实现这些方法的标准Python类中。通过使用这个装饰器，可以为预定义的Python类自动生成比较方法，确保不同实例之间能够进行正确的比较。 总结来说，Python 装饰器是设计用来简化代码、增强函数功能和提升程序效率的工具。它们使得开发者能够在保持函数接口不变的同时，灵活地扩展其功能。 问题解决问题： ‘utf-8’ 解析出错 UnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xf1 in position 5079963: invalid continuation byte 解决1：使用默认字符替换，默认是 “?”。with open(args.train_path, &quot;r&quot;, encoding=&quot;utf-8&quot;, errors=&#39;replace&#39;) as f 解决2：定位到没有解析的字符行，重新输入 问题：显存不足 使用 watch -n 0.1 nvidia-smi 查看 显存情况，可以看到运行的一瞬间，显存爆掉了。"},{"date":"2024-04-22T13:14:48.683Z","url":"/2024/04/22/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/GO_lang/grpc/grpc_1/","categories":[["undefined",""]],"content":" 假设当前文件夹为 project 初始文件 server.go myserver.proto 在命令行中执行 生成可执行文件 执行文件 "},{"date":"2024-04-22T13:14:48.680Z","url":"/2024/04/22/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/GO_lang/go%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/","categories":[["undefined",""]],"content":"在线调试平台 项目 地址 [[grpc_1]] [[go 基础]] [[go 高级]]"},{"date":"2024-04-22T13:14:48.678Z","url":"/2024/04/22/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/GO_lang/go%20%E9%AB%98%E7%BA%A7/","categories":[["undefined",""]],"content":"常见库Flag 包flag包基本使用官方文档]参考文档 导入flag包 flag参数类型flag包支持的命令行参数类型有bool、int、int64、uint、uint64、float float64、string、duration。 flag参数 有效值 字符串flag 合法字符串 整数flag 1234、0664、0x1234等类型，也可以是负数。 浮点数flag 合法浮点数 bool类型flag 1, 0, t, f, T, F, true, false, TRUE, FALSE, True, False。 时间段flag 任何合法的时间段字符串。如”300ms”、”-1.5h”、“2h45m”。合法的单位有”ns”、“us” &#x2F;“µs”、“ms”、“s”、“m”、“h”。 定义命令行flag参数有以下两种常用的定义命令行flag参数的方法。 flag.Type()基本格式如下： flag.Type(flag名, 默认值, 帮助信息)*Type 例如我们要定义姓名、年龄、婚否三个命令行参数，我们可以按如下方式定义： 需要注意的是，此时name、age、married、delay均为对应类型的指针。 flag.TypeVar()基本格式如下： flag.TypeVar(Type指针, flag名, 默认值, 帮助信息) 例如我们要定义姓名、年龄、婚否三个命令行参数，我们可以按如下方式定义： flag.Parse()通过以上两种方法定义好命令行flag参数后，需要通过调用flag.Parse()来对命令行参数进行解析。 支持的命令行参数格式有以下几种： -flag xxx （使用空格，一个-符号） --flag xxx （使用空格，两个-符号） -flag=xxx （使用等号，一个-符号） --flag=xxx （使用等号，两个-符号） 其中，布尔类型的参数必须使用等号的方式指定。 Flag解析在第一个非flag参数（单个”-“不是flag参数）之前停止，或者在终止符”–“之后停止。 flag其他函数 完整示例定义 使用命令行参数使用提示： 正常使用命令行flag参数： 使用非flag命令行参数： Log 包日志相关，参考 基本结构常量列表： Ldate 日期 年&#x2F;月&#x2F;日 Ltime 时间 时:分:秒 Lmicroseconds 时间 .毫秒于Ltime之后 Llongfile 完整文件名:行号 Lshortfile 文件名，此标志位优先于 Llongfile LstdFlags &#x3D; Ldate 并且 Ltime 功能说明：标志位常量控制日志格式。 示例： 函数列表: 自定义 logger: 参数列表： out 输出目标 prefix 输出前缀 flag 格式配置标识值返回值： 自定义的logger功能说明： 这个方法用来自定义logger，指定输出目标、格式等示例： Fatal系列函数1.1 func Fatal(v …interface{})参数列表： v 待输出参数列表返回值： 无功能说明： 打印日志并退出。相当于调用Print()并os.Exit(1) 1.2 func Fatalf(format string, v …interface{})参数列表： format 输出格式 v 带输出参数列表返回值： 无功能说明： 按格式输出日志，并退出。相当于调用Printf()并调用os.Exit(1) 1.3 func Fatalln(v …interface{})参数列表： v返回值： 无功能说明： 打印一行日志并退出。相当于调用Println()并os.Exit(1) 1.4 func (l *Logger) Fatal(v …interface{})参数列表： v 待输出参数列表返回值： 无功能说明： 打印日志并退出。相当于调用l.Print()并os.Exit(1) 1.5 func Fatalf(format string, v …interface{})参数列表： format 输出格式 v 带输出参数列表返回值： 无功能说明： 按格式输出日志，并退出。相当于调用l.Printf()并调用os.Exit(1) 1.6 func (l *Logger) Fatalln(v …interface{})参数列表： v返回值： 无功能说明： 打印一行日志并退出。相当于调用l.Println()并os.Exit(1) 示例 Flags系列函数2.1 func Flags() int参数列表： 无返回值： 默认logger的配置值功能说明： 返回默认logger配置值。 2.2 func (l *Logger) Flags() int参数列表： 无返回值： 当前logger的配置值功能说明： 返回当前logger配置值。 2.3 func SetFlags(flag int)参数列表： flag logger配置值返回值： 无功能说明： 这个方法用来设置标准logger的配置，默认为3（logger.LstdFlags） 2.4 func (l *Logger) SetFlags(flag int)参数列表： flag logger配置值返回值： 无功能说明： 这个方法用来设置标准logger的配置，默认为3（logger.LstdFlags） 示例 Panic系列函数3.1 func (l *Logger) Panic(v …interface{})参数列表： v 待输出参数列表返回值： 无功能说明： 这个方法相当于调用l.Print()及panic() 3.2 func (l *Logger) Panicf(format string, v …interface{})参数列表： format 输出格式 v 待输出参数列表返回值： 无功能说明： 相当于调用l.Printf()，之后调用panic() 3.3 func (l *Logger) Panicln(v …interface{})参数列表： v 待输出参数列表返回值： 无功能说明： 相当于调用l.Println()并调用panic() 3.4 func Panic(v …interface{})参数列表： v 待输出参数列表返回值： 无功能说明： 这个方法相当于调用Print()及panic() 3.5 func Panicf(format string, v …interface{})参数列表： format 输出格式 v 待输出参数列表返回值： - 无功能说明： 相当于调用Printf()，之后调用panic() 3.6 func Panicln(v …interface{})参数列表： v 待输出参数列表返回值： 无功能说明： 相当于调用Println()并调用panic() 示例 Print系列函数4.1 func (l *Logger) Print(v …interface{})参数列表： v 待输出参数列表返回值： 无功能说明： 输出日志到logger。参数处理方式同fmt.Print 4.2 func (l *Logger) Printf(format string, v …interface{})参数列表： format 输出格式 v 待输出参数列表返回值： 无功能说明： 调用l.Output输出日志到logger l。参数处理方式同fmt.Printf 4.3 func (l *Logger) Println(v …interface{})参数列表： v 待输出参数列表返回值： 无功能说明： 调用Output打印日志到当前logger，参数处理方式同fmt.Println 4.4 func Print(v …interface{})参数列表： v 待输出参数列表返回值： 无功能说明： 输出日志到标准logger。参数处理方式同fmt.Print 4.5 func Printf(format string, v …interface{})参数列表： format 输出格式 v 待输出参数列表返回值： 无功能说明： 调用Output输出日志到标准logger。参数处理方式同fmt.Printf 4.6 func Println(v …interface{})参数列表： v返回值： 无功能说明： 调用Output打印日志到标准logger，参数处理方式同fmt.Println 示例 其它函数5.1 func (l *Logger) Prefix() string参数列表： 无返回值： logger前缀，字符串类型功能说明： 返回当前logger的输出前缀 5.2 func Prefix() string参数列表： 无返回值： 标准logger前缀，字符串类型功能说明： 返回标准logger的输出前缀 5.3 func SetPrefix(prefix string)参数列表： prefix 前缀返回值： 无功能说明： 设置logger的输出前缀 5.4 func SetPrefix(prefix string)参数列表： prefix 前缀返回值： 无功能说明： 设置logger的输出前缀 5.5 func (l *Logger) Output(calldepth int, s string) error参数列表： calldepth 深度 s 字符串返回值： error 错误功能说明： 输出日志事件。字符串s包含待打印内容，跟在预定义的prefix后面，并且根据flags设置会有区分。如果s末尾没有换行符，这个方法会默认加上一个。calldepth目前预定义均为2,以后会用来支持通用场景，支持其他值配置。（本人注：日志输出不建议直接使用该方法） 5.6 func SetOutput(w io.Writer)参数列表： w 目标流，io.Writer类型返回值： 无功能说明： 设置标准logger的输出目标 示例 Badger 包参考官方文档badger是一个纯Go实现的快速的嵌入式K&#x2F;V数据库，针对LSM tree做了优化。 安装$ go get github.com/dgraph-io/badger/... 数据库打开一个数据库 存储存储kv使用 Txn.Set()方法 批量设置 WriteBatch不允许任何读取。对于读-修改-写，应该使用事务API。 设置生存时间 TTLBadger 允许在键上设置一个可选的生存时间 (TTL) 值。一旦 TTL 结束，KEY 将不再是可检索的，并且将进行垃圾收集。TTL 可以使用 Txn.SetWithTTL() 设置为一个time.Duration的值 设置元数据Txn.SetWithMeta() 设置用户元数据 使用 Txn.SetEntry() 可以一次性设置 key, value, user metatadata 和 TTL 遍历 keys要遍历键，我们可以使用迭代器，可以使用 Txn.NewIterator()方法获得迭代器。迭代按字节字典排序顺序进行。 前缀扫描要遍历键前缀，可以将 Seek() 和 ValidForPrefix() 组合使用：（这里的前缀是键值的） 键的遍历Badger支持一种独特的迭代模式，称为只有键的迭代。它比常规迭代快几个数量级，因为它只涉及对 lsm 树的访问，而 lsm 树通常完全驻留在 RAM 中。要启用只有键的迭代，您需要设置 IteratorOptions 。PrefetchValues 字段为 false 。这还可以用于在迭代期间对选定的键执行稀疏读取，只在需要时调用 item.Value() 。(获取键，而不获取对应的值) 数据流Badger 提供了一个流框架，它可以并发地遍历数据库的全部或部分，将数据转换为自定义键值，并连续地将数据流输出，以便通过网络发送、写入磁盘，甚至写入 Badger。这是比使用单个迭代器更快的遍历 Badger 的方法。Stream 在管理模式和正常模式下都支持Badger 。 删除一个key使用Txn.Delete() 方法删除一个key 获取 key value通过 txn.Get 获取 value 如果不存在 Txn.Get() 将会返回一个 ErrKeyNotFound 错误 请注意，Get()返回的值只在事务打开时有效。如果需要在事务外部使用值，则必须使用copy() 将其复制到另一个字节片。 事务只读事务只读事务使用 DB.View()方法 读写事务锁读写事务可以使用 DB.Update()方法 手动管理事务直接使用DB.NewTransaction()函数，手动创建和提交事务。它接受一个布尔参数来指定是否需要读写事务。对于读写事务，需要调用Txn.Commit()来确保事务已提交。对于只读事务，调用 txn.reject()就可以了。commit()也在内部调用 txn .reject()来清除事务，因此只需调用Txn.Commit()就足以执行读写事务。 但是，如果您的代码由于某种原因(出错)没有调用Txn.Commit()。就需要在defer中调用 txn . reject() Error 包 Server 常用 获取 URL 的路径参数 比如： 前端访问： ， 如何获取 haha 这个字符串？ 看代码： http 状态码返回 闭包 net 和 http同时使用 Go 语言中，可以同时使用 net 和 http 包来监听同一个接口，但是需要小心处理并避免冲突。 net 包提供了底层的网络功能，可以通过 net.Listen 函数来监听指定的网络地址和端口。而 http 包是建立在 net 包之上的，提供了更高级的 HTTP 服务器和客户端功能。 以下是一个示例代码，同时使用 net 和 http 监听同一个接口： 在上述示例中，我们使用 net.Listen 函数监听 localhost:8080，然后使用 http 包启动了一个 HTTP 服务器。通过 http.HandleFunc 函数，我们定义了一个简单的处理函数来响应 HTTP 请求。最后，通过 http.Serve 函数将监听器与 HTTP 服务器关联起来。 你可以在 // 其他的网络处理逻辑... 的部分添加其他网络处理逻辑，如基于 net 包的 TCP 或 UDP 服务器。只需确保网络处理逻辑不会与 HTTP 服务器冲突，比如使用不同的端口或处理不同的网络协议。 需要注意的是，当使用 http.Serve 函数时，它将阻塞当前的 goroutine，因此我们在示例中使用了 select &#123;&#125; 来阻止 main 函数退出。这样可以保持服务器的运行，直到显式地退出程序。 mymux myMux := http.NewServeMux() 这条语句用于创建一个新的 ServeMux 对象。 在 Go 的 http 包中，ServeMux 是一个 HTTP 请求多路复用器（multiplexer），用于将收到的 HTTP 请求分发到相应的处理器。ServeMux 类型实现了 http.Handler 接口，因此它本身可以作为一个处理器来处理请求。 通过调用 http.NewServeMux() 函数，我们可以创建一个新的 ServeMux 对象，它将用于注册和管理不同路径的处理器。 例如，下面是一个简单示例，使用 ServeMux 对象来管理不同路径的处理器： 在上述示例中，我们首先使用 http.NewServeMux() 创建了一个新的 ServeMux 对象 myMux。然后，我们使用 myMux.HandleFunc 方法来注册处理器函数，每个函数对应一个特定的路径。 最后，我们创建了一个 http.Server 对象，并将 myMux 对象作为处理器指定给该服务器。这样，当服务器收到请求时，就会使用 myMux 对象来根据请求的路径选择相应的处理器函数进行处理。 总结起来，myMux := http.NewServeMux() 用于创建一个 ServeMux 对象，以便注册和管理不同路径的处理器，并根据路径选择相应的处理函数来处理请求。 go routine 实现并发执行 Go 的 goroutine 来实现并发执行。以下是一个示例代码，演示了如何同时运行 HTTP 服务、gRPC 服务和连接其他 gRPC 服务器： 在这个示例中，我们使用 goroutine 启动了 HTTP 服务器和 gRPC 服务器，并在主函数中创建了一个与其他 gRPC 服务器的连接。 注意在连接其他 gRPC 服务器时，我们使用了 grpc.Dial 函数，并传递了 grpc.WithTransportCredentials(credentials.NewInsecure()) 选项来创建一个不安全的连接。你应该根据实际情况选择正确的连接配置，例如使用安全的连接和认证。 最后，使用 select &#123;&#125; 阻塞主函数，使程序保持运行状态，直到显式退出。 请根据你的实际需求，将代码中的注释部分替换为适当的 gRPC 服务注册和其他操作代码。 连接阻塞 循环变量副本 for i :&#x3D; 1; i &lt;&#x3D; *rpc_num; i++ { if i &#x3D;&#x3D; *rpc_id { continue &#x2F;&#x2F; 跳过与 当前相同的 端口号 ，不能和自己连接 } go func(){ conn[i], err &#x3D; grpc.Dial(rpc_addr[i], grpc.WithTransportCredentials(insecure.NewCredentials()), grpc.WithBlock()) if err !&#x3D; nil { log.Fatalf(“Failed to connect to server %s: %v”, rpc_addr[i], err) return } log.Printf(“Connect to %s OK”, rpc_addr[i]) }() } 这里有个提示信息但是没有报错，提示为：loop variable i captured by func litera “loop variable i captured by func literal” 意味着在你的代码中，匿名函数字面量（func literal）捕获了循环变量 i。 在 Go 语言中，当你在循环体内使用匿名函数时，该匿名函数会形成一个闭包，它可以访问和引用外部作用域中的变量。在这种情况下，匿名函数引用了循环变量 i。然而，循环变量在每次迭代中的值会发生变化，这可能会导致一些意外的行为。 为了避免这种问题，Go 语言引入了循环变量副本的概念。在每次迭代中，会为循环体内的匿名函数创建一个新的副本变量，以确保在闭包内部引用的是正确的值。这样可以避免循环变量值的竞争条件。 解决这个问题的常见方法是在循环体内部创建一个局部变量，将循环变量的值复制给局部变量，然后在匿名函数中使用局部变量。例如： 通过创建局部变量 iCopy 并使用它，你可以确保每个匿名函数引用的是该迭代的正确值，而不会受到循环变量的更改影响。 Atomic 包 Done Wait 同步 Go 内存模型中的术语。它说明了在调用 Done 方法之前，它会与任何由它解除阻塞的 Wait 调用之后的返回之前进行”同步在”（synchronizes before）的关系。 具体解释如下： Done 方法是用于通知等待组（WaitGroup）中某个操作的完成。 Wait 方法是等待等待组中所有操作完成的调用。 “同步在”（synchronizes before）是 Go 内存模型中的同步关系，指示在同一个 goroutine 中，一个操作的执行在另一个操作之前进行了同步。 根据这句话的解释，当一个 goroutine 调用 Done 方法时，它会在解除等待组中的某个操作的阻塞之前与该操作的返回之前进行同步。 具体来说，如果一个 goroutine 在调用 Wait 方法等待等待组中的操作时被阻塞，那么当另一个 goroutine 调用 Done 方法通知该操作的完成时，调用 Done 的 goroutine 会在被阻塞的 goroutine 返回之前进行同步。这确保了在 Done 方法解除阻塞之后，等待的 goroutine 可以安全地继续执行。 总之，这句话的含义是在 Go 内存模型中，Done 方法的调用会在解除阻塞的 Wait 调用的返回之前进行同步。这个同步关系确保了等待组的协同操作的正确性和可靠性。"},{"date":"2024-04-22T13:14:48.676Z","url":"/2024/04/22/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/GO_lang/go%20%E5%9F%BA%E7%A1%80/","categories":[["undefined",""]],"content":"go个人笔记  go 基础  go 环境安装  零、配置环境变量1.临时添加环境变量 PATH export PATH&#x3D;$PATH:&#x2F;usr&#x2F;local&#x2F;go&#x2F;bin 2.所有用户永久添加环境变量 vim &#x2F;etc&#x2F;profile 末尾添加：export PATH:$PATH:&#x2F;usr&#x2F;local&#x2F;go&#x2F;bin 刷新：sourse &#x2F;etc&#x2F;profile 3.golang 工作目录 GOPATH 是一个环境变量，用于指定Go项目的工作目录，默认情况下，GOPATH是 $HOME是当前用户的主目录。(&#x2F;root 或者 &#x2F;home&#x2F;user1) 获取： go env GOPATH 更改： 编辑 .bashrc 或者 .zshrc 找到 GOPATH 这行，将其修改为 &#x2F;path&#x2F;to&#x2F;your&#x2F;gopath 4.go.sum 与 go.mod go.sum 和 go.mod 是 Go 语言中用于管理模块依赖的文件。 go.mod 文件：go.mod 文件用于定义和管理项目的模块依赖关系。它位于项目的根目录下。当你使用 go get 命令来安装或更新依赖包时，go.mod 文件会被更新。它记录了项目所依赖的模块及其版本。你可以手动编辑 go.mod 文件来添加、移除或升级依赖包。在构建项目时，Go 会根据 go.mod 文件获取所需的依赖包。 go.sum 文件：go.sum 文件用于记录项目所使用模块的校验和信息。每个模块都有一个唯一的校验和，用于确保下载的模块的完整性和安全性。go.sum 文件会自动生成并更新，其中包含了所有依赖模块的校验和。当你构建项目时，Go 会根据 go.sum 文件验证模块的完整性，以确保下载的模块与之前的校验和匹配。 使用 go.mod 和 go.sum 文件的基本操作如下： 初始化模块：在项目的根目录下执行 go mod init 命令，它会根据项目的路径和名称创建一个新的 go.mod 文件。 添加依赖包：执行 go get 命令来添加所需的依赖包。例如，go get github.com/example/package。这会自动更新 go.mod 文件并下载依赖包。 移除依赖包：执行 go mod tidy 命令来移除不再使用的依赖包。它会自动更新 go.mod 文件并删除不需要的依赖。 升级依赖包：执行 go get -u 命令来升级依赖包到最新版本。这会自动更新 go.mod 文件。 构建项目：使用 go build 或 go run 命令来构建或运行项目。Go 会根据 go.mod 文件下载依赖包并构建项目。 需要注意的是，go.mod 和 go.sum 文件对于保证项目的可复现性和依赖包的一致性非常重要。当你与他人共享项目时，确保将 go.mod 和 go.sum 文件一同共享，以便其他人可以获取相同的依赖包版本。 管理和安装 go.mod 和 go.sum 中列出的依赖项： 下载依赖项 当你首次克隆一个 Go 项目或者在已有项目中更新了依赖项时，可以使用以下命令来下载 go.mod 文件中指定的所有依赖项： 这个命令会将依赖项下载到本地的 Go 模块缓存中，但不会在项目目录中创建任何文件。 同步依赖项 如果你想确保项目目录下的依赖项与 go.mod 文件中定义的依赖项完全同步（即添加缺失的模块，删除不需要的模块，并更新 go.sum），可以使用： go mod tidy 命令会添加缺少的模块，移除未使用的模块，并生成一个新的 go.sum 文件。这是确保 go.mod 和 go.sum 文件准确反映项目依赖项的好方法。 查看依赖项 如果你想查看当前项目的依赖树，可以使用： 这个命令会打印项目的所有依赖项及其版本，帮助你理解项目依赖的结构。 更新依赖项 要更新项目中的某个依赖项到最新版本，可以使用： 其中 package 是依赖项的路径，version 是你想要更新到的版本。如果你想更新所有依赖项到最新版本，可以使用： 但请注意，这可能会引入重大更改，因此建议仔细测试更新后的依赖项。 修改 proto 文件并运行 make proto 来更新 proto/pkg/xxx/xxx.pb.go 中相关生成的 go 代码 一、变量一般变量 变量声明 标准格式 简短格式 注意：变量的命名规则遵循骆驼命名法，即首个单词小写，每个新单词的首字母大写，例如：numShips 和 startDate 。 变量类型 bool string 反引号可以实现多行 字符串长度 len(), 判断的是 ASCII 长度 或者是 字节长度 在 go 语言中， 所有字符串都是以 utf-8 的格式保存，因此 汉字 3 字节，字符 1 字节 若要将汉字作为一个整体统计， 可以使用下面的函数 RuneCountInString() , 判断的是字符的个数，或者说是 Unicode 字符串长度 int、int8、int16、int32、int64 uint、uint8、uint16、uint32、uint64、uintptr byte &#x2F;&#x2F; uint8 的别名 rune &#x2F;&#x2F; int32 的别名 代表一个 Unicode 码 不对 是 UTF-8 float32、float64 complex64、complex128 内建函数 interface 接口类型  map[string] *sync.WaitGroup 键值对类型 变量赋值 注意：当一个变量被声明之后，系统自动赋予它该类型的零值：int 为 0，float 为 0.0，bool 为 false，string 为空字符串，指针为 nil 等 所有的内存在 Go 中都是经过初始化的。 匿名变量不占用内存空间，不分配内存 作用域 全局变量声明必须以 var 关键字开头，如果想要在外部包中使用全局变量的首字母必须大写。 特殊变量定义位置 比较 只有同类型的变量才能进行比较 包括 0 1 到 true false 的转换 类型转换 bool 不能强转为 int 不能获取数组某个位置的的地址 结构体变量 结构体声明 结构体实例化 匿名结构体 结构体返回值 键值对类型map[string] *sync.WaitGroup 键值对类型 键值对变量 键值对作为结构体成员变量 数组 定义 二、输入、输出 fmt.Sprintf(格式化样式 ， 参数列表) 和 C 语言类似 printf fmt.Println(str) 动 词 功 能 %v 按值的本来值输出 %+v 在 %v 基础上，对结构体字段名和值进行展开 %#v 输出 Go 语言语法格式的值 %T 输出 Go 语言语法格式的类型和值 %% 输出 % 本体 %b 整型以二进制方式显示 %o 整型以八进制方式显示 %d 整型以十进制方式显示 %x 整型以十六进制方式显示 %X 整型以十六进制、字母大写方式显示 %U Unicode 字符 %f 浮点数 %p 指针，十六进制方式显示 四、 库flag功能：Go 语言标准库中的一个包，用于解析命令行参数 关键函数： flag.type(arg_name,arg_default_value,arg_discription) -&gt; value flag.Parse() ：解析命令获得各个参数的值 实例： 三、常见技巧常见调用 获取变量类型 varb.(type) 获取数组长度 len(arr) 键值对KVgo语言中可以使用map[string]interface&#123;&#125;类型来实现，键的索引和值的多种类型 数据排序对数据集合（包括自定义数据类型的集合）排序需要实现 sort.Interface 接口的三个方法，以下为该接口的定义： 数据集合实现了这三个方法后，即可调用该包的 Sort() 方法进行排序。 Sort() 方法定义如下： 例子： 数据比较reflect包中的DeepEqual函数完美的解决了比较问题。 函数签名： func DeepEqual(a1, a2 interface{}) bool 文档中对该函数的说明：DeepEqual函数用来判断两个值是否深度一致：除了类型相同；在可以时（主要是基本类型）会使用&#x3D;&#x3D;；但还会比较array、slice的成员，map的键值对，结构体字段进行深入比对。map的键值对，对键只使用&#x3D;&#x3D;，但值会继续往深层比对。DeepEqual 函数可以正确处理循环的类型。函数类型只有都会nil时才相等；空切片不等于nil切片；还会考虑array、slice的长度、map键值对数。示例： func main() { m1 :&#x3D; map[int]interface{}{1: []int{1, 2, 3}, 2: 3, 3: “a”} m2 :&#x3D; map[int]interface{}{1: []int{1, 2, 3}, 2: 3, 3: “a”} if reflect.DeepEqual(m1, m2) { fmt.Println(“相等”) }} uestc.leemen.org"},{"date":"2024-04-22T13:14:48.629Z","url":"/2024/04/22/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/TEMP/","categories":[["undefined",""]]},{"date":"2024-04-22T12:47:56.511Z","url":"/2024/04/22/test/","categories":[["undefined",""]],"content":"这是一个测试文件"},{"title":"Hello Worldaaaaaaa","date":"2024-04-22T08:33:35.137Z","url":"/2024/04/22/hello-world/","categories":[["undefined",""]],"content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post More info: Writing Run server More info: Server Generate static files More info: Generating Deploy to remote sites More info: Deployment"}]