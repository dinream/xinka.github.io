# 关键点
1. 第一个使用 GANs 生成密码


# 摘要

已有工作 HashCat 和 John the Ripper 很成熟，但是还是需要扩展来进一步建模密码；
PassGAN 使用的是 GAN；
PassGAN 可以提取相当数量的密码属性，而其他工具没有；

# 引言
密码常见；用户使用的密码简单；导致密码猜测很有效；
密码猜测的效率由匹配大量可能密码和特定密码哈希的速度有关；（可能的密码不是穷尽所有组合，而是有一定的候选密码）
最先进的工具在密码猜测时使用一些启发式算法可以构造许多新的候选密码。
但是，这些启发式的变换规则基于直觉，而不是基于大型密码数据集的分析。

因此，生成的候选密码覆盖范围小并且这些启发式的变换规则的开发和测试是很耗时的事情；
总结：当前方法的伸缩较差。

# 我们的方法
**核心：**训练一个模型自动决定密码的特征和结构，并利用这些知识生成新的候选密码；同时神经网络的训练必须要任何先验知识和属性的假定。
**比较：**
1. 马尔可夫模型：隐含地假设所有相关的密码特征都可以用n元语法定义；
2. 基于规则：只能猜测与可用规则匹配的密码；
**结果：**
1. 神经网络生成的密码覆盖范围很广
GANs：在高维空间中进行密度估计
	通过训练一个深度神经网络架构来执行隐式生成建模，该架构会输入一个简单的随机分布（如高斯分布或均匀分布），并生成遵循可用数据分布的样本。
	在某种程度上，他们用深度神经网络隐含地模拟了累积分布的逆模型，即 x = F-1 θ (s)，其中 s 是一个均匀分布的随机变量。
	训练过程：GAN 使用了一种猫捉老鼠的游戏
	1. 其中深度生成网络（G）试图模仿样本的底层分布，
	2. 而判别型深度神经网络（D）则试图区分原始训练样本（即 "真样本"）和由 G 生成的样本（即 "假样本"）。
	3. 这种对抗性程序迫使 D 泄露训练数据的相关信息，这些信息有助于 G 充分再现原始数据的分布

# 贡献
1. 预测率高
2. 输出质量高于规则
3. PassGAN 可以输出几乎没有限制的密码猜测数
4. 实验中 PassGAN猜出来的密码最多，但是生成的候选密码也最多
5. 与基于深度神经网络的密码猜测算法相比有竞争力。
6. PassGAN可以用于有效增强密码生成规则
7. 缺点：
	分析：有缺点
	1. 优势(表现力、通用性和从样本中自主学习的能力) 和 输出大小方面的成本 之间存在权衡。
	2. 基于规则的密码猜测工具可以在极少的尝试次数内生成大量的匹配结果，而 PassGAN 则必须输出更多的密码才能达到同样的效果。
	分析：不是问题
	1. 密码猜测工具可以很容易地组合在一起，先 hashcat 不行再 PassGAN
	2. 存储不是问题，可以离线生成。
	结论：用匹配个数作为主要指标是有意义的，而不是匹配生成的速度 
# 背景知识
1. GAN
2. 当前工具，hashcat | JTR 
	1. 基于字典
	2. 基于规则
	3. 基于马尔可夫模型： [HC解释](https://www.trustwave.com/en-us/resources/blogs/spiderlabs-blog/hashcat-per-position-markov-chains/)[JTR解释](https://openwall.info/wiki/john/markov)
	4. 神经网络的使用（注意点在密码强度估计）
# 使用

1. 模型：使用改进型 Wasserstein GANs（IWGAN）训练。
2. 优化器：ADAM 优化器
3. 参数：
	1. 批次大小：
	2. 迭代次数：在每次迭代中，GAN 会运行一次生成器迭代和一次或多次判别器迭代。
	3. 每次生成器迭代的判别器迭代次数：表示判别器在每次 GAN 迭代中执行的迭代次数。每次生成器迭代的判别器迭代次数设为 10，这是 IWGAN 使用的默认值。
	4. 模型维度：表示每个卷积层的维数。我们尝试在生成器和鉴别器中使用 5 个残差层，两个深度神经网络中的每个层都有 128 个维度。
	5. 梯度惩罚系数 (λ)：它规定了应用于判别器相对于输入的梯度准则的惩罚。增加该参数可使 GAN 的训练更加稳定。在实验中，我们将梯度惩罚值设为 10。
	6. 输出序列长度，表示生成器 (G) 生成的字符串的最大长度。我们将 GAN 生成的序列长度从 32 个字符（IWGAN 的默认长度）修改为 10 个字符，以符合训练过程中使用的密码最大长度。
	7. 输入噪声矢量（种子）的大小：它决定了正态分布中的随机数有多少个作为 G 的输入，以生成样本。我们将这一大小设置为 128 个浮点数。
	8. 最大示例数：表示要加载的训练项（PassGAN 中为密码）的最大数量。GAN 加载的最大示例数设置为整个训练数据集的大小。
	9. ADAM 优化器的超参数：
	- 学习率，即调整模型权重的速度 
	- 系数 β1，表示梯度运行平均值的衰减率。
	- 系数 β2，表示梯度平方运行平均值的衰减速度。
	- ADAM 优化器的系数 β1 和 β2 分别设置为 0.5 和 0.9，学习率为$$
	10^{-4}
$$这些参数是 Gulrajani 等人使用的默认值。
4. 整体结构：
	1. 残差块结构
		![[Pasted image 20240222172953.png]]
	2. 生成器和判别器各有五个残差块
		![[Pasted image 20240222173025.png]]
5. 运行环境
	1. 软件
		1. IWGAN 的 TensorFlow 版本【代码】
		2. TensorFlow 1.2.1
		3. Python 2.7.12
		4. Ubuntu 16.04.2 LTS
	2. 硬件：
		1.  64GB 内存
		2. 12 核 2.0 GHz 英特尔至强 CPU 
		3. 英伟达™（NVIDIA®）GeForce GTX 1080 Ti GPU（11GB 全局内存）。
6. IWGAN
	1. 寻常在训练深度神经网络时，最初的训练误差会随着层数的增加而减小。然而，在达到一定层数后，训练误差又开始增加。
	2. 但是 ResNet 包含层与层之间的 "快捷连接"。这可以看作是对这些层的封装，并以标识函数（图 1 中表示为残差块）的形式实现。通过使用多个连续的残差块，ResNet 可以随着层数的增加不断减少训练误差。
	3. PassGAN 中的残差区块由两个一维卷积层组成，并通过整流线性单元（ReLU）激活函数相互连接，如图 1 所示。区块的输入是标识函数，并与 0.3 个卷积层的输出相加，产生区块的输出。
7. 训练数据
	1. RockYou 数据集：选择长度为10，并且选择 80% 作为训练集，20% 作为测试集
	2. LinkedIn 数据集：数据是散列形式的，基于规则的系统具有潜在的优势。
8. 测试方向
	1. 相同的密码分布上训练和测试时，PassGAN 的预测效果如果
	2. PassGAN 在不同数据集的预测效果
9. 比较
	1. 用 PassGAN 的训练集作为HashCat Best64、HashCat gen2、JTR Spiderlab 规则、马尔可夫模型、PCFG 和 FLA 的输入数据集
		1. 按频率降序排序的密码实例化了 HashCat 和 JTR 的规则
			1. HashCat Best64 生成了 754,315,842 个密码，其中 361,728,683 个密码是唯一的，长度不超过 10 个字符。请注意，这是 Best64 规则集在给定输入集（即 RockYou 训练集）上生成的最大样本数。
			2. 对于 HashCat gen2 和 JTR SpiderLab，我们从它们的输出中统一抽取了一个大小为 10的9 的随机子集。该子集由长度不超过 10 个字符的密码组成。
		2. 对于 FLA，我们根据中提供的说明设置了 [44] 中的代码。我们训练了一个包含 2 个隐藏层和 1 个大小为 512 的密集层的模型（全部参数列表见附录 A 表 6）。为了与其他工具保持一致，我们没有对训练集进行任何转换（例如删除符号或将所有字符转换为小写）。训练完成后，FLA 会枚举其输出空间的一个子集，该子集由概率阈值 p 定义：当且仅当一个密码的估计概率至少为 p 时，该密码才属于 FLA 的输出。这样，长度为 10 个字符或以下的密码总数达到了 747,542,984 个。在评估中使用这些密码之前，我们按照概率从大到小进行了排序。
		3. 我们使用 3-gram Markov 模型生成了 494,369,794 个长度不超过 10 的唯一密码。我们使用该模型的标准配置运行了该模型[18]。
		4. 我们使用韦尔等人[91]的 PCFG 实现生成了 109 个长度在 10 或以下的唯一密码
10. 评估
	1. 首先评估了 PassGAN 的输出所生成的匹配数
	2. 然后将其与 FLA、马尔可夫模型的一种流行的 3-gram 实现[18]、PCFGs [91]以及 JTR 的密码生成规则（SpiderLab 混淆规则[82]）和 HashCat 的密码生成规则（Best64 和 gen2 规则[29]）进行了比较，（这些工具都在本文的训练数据集上做了多年的优化）。
	3. 结合 HashCat Best64 对 PassGAN 进行评估的实验结果。最后，我们从概率密度和密码分布的角度对 PassGAN 和 FLA 进行了比较。
	4. 如何避免过拟合：
		1. 比较不同迭代次数时的生成匹配个数，个数开始下降说明即将过拟合
	5. 如何 PassGAN 生成密码的质量
		1. 唯一个数
		2. 生成的密码和其他数据集的匹配个数
		3. 结果：
			1. 可以生成不少于其他工具的匹配个数的密码
			2. 且总的密码和其他工具相差在一个数量级之内
			3. 猜测不同于训练数据集的密码时，比基于规则的密码生成更有优势。
	6. 结合 PassGAN 和 HashCat
		1. 首先删除训练集中所有匹配HashCat Best64 的密码 
		2. 实验结果表明结合后可以匹配更多密码，
		3. Hashcat 的下新版本的  “slow candidates” 支持了这重结合
	7. 比较 PassGAN 和 FLA 
		1. FLA 是一个概率估计模型，本质是一个参数估计模型，会输出密码以及其概率估计
		2. FLA 受马尔可夫过程限制，变异性与 n-gram 范围有关，而 PassGAN 的变异性更高
		3. 次数较少时，FLA 的概率模拟不准确，次数较高，两者差不多 

# 结论
1. PassGAN 很有效，能从一个数据集中猜测另一个数据集的密码
2. 基于规则的限制很大。
3. FLA  和 PassGAN 也差不多
# 总结
1. 方法是革命性的，无需用户干预就可以生成密码，也无需用户分析密码库
2. PassGAN 可以输出相同的匹配数目，但是需要输出更多密码，这个成本可以忽略。
3. 将 PassGAN 的生成模型换位条件 GAN 可以生成特定字符字段的密码

# 专业名词

| 名词 | 含义 |  |
| ---- | ---- | ---- |
| leet speak | 黑客文 |  |
