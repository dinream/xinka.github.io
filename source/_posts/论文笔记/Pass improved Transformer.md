# 摘要
1. 分析了个人信息和密码之间的关联性。
2. 实现了一个基于改进 Transformer  的密码猜测模型
	1. 数据预训练时引入信息权重。
	2. 使用改进的波束搜索算法来快速搜索排名考前的输出结果。
# 模型和使用
## 模型结构
与原始Transformer模型不同的是，我们在词嵌入层之后添加了信息权重层。信息权重层涉及一个权重矩阵，权重矩阵表示每个字符携带的用户信息，该矩阵的维度与词嵌入矩阵相同。信息权重矩阵中，同一输入位置对应的参数值相同。参数值是通过模型的训练来确定的。信息权重矩阵将添加到输入嵌入中。那么后续步骤就和原来的Transformer一样了。
![[Pasted image 20240327152521.png]]


## 模型训练

1. **输入数据处理**：模型的输入是一批代表用户个人信息的数值向量。首先，这些输入通过嵌入层（embedding layer）被转换成字符嵌入（character embeddings），即将数值向量转换为能够代表这些信息的嵌入向量。
    
2. **信息权重处理**：接着，通过信息权重层（information weight layer），这些字符嵌入会根据用户信息的权重进行调整，以更准确地反映每部分用户信息的重要性。
    
3. **位置编码**：之后，这些调整后的字符嵌入通过位置编码层（positional encoding layer）来获得位置向量，这一步是为了让模型能够理解字符在序列中的位置关系。
    
4. **自注意力机制**：调整后的字符嵌入随后进入编码器（encoder）的自注意力层（self-attention layer）。在这一层中，模型计算字符之间的依赖关系，而这种计算忽略了字符之间的距离，即无论字符相隔多远，都能捕捉到它们之间的联系。
    
5. **向前传播和解码**：编码器的输出接着被用作前向神经网络（forward neural network）的输入。在编码器的最终层输出的基础上，将这些输出转换成一组键值对（Key, Value），然后这组键值对被传递给解码器（decoder），以帮助解码器获取输入序列的信息。

解码器的输入是一批代表用户密码的数字向量。解码器嵌入输入并添加每个字符的位置嵌入。与编码器不同，解码器中的自注意力层仅关注当前已确认的输出字符。解码器的输出是浮点数的向量列表。接下来，通过线性层将向量列表转换为向量。它称为对数向量。对数向量的每个元素对应一个字符，其值代表该字符的得分。最后，对数向量通过softmax层转换为每个字符出现的概率。 

## 模型预测
使用改进的束波搜索来快速找到排名考前的预测结果。


## 字符匹配算法优化
存储每个生成密码的 key 和 value，使用hash 表来存储，key 值计算过程。
Key = (char2int(a)\*M + char2int(b)\*M )%p

# 评估
## 数据集
1. 收集了 一亿条密码。
2. 筛选了19种个人信息的 31 个网站。
3. 统计了密码格式：大多数是 单词+数字，少部分是 单词 和 数字
4. 分析了密码和数字的关联性
	1. 邮箱 + 后缀
	2. 生日：年月日组合
	3. 姓名：拼音
	KMP算法 进行 字字符串比配
	结果：关联很大。经过统计，我们得出的结论是，用户密码与邮箱、生日、昵称、电话号码、姓氏、名字、登录网站的域名高度相关。其中，电子邮件相关密码占比最高，占比12.551%，手机号码相关密码占比5%，用户名​​相关密码占比2.617%，生日相关密码占比0.863%，密码相关密码占比最高。与网站域名相关的密码占0.173%，与名字相关的密码占0.794%，与姓氏相关的密码占0.002%。
	选择了与密码相关性较大的七个个人信息来构成输入序列，分别是生日、电话号码、网站域名、电子邮件地址、昵称、姓氏和名字。
5. 研究序列的长度：主要是长度为8 的密码，用 包含8 的长度训练，用包含8的密码测试，生成长度为8 的密码
## .实验环境
改进的 Transformer 是用 TensorFlow 实现的。我们使用 TensorFlow 版本 1.12.0 和 Python 版本 3.5.2。我们在 GPU 服务器上进行实验，该服务器具有 2 核 2.2GHz CPU、NVIDIA Tesla V100 GPU 和 16GB 全局内存。
## 对比实验
三个序列到序列模型：1. LSTM 2. encoder-decoder 3. Transformer
在相同的数据集中进行实验：x 轴表示测试集大小 ；y轴表示破解的数量，当生成的密码数量超过 10,000 个时，Transformer 的准确率将高于其他两个模型。我们的实验结果表明，Transformer 在基于用户信息的密码破解研究中表现最好。
## 实验结果
### 时间优化
匹配算法的优化（相同时间内匹配的次数更多）
### 模型优化
信息权重和无信息权重（），束波搜索和随机搜索

### 密码长度
五个不同密码长度： 8  9 10 11 12 
密码长度越长，相同的猜测次数下越难猜出密码。


# 总结
1. 分析了用户个人信息和密码之间的关联性，邮箱关联度很高
2. 提出了一种基于改经的 Transformer 模型，数据预处理中引入了信息权重
3. 波束搜索算法，