# 摘要
sformer 的密码猜测框架。
2. 使用预训练/微调范例。
3. 设计了三个特定场景下的密码猜测任务的微调方式：
	1. 条件性口令猜测，在给定部分口令的情况下回复完整口令。
	2. 针对、定向性口令猜测，利用特定用户的个人信息猜测其口令。
	3. 基于规则的自适应密码猜测，其为单词(即，基本密码)选择自适应调整规则以生成经规则转换的候选密码。
4. 最后我们提出了一种**混合口令强度计量器**来降低这三种攻击的风险。

# 引言

1. 数据驱动模型(例如，马尔科夫[28，32])和基于规则的工具(例如，Hashcat[17])来有效地离线破解密码
2. 现实世界猜测攻击：使用先验信息，
	1. 定向密码猜测 TPG。
	2. 条件密码猜测 CPG
	3. 基于自适应规则密码猜测 ARPG
3. 双向转换器在自然语言处理领域受到了极大的关注[12，27，51]。由于能够捕获双向上下文信息和高度可转移性[51]，转换器在多语言任务(例如，文本分类[59]、语法校正[34])中是有效的。
	1. 有的密码猜测攻击，无论是一般的还是基于现实世界的，都可以概念化为近似密码(即文本)的概率分布的努力，
	2. 这表明它自然适合基于双向转换器的猜测框架。
4.  现有有一些琐碎的工作（看了一下只是 神经网网络，不是特指 Transformer）效果不佳。
5. 成功使用 Transformer 需要一些特定的设计，比如这里使用了序列标签，参考之前的序列到序列机制。
6. 本文设计了一个基于字符级别的双向变化猜测框架。设计了特定方式的微调。取得了一定的效果。
7. 本文测试了预训练的效果。预先训练的密码模型比预先训练的自然语言模型能够产生更好的猜测性能，这表明了预先训练在特定密码语料库上的有效性。
8. 本文引入了混合密码强度计（HPSM）来降低三种攻击的风险。HPSM可以与密码泄漏检查[23，47]相结合来检测输入是否公开泄漏，一旦泄漏，输入就会遭受ARPG攻击。
# 相关工作
## 一般密码猜测
1. 基于数据驱动模型，基于密码数据对模型进行训练
2. 基于规则猜测，由专家知识定制初始配置，转换初始密码到候选密码。
## 基于额外信息攻击
 CPG：Pasquini等人[38，39]提出了一种基于Wasserstein自动编码器的最先进的CPG方法[48]。他们将最终的模型称为CWAE(上下文瓦瑟斯坦自动编码器)，它由将部分密码嵌入到潜在表示中的编码器和将部分密码的潜在表示转换为密码的解码器组成。
 TPG：其中，Pal等人提出的Pass2Path。[35]2019年，是最新、最有效的定向竞猜模式。他们提出了凭据调整，以破解用户历史密码的变体(调整)
 ARPG：021年，Pasquini et al.[37]提出了第一个基于规则的自适应猜测框架ADAMS(Adaptive Dynamic Mgling Rules Attack)，该框架构建了一个卷积规则(CNN神经网络)建模，为每个单词选择自适应规则。


## 基准攻击模型
我们使用最先进的CWAE、Pass2Path和ADAMS模型分别作为CPG、TPG和ARPG攻击的基线，因为这些模型与其他模型相比具有最高的猜测性能。例如，CWAE已被证明比基于PCFG、马尔可夫和神经网络的模型更好。因此，我们不会检查与其他型号的比较。

## 双向 Transformer
基于自我注意机制(即将给定的标记与所有文本环境连接起来)提出的，由于能够捕获深层文本特征而在自然语言处理领域获得了广泛的应用。
BERT(来自Transformers的双向编码器表示)[12，27]是一种流行的基于变压器的架构，已经在11个单独的NLP任务上取得了最先进的结果。
BERT针对 **MLM(掩蔽语言建模)** 和 **NSP(下一句预测)** 两个目标进行预训练，以建立基于大量未标记Web语料库的预训练语言模型。
1. 对于 MLM 目标，BERT 训练模型，使其能够预测屏蔽位置的正确标记。由于自然语言的特性，BERT很大程度上是为了预测屏蔽词而设计的。
2. NSP的目标是取一个句子对A和B，并预测B是否是A之后的实际下一个句子。Bert[12]提出了两种次级训练方法：微调和基于特征。
	1. 在微调方法中，所有参数都在下行任务期间更新。
	2. 而在基于特征的方法中，通过冻结一些普通的预训练层来从预训练的参数中提取固定的特征。 
	通常，微调方法会随着训练时间的增加而产生更好的结果[12]。在本文中，我们选择了所有参数都是可学习的微调方法。

# 预任务

## 工作流
攻击模型的工作流程是根据监督信号训练监督模型。我们将它们的监管信号总结在表1中。
1. 具体来说，CPG的监管信号是部分密码及其完整密码。 CPG 旨在训练模型，使其在给定部分密码的情况下预测正确的密码。 CPG 的输出直接作为密码候选。
2. TPG的监督信号是通过动态规划算法计算出的最短编辑路径的密码（在[35]中实现）。编辑路径是一系列原子编辑操作（在我们的攻击设计中预定义），
3. ARPG的监督信号是具有命中规则（即规则集的子集）的单词，例如根据两个假设数据集之间的命中信息删除最后三个字符。 ARPG模型输出自适应规则，然后将其应用于单词以获得候选密码。自适应规则通常与单词更兼容，从而可以尽早产生命中。 ARPG 中使用的修改规则在 Hashcat 中定义。一般情况下，大多数重整规则可以是常见原子规则的组合（例如，“删除最后三个字符”是“删除最后一个字符”的三个原子规则的组合），并且可以自然地模拟应用多个规则的场景按顺序到基本词。

## 威胁模型
假设攻击者可以选择预先训练的自然语言和密码特定参数（作为先验知识）或随机变量来初始化他们的攻击模型。

## 密码泄漏数据集
### 数据集选择
在实验中选择了先前作品[16、21、37、38、43、54、60]中使用的几个数据集。用于定向猜测的数据集是电子邮件，而用于非定向猜测攻击的数据集是明文密码。

密码预训练、CPG和ARPG使用由明文密码组成的数据集，
Rockyou-2009、000Webhost、Neopets、Cit0day、Rockyou-2021：

对于TPG攻击，我们选择以下两个包含电子邮件的数据集，并总结表2中的基本信息：
BreachCompilation (4iQ)
Collection#1
### 数据处理
账户加入：为了找到属于同一用户的密码列表，我们根据相同的电子邮件地址合并帐户（用户）[35]

数据集清理：
我们采用常用的清理策略[16,35,37,60]来过滤掉原始数据集中的哈希密码、非ASCII密码和超过32个字符的异常长密码。


## 密码二向性
顺序性：字符通常与其相邻字符更相关，这也可以是单向性的体现。聚合：相关字符的内部序列（例如“password123”中的“p@ssw0rd”和“123”；“mike199730”中的“199730”）有更多的连接线。
捕获双向表示可以提供更好的候选密码，从而提高密码猜测效率!!!! ！



# PassBERT

## 预训练
预训练模型主要捕获输入密码的上下文嵌入，这是密码中每个字符的具有上下文信息的高维表示。上下文嵌入是预训练层（即最后一个变压器块）的输出。
### 嵌入
1. 密码标记为字符序列，
2. 带有表示开头 ([CLS]) 和结尾 ([SEP]) 的附加符号，因为密码通常比句子短，
3. 对比：与 BERT 不同，BERT 通常在 token 级别对文本进行 token 化，其 token 主要是单词，并将每个句子剪辑为 512 个 token。
4. 我们考虑最大密码长度为32个字符，并考虑总共99个有效字符，包括95个ASCII字符（表示为Σ）和4个附加符号开始、结束、占位符和未知字符。
5. 嵌入层：
	1. 其字符嵌入和位置嵌入 求和。将标记化输入转换为其输入嵌入。
	2. 删除了 BERT 中的句子嵌入
	3. ![[Pasted image 20240314160458.png]]

### 数据处理
1. 为了使用 MLM 目标预训练密码模型（即预测屏蔽位置后面的屏蔽字符），我们将训练集中的每个密码 (Dtraining) 预处理为部分密码的形式（表示为pivot）：关联完整的密码（表示为 pwd）。我们遵循 BERT [12] 中相同的掩码比例：我们随机选择密码中 15% 的字符，然后分别以 80%、10% 和 10% 的概率用掩码符号、随机字符和未更改字符替换所选字符 。随机且未改变的字符可以防止模型记住被屏蔽的字符。一个密码可以预处理到很多个pivot，本文设置了20个pivot。我们设置预训练任务，找到参数 θ 来最大化以下似然：
![[Pasted image 20240314161946.png]]

### 数据集选择
我们使用 Rockyou-2021 作为我们的预训练数据集，该数据集非常大。为了在训练时间和模型性能之间取得平衡，我们从 Rockyou-2021 中随机抽取了 6000 万个密码。


### 计算性能
我们的工作是在一台配备 Nvidia GeForce RTX 2080 Ti 的 Ubuntu 20.04 机器上执行的，大约需要 2 天才能完成预训练。存储预训练模型需要 8.9 MB。

## 密码微调
根据特定的攻击场景定制预训练模型。微调方法通常包括架构修改和模型重新训练。

1. 架构修改期间，
	1. 通常修改预训练模型架构的任务特定层（表9中的全连接层和输出层），并保留预训练层，包括顶部输入层、嵌入层和几个 Transformer 块。
	2. 然后，我们使用特定的监督信号重新训练下游模型，以学习特定于任务的功能（例如密码规则兼容性）。（注意，重新训练过程中所有的参数都会改变：包括上面的所有层）。

# 用于现实世界攻击模型的 PassBERT
介绍三种 微调。

三种预训练模型：
1. 预训练的密码模型：PassBERT
2. 预训练的 BERT 自然语言模型：Vanilla BERT 
	Vanilla BERT 可以在字级、子字级和字符级对文本（即密码）进行标记，而我们仅根据词汇表将密码标记为字符序列。由于我们的 Vanilla BERT 的词汇表只有小写字母，因此我们扩展词汇表以涵盖 Σ 中的所有有效字符（通过替换未使用的标记） 
3. 随机变量模型：PassBERT

## 条件密码猜测
任务：pivot（例如，“p ＊ ＊ ＊w0rd ＊ ＊ ＊ ”） ————>恢复密码。
建模：掩码语言模型任务，预测缺失字符的 pivot 的条件概率
数据构建：使用与 CWAE [38, 39] 相同的策略创建：
1. 我们以一定比例（即 50%）随机用表示缺失字符的屏蔽符号替换每个字符。
2. 然后，我们只保留那些包含至少四个可观察字符和至少五个屏蔽符号的生成主元。
### 微调
攻击设计：根据 Pivot 的创建机制调整 屏蔽机制。
1. 增加掩码比例：只用掩码符号，
2. （默认的屏蔽机制结果不佳）
模型再训练：从中提取具有正确密码的有效枢轴以进行模型重新训练。
评估：
CPG 与 CWAE 具有相同的 评估 pivots 。
1. 对一个 pivot 生成相同数量的候选密码。然后，比较匹配的个数。
2. 按照匹配的个数分类这些 pivot 。区分不同频率的 pivot 的猜测性能
评估集： Neopets 和 Cit0day 空间较大：
1. WAE 仅为每个主元类别生成 30 个评估主元，可能会引起偏差。
2. 我们从评估集中为每个类别总共提取了 120 个评估基准
评估指标： 每个 pivot 的平均破解率。
结果：很好
预训练效果：用密码预训练的效果更好。
改进原则：模型通过以递减的概率耗尽屏蔽位置中的字符来生成候选密码，而 CWAE 通过解码器将潜在表示（即高维空间中的点）转换为候选密码。很优
计算性能：训练我们的 CPG 模型大约需要 2 天（8.9 MB），训练 CWAE 大约需要一天半的时间（4.4 MB）

## 定向的密码猜测
目标：用历史密码生成其变体
目标：对给定的密码训练模型输出编辑路径。

### 微调：
攻击设计： Pass2path 基于 RNN 模型的序列机制将密码转换为编辑路径。Transformer 不支持，修改：使用序列标记机制来预测密码中每个字符的一个编辑操作。其中一系列编辑操作构成编辑路径。在序列标注机制中，每个字符位置只能输出一次编辑操作。

为了适应序列标记机制，我们预先定义了新的编辑操作如下：保留（keep）、删除（del）和替换（rep1，rep2）这里，替换涉及用一个（表示为rep1）或两个字符替换（表示为rep2）。

缺陷：无法捕获一些转换，比如：插入三个字符
模型结构：
![[Pasted image 20240314203049.png]]
 修改特定于任务的层以学习密码中每个字符的编辑操作（例如，(op, str)）的概率。
模型再训练：
遵循 Pass2path [35]，我们在 BreachCompilation 中从同一用户中随机抽取 80% 的密码对，并在最小编辑距离不超过 4 时选择合格的密码对，最终得到 85,269,455 个密码对。

评估：
评估设置： 和 Pass2pass 保证相同数据集训练对
评估指标。我们选择特定用户泄露的密码之一作为输入，然后根据 TPG 模型推断其变体（限制为 1,000 次猜测）计算了 Ncracked Naccounts 给出不同猜测（即 10、100 和 1,000）的评估用途/帐户之间的破解率
实验结果：三个模型都高于 Pass2path

预训练效果：密码和自然语言模型在 TPG 中表现出边际收益。发现预训练的改进空间仍然很小，这表明预训练的作用较弱。这种现象是可以理解的，因为有针对性的攻击集中在个性化密码转换上，这与预训练模型中的全局密码分布关系不大。两个预训练模型生成的上下文嵌入在帮助理解密码转换方面几乎没有效果。 TPG 往往依赖于任务，主要根据特定攻击的数据集形成其模型参数。

改进原则：更关注本地角色信息。准确地说，我们的编辑操作二元组 (op, str) 可以比 Pass2path 中使用的三元组操作 (op, str,pos) 减少一维搜索空间，其中 pos 指密码中的位置。 

看法：随着编辑距离的增加，破解率下降。

计算性能比较。训练 PassBERT (82 MB) 和 Pass2path (166 MB) 分别需要大约 13.6 和 42 小时。我们根据经验计算得出，两种模型都实现了相似的推理速度。 PassBERT 和 Pass2path 每秒可以推断 4.38 和 4.63 个密码对（pairs/s）。

## 基于自适应规则的密码猜测
目标：每个单词仅与选定的自适应修改规则关联以生成猜测。
		[37]尝试使用默认变压器，但与 ADaM 相比没有取得实质性改进。
本质：找出单词适应的规则。
效果：输入单词和规则输出破解的可能性
方式：构建分类模型，
### 微调
捕获整个单词和密码级别的修饰规则之间的自适应关系，因为修饰规则最终适用于单词。
监督信号：是在两个假设数据集上带有标记规则（即命中规则）的单词。
模型架构：
1. 修改了特定任务层，推断重整规则和具有焦点损失函数的基本单词之间的自适应概率。
2. 输出层没有序列长度。
模型再训练：
1. 针对两个规则集训练了两个 APRG 模型
2. 使用 2 亿个密码作为目标空间
3. 使用 1000 万条数据作为初始空间
4. 训练：Ri 是个向量，每个元素对应一个规则的匹配可能性
	注意：0和1的比例极其不平衡，即超过95%的标签都是0。因此，我们应用与ADAM相同参数策略的焦点损失[25,37]来关注硬标签。
5. 设置一个 阈值 来表示规则使用的条件。

### 评估
评估设置：比较了静态和动态模型。
专注于单词和规则之间的关系建模，即我们的目标是为单词选择更具适应性的规则，以在相同的猜测下获得更高的猜测效率。
评估指标：动态策略模型的最终破解率作为评估指标。
试验结果：动态模型和静态模型都高于 ADaMs
1.  Dynamic PassBERT 在这些模型中实现了最高的破解率，并且在四个实验中比 ADaM 平均提高了 4.86%。
2.  静态 ARPG 的显着提前停止是因为每个单词仅与自适应规则关联，其大小小于标准基于规则的攻击中的所有规则。
预训练效果：
1. 密码预训练使 Transformer 的性能优于 ADaM
2. 用默认 Transformer 基本没改进。所以这些个模型的改进没有效果。
改进原则：密码预训练对于 ARPG 的提升有着重要的作用。 
见解：发现了容易收到攻击的破坏规则。
计算性能比较：
1. 我们两个小时： ADam 需要 十个小时
2. 推理速度相似。
# 密码强度估计


# 讨论
密码双向性：
1. 双向 Transformer 在文本特征提取方面比自动编码器 (CWAE)、RNN (Pass2path) 或 CNN (ADaMs) 等领先方法效果更好，
2. 工作[22]已经设置了单向变压器，并显示了双向训练机制在一般猜测任务中的优越性。
3. 密码具有双向性，对字符进行不同的权重。
4. 预训练对于提高猜测效率也能起到重要作用。深度学习模型通常由通用预训练层和特定于任务的层组成，其中预训练层的上下文嵌入层可以为任务相关层提供更多上下文信息。
预训练的合适场景猜测：
密码训练和自然语言训练：只能为有针对性的攻击提供效益