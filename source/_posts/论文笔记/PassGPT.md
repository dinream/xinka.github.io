# 摘要
1. LLM 无需显式监督： PassGPT
2. 引导密码生成：利用 PassGPT 来生成匹配任意约束的密码。
# 引言
贡献：
1. 密码猜测
2. 密码强度估计
PassGPT 多猜测 20% 的未见过的密码，并且对新的泄露展现了良好的泛化能力。
此外，我们通过矢量量化增强了 PassGPT [55]。由此产生的架构是 PassVQT，它可以增加生成密码的复杂性。

与之前整体生成密码的深度生成模型不同，PassGPT 对每个字符进行顺序采样，从而引入了引导密码生成的新颖任务。此方法确保对搜索空间进行更细粒度（字符级）的引导探索，其中根据任意约束对生成的密码进行采样。

最后，与 GAN 相比，PassGPT 提供了密码概率分布的明确表示。我们证明密码概率与最先进的密码强度估计器一致：PassGPT 为更强的密码分配较低的概率。

总结如下：
– 我们引入了 PassGPT，一种自回归转换器，它在密码生成和对未见过的数据集的泛化方面获得了最先进的结果。 
– 我们展示 PassGPT 如何在任意约束下启用一种新颖的密码生成方法：引导密码生成。 
– 我们检查 PassGPT 下的密码概率以及它们如何与强度保持一致。我们讨论如何使用该指标来改进当前强度估计器。 
 – 我们提出了 PassVQT，这是一种通过矢量量化增强的类似架构，以增加生成的复杂性。

# 实验设置


## 数据集
![[Pasted image 20240319110425.png]]
1. 最主要的是  RockYou 和 LinkedIn
2. 对于 RockYou，我们分别获取最多 10 个和 16 个字符的所有密码列表。
	1. 我们将此列表中的 80% 作为训练数据。
	2. 在剩下的 20% 中，我们将训练分割中未包含的所有密码保留为测试数据，仅保留频率较低的密码。
		1. 测试集中最常用的密码仅出现了7 次
		2. 平均频率为 1.03
		3. 有助于测试生成低概率密码的能力。
3. 对于 LinkedIn 
	1. LinkedIn 泄露事件没有提供有关密码频率的信息，因此我们将 80% 作为训练数据，其余 20% 用于评估。同样保证没有相同的数据 同时 出现在两组中
	2. 评估集中删除 RockYou 训练密码来定义交叉评估测试集，反之亦然，以评估对未见分布的泛化。
4. 最后，我们还将 MySpace、phpBB 和 Hotmail [52] 泄漏视为评估集。我们执行类似的交叉评估程序，从所有数据中删除 RockYou 和 LinkedIn 训练数据。 OST 10 OST。

## 模型
自回归生成模型。PassGPT 和PassVQT 对密码中某个字符的出现概率进行建模，给定前面的值，对密码中某个字符的概率进行建模，给定前面的值，从分布中顺序采样可以生成可能的密码。
1. 模型在词汇表 Σ 上运行，包含 256 个 UTF-8 字符。
2. tokenizer 定义为将词汇表中的每个字符 σ 映射为整数的函数，分词器：Σ 7→ [0, |Σ| − 1]。
3. 然后，在 tokenizer 函数下使用其图像的 one-hot 编码为每个 token σ 创建向量表示。这会产生一个维度向量 |Σ|，在标记器 (σ) 位置处所有条目均等于 0，单个条目等于 1。
### PassGPT
![[Pasted image 20240319150308.png]]
GPT 模型利用 Transformer 的解码器组件，并经过训练以自回归方式预测序列中的下一个标记。
1. 为了预测密码中的特定字符 xi，转换器解码器仅考虑之前的字符 x0,...。 。 。 ，xi−1 作为输入，
2. 并输出一个维度为 d 的潜在向量（在我们的工作中 d = 768）。
3. 然后这个潜在向量被映射到维度 |Σ| 的实数向量通过线性层并使用 softmax 函数进一步转换为词汇表上的概率分布。
4. 词汇表上的输出分布表示 p(xi|x<i; θ)。使用相对于在该位置找到的真实字符的独热编码表示的交叉熵损失来优化该分布。
一旦网络经过训练，它就会为我们提供基于先前标记的词汇表的参数化分布，即 p(xi|x<i; θ)。
1. 我们可以从密码开始标记 <\s>  开始，并找到 p(x1|x0 = <\s>)。这为我们词汇表中的每个字符分配了成为密码中第一个标记的概率。如果我们从这个分布中采样，我们可以固定第一个字符并通过计算 p(x2|x0, x1) 重复该过程来找到第二个字符。当在任何给定步骤从分布中采样到密码结束标记 <\/s> 时，密码采样过程即告完成。
	1. 与训练不同，此过程是连续的。我们的 PassGPT 实现使用 HuggingFace 库 [53]，并具有以下规范：12 个注意力头、8 个解码器层和 GeLU 激活 [24]。此外，我们使用 AdamW 优化器对所有模型进行 1 轮训练，起始学习率为 5e-5，训练期间线性衰减。
# PassVQT
使用潜在空间的矢量量化增强了转换器架构。在计算机视觉领域，这已被证明可以提高样本质量[55]。 PassVQT 遵循 Yu 等人设计的架构。  [55]。 
1. 在对与 PassGPT 相同的条件分布进行建模时，我们的目标是评估量化是否可以提供任何额外的好处。
2. 在图 2 所示的架构中，变压器编码器将每个输入标记映射到维度为 768 的潜在表示。
3. 然后使用线性层将该潜在表示映射到 10 维，并使用 k 均值和具有 N 的码本进行量化条目。量化的 10 维向量通过线性层映射回 768 维，并作为变压器自回归解码器的输入。
4. 该解码器经过训练，可以仅使用先前标记的量化表示来逐个字符地重建输入密码。
5. 我们通过最小化 RockYou 泄漏训练分割的重建损失来进行超参数搜索。
6. 我们的研究结果表明，更深的编码器和解码器结构可以提供更好的结果，码本大小为 300 可以提供最佳性能。
7. PassVQT 采用变压器编码器和 GPT-2 解码器，分别具有 12 个注意力头和 8 个层。它是使用 HuggingFace 库 [53] 实现的，并使用 AdamW 优化器进行端到端训练，起始学习率为 5e-5，具有线性衰减。一旦编码器-解码器网络收敛，该模型就可以从压缩的量化潜在表示重建输入密码。如果我们对潜在代码的分布进行建模，我们可以从中采样以生成可能的代码序列，然后解码器可以将其转换为可能的密码。为此，我们在训练数据集的量化表示上训练自回归代码模型。在推理过程中，我们通过从代码模型中采样代码序列并使用原始解码器将它们转换为密码来创建新密码。不再需要编码器。

4.3 引导式生成 
我们提出了一种新颖的密码生成方法：引导式密码生成。
与之前生成整体密码的深度生成方法不同，PassGPT 分别对每个令牌进行建模，从而授予对每个字符的完全控制权。
这使得生成过程能够满足特定的约束。
这些限制的一些示例包括：密码长度、固定字符（例如，第一个位置的“a”）和模板（例如，四个小写字母和两个数字）。
这可以通过限制采样分布 p(xi|x0, · · ·, xi−1) 以仅考虑分配给感兴趣子集 Σ′ ⊂ Σ 的概率质量来实现；例如，限制 Σ′。

## 实验结果
将 PassGPT 和 PassVQT 和最新的深度生成模型，并演示了它们对不同数据集的泛化，而无需进一步训练。
检查了 PassGPT 下密码的概率和熵，以深入了解其功能和建模分布。

训练集的两种变体：（1）唯一密码和（2）所有出现的密码。
1. 在使用唯一密码进行训练时，PassGPT 表现出卓越的泛化能力，
2. 相反，在使用唯一条目进行训练时，PassVQT 会遇到生成分发内密码的困难，但在合并其绝对出现次数后会显着改进。
3. 从 PassGPT（针对唯一密码进行训练）和 PassVQT（针对所有密码进行训练）中对越来越大的密码猜测池进行采样，对越来越大的密码猜测池进行采样，并计算它们恢复的 RockYou 测试分割的百分比。
4. 结果表明 PassGPT 优于所有其他模型。它在 109 个猜测中恢复了测试集的 41.9%，而最先进的 GAN 模型的匹配率为 23.33%。 

密码生成评估的另一个重要因素是生成新颖且独特样本的能力。
	在 109 次猜测中，PassGPT 保留唯一密码的比例最高 (60%)，而 PassVQT 则下降至 20%。由于 PassVQT 是针对所有出现的密码进行训练的，因此在其发行版下

测试集的恢复比例
![[Pasted image 20240327105738.png]]

长密码效率：
PassGPT 在对独特样本进行训练时表现最佳，PassVQT 现在在使用唯一密码进行训练时获得了更好的性能。在对这个新分布进行训练后，模型保持了相似的准确性。从 108 个猜测中，PassGPT 和 PassVQT 分别恢复了测试集的 15.5% 和 8.57%，而在 10 个字符设置中则为 19.37% 和 10.30%。

模型对为见过密码分布的泛化能力：。分别用 RockYou 和 LinkedIn 的数据集进行训练，然后都用  LinkedIn 的测试集测试。
![[Pasted image 20240327142047.png]]

用 Rockyou 训练的模型 泛化效果更好
![[Pasted image 20240327111524.png]]
