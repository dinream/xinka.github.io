## 自注意力机制

[https://0809zheng.github.io/2020/04/24/self-attention.html](https://0809zheng.github.io/2020/04/24/self-attention.html)

TODO

下面是关于NLP中常用的一些知识的简要说明：

1. Attention（注意力机制）： Attention是一种**用于提升神经网络模型性能的机制**，特别是在序列任务中。它允许**模型在处理输入序列时将重点放在相关的部分上。**通过计算每个输入位置的权重，模型可以自适应地决定要关注哪些部分。注意力机制在机器翻译、文本摘要、问答系统等任务中得到广泛应用。
    
2. Transformer（变换器）： Transformer是一种基于注意力机制的神经网络架构，用于处理序列数据。它在自然语言处理任务中取得了重大突破，并成为许多最先进的模型的基础。Transformer通过自注意力机制（self-attention）来捕捉输入序列中的依赖关系，避免了传统循环神经网络中的顺序计算，并且能够并行处理输入。Transformer的典型应用包括机器翻译（如Google的Transformer模型）和语言模型。
    
3. BERT（Bidirectional Encoder Representations from Transformers）： BERT是一种预训练的语言表示模型，基于Transformer架构。通过在大规模文本数据上进行无监督的预训练，BERT可以学习出通用的语言表示，然后可以在各种下游任务上进行微调。BERT引入了掩码语言模型（Masked Language Model, MLM）和下一句预测（Next Sentence Prediction, NSP）等任务来训练模型。BERT的出现对各种NLP任务，如文本分类、命名实体识别、问答系统等都产生了显著影响。
    
4. GPT（Generative Pre-trained Transformer）： GPT是一种基于Transformer架构的预训练语言模型，用于生成文本。GPT通过在大规模文本数据上进行自监督的预训练，学习出对输入序列的概率分布建模能力。然后，可以使用该模型生成具有连贯性和语法正确性的文本。GPT模型在生成式任务中表现出色，如文本生成、对话系统、机器写作等。
    
5. Prompt（提示）： Prompt是指在进行自然语言处理任务时，向模型提供一种任务描述或问题陈述的方式。通过给定一个显式的提示文本，模型可以更好地理解任务需求和上下文，从而产生更准确的输出。Prompt工程化是近年来在NLP任务中的一种重要技术，它可以帮助改进模型的可控性、减少模型的偏见，并提高模型的性能。
    

下面是这些概念之间的关系：

1. Attention（注意力机制）是Transformer（变换器）模型的核心组件之一。Transformer通过自注意力机制实现了对输入序列的建模，其中每个位置可以根据其与其他位置的相关性来调整其重要性。
    
2. Transformer是一种神经网络架构，被广泛用于自然语言处理任务。它的设计中包含了多头注意力机制，可以同时关注不同位置的不同方面。Transformer的出现使得处理长序列数据变得更加高效，并在机器翻译、文本生成等任务中取得了显著的性能提升。
    
3. BERT（Bidirectional Encoder Representations from Transformers）是基于Transformer的预训练语言模型。它通过大规模的无监督预训练，在理解上下文和建模语言表示方面取得了巨大成功。BERT的预训练模型可以通过微调适应各种下游任务，如文本分类、命名实体识别等。
    
4. GPT（Generative Pre-trained Transformer）也是基于Transformer的预训练语言模型，但其目标是生成连贯的文本。GPT通过自监督学习来提前训练一个语言模型，然后可以用于生成各种文本，如文章、对话等。GPT模型在生成式任务中表现出色，可以产生具有语法正确性和连贯性的文本。
    
5. Prompt（提示）是在进行NLP任务时向模型提供任务描述或问题陈述的方式。Prompt的引入主要是为了改进模型的可控性和减少模型的偏见。通过设计合适的提示文本，可以引导模型在特定任务上产生更准确、更符合预期的输出。
    

总体而言，Attention是一种机制，Transformer是一种基于Attention的网络架构，BERT和GPT是基于Transformer的预训练语言模型，而Prompt是一种用于指导模型输出的技术手段。它们在NLP领域中相互关联，相互借鉴，共同推动了自然语言处理技术的发展。

## 深度卷积神经网络

至少在网络的一层中使用卷积运算来代替一般的矩阵乘法运算的神经网络，因此命名为卷积神经网络

![img](file://D:\Users\fyn\Documents\Note\Typora\%E5%AD%97%E5%85%B8%E7%94%9F%E6%88%90\%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.assets\b98e8b23b2eec30346b082aa1fbca27ca8c377ab.png@1256w_712h_!web-article-pic.webp?lastModify=1708588580)

### 卷积核

卷积核（kernel） ，一个权重矩阵，逐步在二维输入数据上“扫描”，卷积核“滑动”的同时，计算权重矩阵和扫描所得的数据矩阵的乘积，然后把结果汇总成一个输出像素。

**大小一般为奇数：**原因

1. 更容易 padding，计算结果为整数
    
2. 使用奇数大小的卷积核还可以保持对称性。对称性在卷积操作中是重要的，因为它可以确保输出特征图的空间分辨率与输入特征图相同。如果使用偶数大小的卷积核，由于缺少中心像素，可能会导致输出特征图的空间分辨率减小。
    
3. 在CNN中，进行卷积操作时一般会以卷积核模块的一个位置为基准进行滑动，这个基准通常就是卷积核模块的中心。 卷积核大小为奇数时，它具有一个中心元素，这使得在进行卷积操作时，可以确保输入图像的每个像素都有对应的卷积核元素与之对齐。这种中心对齐的特性有助于提取局部特征，同时减少了信息丢失的可能性。
    

### 卷积（Convolution）

所谓的卷积运算，其实它被称为**互相关（cross-correlation）运算：**将图像矩阵中，从左到右，由上到下，取与滤波器同等大小的一部分，每一部分中的值与滤波器中的值对应相乘后求和，最后的结果组成一个矩阵，其中没有对核进行翻转。

> 数学上，给定两个函数 f(x) 和 g(x) 的卷积运算表示为：
> 
> (f * g)(x) = ∫[−∞,∞] f(t)g(x−t) dt
> 
> 对于每一个 x 得到的值 是 g 在 x 偏移 t 处 的值乘以 f(t) (权重) 的累加值，

![img](file://D:\Users\fyn\Documents\Note\Typora\%E5%AD%97%E5%85%B8%E7%94%9F%E6%88%90\%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.assets\d646245dc94d4788dc48d0fcf6cd358f9d29f2c6.gif@1256w_1334h_!web-article-pic.webp?lastModify=1708588580)

**核心操作，用于提取特征**

卷积运算具有一些重要的性质，例如交换律、结合律和分配律。

深度学习里面所谓的卷积运算，其实它被称为**互相关（cross-correlation）运算：**将图像矩阵中，从左到右，由上到下，取与滤波器同等大小的一部分（f(x-t)），每一部分中的值与滤波器中的值（g(t)）对应相乘后求和，最后的结果组成一个矩阵，其中没有对核进行翻转。

### 填充（Padding）

**避免信息损失**

输入图像与卷积核进行卷积后的结果中损失了部分值，输入图像的边缘被“修剪”掉了（边缘处只检测了部分像素点，丢失了图片边界处的众多信息）。这是因为边缘上的像素永远不会位于卷积核中心，而卷积核也没法扩展到边缘区域以外。

这个结果我们是不能接受的，有时我们还希望输入和输出的大小应该保持一致。为解决这个问题，可以在进行卷积操作前，对原矩阵进行边界**填充（Padding）**，也就是在矩阵的边界上填充一些值，以增加矩阵的大小，通常都用“”来进行填充的。

![img](file://D:\Users\fyn\Documents\Note\Typora\%E5%AD%97%E5%85%B8%E7%94%9F%E6%88%90\%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.assets\c883f882924c3bfe1784b2a8b1c507c3dbe42963.gif@1256w_1428h_!web-article-pic.webp?lastModify=1708588580)

通过填充的方法，当卷积核扫描输入数据时，它能延伸到边缘以外的伪像素，从而使输出和输入size相同。

常用的两种padding：

（1）valid padding：不进行任何处理，只使用原始图像，不允许卷积核超出原始图像边界

（2）same padding：进行填充，允许卷积核超出原始图像边界，并使得卷积后结果的大小与原来的一致 作者：2kb的卷心菜 [https://www.bilibili.com/read/cv16432604/](https://www.bilibili.com/read/cv16432604/) 出处：bilibili

### 步长(Stride)

**压缩一部分信息，或者使输出的尺寸小于输入的尺寸**

[每天五分钟计算机视觉：卷积步长(Stride) (baidu.com)](https://baijiahao.baidu.com/s?id=1781826967010713030&wfr=spider&for=pc)

**滑动卷积核时**，我们会先从输入的左上角开始，每次往左滑动一列或者往下滑动一行逐一计算输出，我们将**每次滑动的行数和列数**称为Stride，在之前的图片中，Stride=1；在下图中，Stride=2。

![img](file://D:\Users\fyn\Documents\Note\Typora\%E5%AD%97%E5%85%B8%E7%94%9F%E6%88%90\%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.assets\af6e5cdf758fc9f2b6ba30235eae9e1c8128576b.gif@!web-article-pic.webp?lastModify=1708588580)

[https://www.bilibili.com/read/cv16432604/](https://www.bilibili.com/read/cv16432604/) 出处：bilibili-2kb的卷心菜

### 通道

比如 RGB 有 红绿蓝三个 通道

这里就要涉及到“卷积核”和“filter”这两个术语的区别。在只有一个通道的情况下，“卷积核”就相当于“filter”，这两个概念是可以互换的。但在一般情况下，它们是两个完全不同的概念。每个“filter”实际上恰好是“卷积核”的一个集合，在当前层，每个通道都对应一个卷积核，且这个卷积核是独一无二的。

**多通道卷积的计算过程**：将矩阵与滤波器对应的每一个通道进行卷积运算，最后相加，形成一个单通道输出，加上偏置项后，我们得到了一个最终的单通道输出。如果存在多个filter，这时我们可以把这些最终的单通道输出组合成一个总输出。

**某一层输出特征图的通道数**=当前层滤波器的个数。如上图所示，当只有一个filter时，输出特征图（4×4）的通道数为1；当有2个filter时，输出特征图（4×4×2）的通道数为2。

### 降采样

[https://zhuanlan.zhihu.com/p/46633171](https://zhuanlan.zhihu.com/p/46633171)

概念：降采样指的是成比例缩小特征图宽和高的过程

例子：比如从（W，H）变为（W/2，H/2）

方法：

1. stride 大于 1 的 pooling
    
2. stride 大于 1 的 conv
    
3. stride 大于 1 的 reorg（在[YOLOv2的论文](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1612.08242.pdf)里叫passthrough layer）
    

比较：

1. 1 和 2 在深度卷积神经网络中使用非常普遍，3 比较小众，由Joseph Redmon在YOLOv2中首次提出。
    
2. 1 和 2 的对比在[Striving for Simplicity: The All Convolutional Net](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1412.6806)中有详述
    
    用stride=2的conv降采样的卷积神经网络效果与使用pooling降采样的卷积神经网络效果相当；**卷积神经网络小的时候，使用pooling降采样效果可能更好**，卷积神经网络大的时候，使用stride=2的conv降采样效果可能更好。
    
    pooling提供了一种非线性，这种非线性需要较深的conv叠加才能实现，因此当网络比较浅的时候，pooling有一定优势；但是当网络很深的时候，多层叠加的conv可以学到pooling所能提供的非线性，甚至能根据训练集学到比pooling更好的非线性，因此当网络比较深的时候，不使用pooling没多大关系，甚至更好。
    
    pooling的非线性是固定的，不可学习的，这种非线性其实就是一种先验。
    
3. 3 中降采样的优势在于能够较好的保留低层次的信息。**1 和 2 的降采样方式，好处是抽取的特征具有更强的语义性，坏处是会丢失一些细节信息**。而3这种降采样方式与1、2相反，**3 提取的特征语义性不强，但是能保留大量细节信息**。所以当我们既需要降采样，又需要不丢失细节信息的时候，3是一个非常合适的选择。
    

### 升采样

**将输入特征图的尺寸放大或增加分辨率。**

通常与卷积和池化等操作结合使用，用于逆向传播梯度、特征图的恢复或生成更高分辨率的输出。

基本目标是增加特征图的空间尺寸，以便更好地捕获细节信息、提高特征的表达能力或生成更高分辨率的输出。

方法：

1. 反卷积（Transpose Convolution）：反卷积是一种常见的升采样方式，也称为转置卷积、分数步长卷积。它通过在**输入特征图之间插入零值**，并**使用带有适当步长的卷积核进行卷积操作来放大特征图的尺寸**。反卷积操作可以增加特征图的空间尺寸，并在某种程度上恢复输入特征图的细节。
    
2. 双线性插值（Bilinear Interpolation）：双线性插值是一种基于插值的升采样方法，通过**对输入特征图中的每个像素进行插值计算来生成更大尺寸的特征图。**它使用**周围四个像素的权重进行插值，保持了图像的平滑性和连续性**。
    
3. 最近邻插值（Nearest Neighbor Interpolation）：最近邻插值是一种简单的升采样方法，它将输入特征图中每个像素的值复制到放大后的特征图的相应位置。它在放大时**不进行插值计算，而是直接使用最近邻像素的值**。这种方法简单高效，但可能**会导致输出特征图的锯齿状边缘**。
    
4. 其他一些升采样技术，如子像素卷积（Subpixel Convolution）、转置卷积的变种（如反池化操作）、像素重排（Pixel Shuffle）等。这些方法在不同的应用场景中具有各自的优势和适用性。
    

### 池化层

是一种常用的操作层，用于减小特征图的空间尺寸、降低计算量，并增强模型的平移不变性。输入特征图的局部区域进行聚合或采样来生成池化特征图。

池化操作：通常在每个输入特征图的局部区域上应用，通过对区域内的特征进行聚合或采样，生成一个单一的值或特征。这个聚合或采样过程可以是简单的求最大值（最大池化，Max Pooling）或求平均值（平均池化，Average Pooling），也可以是其他聚合方式，如Lp范数池化。

在池化操作中，可以通过调整池化窗口的大小和步幅来控制输出特征图的大小和感受野。池化窗口是应用池化操作的局部区域的大小，**步幅**是**池化窗口**在输入特征图上移动的距离。

常见的池化操作及其特点：

1. 最大池化（Max Pooling）：在池化窗口内选择最大值作为池化特征。最大池化有助于保留显著的特征，提高模型的平移不变性和鲁棒性。
    
2. 平均池化（Average Pooling）：在池化窗口内求特征的平均值作为池化特征。平均池化可以减少特征图的空间维度，并平滑特征。
    
3. Lp范数池化（Lp-norm Pooling）：在池化窗口内对特征进行Lp范数归一化，得到池化特征。Lp范数池化可以对特征进行归一化，并引入更多非线性。
    

池化操作在CNN中具有以下优势：

- 减小特征图的空间尺寸，降低计算量和内存需求。
    
- 提取特征的局部不变性，使模型对目标在图像中的位置变化具有鲁棒性。
    
- 减少模型过拟合的风险，通过减少参数数量和引入局部平均化。
    

池化层通常与卷积层交替使用，以构建深层次的卷积神经网络结构。它在图像分类、目标检测、图像分割等任务中广泛应用，并在提高模型性能和减少计算资源消耗方面发挥着重要作用。

### batch size

"batch size"（批大小）是指在一次训练迭代中同时输入模型的样本数量。批大小决定了在每一次参数更新时，模型所看到的样本数量。

理解批大小可以参考以下几点：

1. **样本数量**：批大小表示一次训练中同时处理的样本数量。例如，如果批大小为32，则在每次参数更新时，模型将同时处理32个样本。
    
2. **内存和计算效率**：较大的批大小可以提高计算效率，因为同时处理多个样本可以充分利用并行计算的能力。然而，较大的批大小可能需要更多的内存存储模型的中间结果。
    
3. **梯度估计**：在训练过程中，批大小还会影响对梯度的估计。较大的批大小可以提供更准确的梯度估计，因为它们包含了更多的样本信息。然而，较小的批大小可能导致模型收敛更快，因为它们更频繁地更新参数。
    
4. **泛化能力**：较大的批大小可能会导致模型过度拟合训练数据，因为它们更倾向于记住样本特定的细节。较小的批大小可以提供更好的泛化能力，因为它们更强迫模型学习更一般化的特征。

### epoch（时期） 
    Epoch（时期）是指将整个训练数据集（包含多个批次）在模型中进行一次完整的训练。在一个epoch中，模型会对数据集中的每个样本都进行一次前向传播和反向传播，并根据损失函数计算的梯度来更新模型的参数。

训练过程通常涉及多个epoch，因为一次完整的训练可能不足以使模型达到最佳性能。通过进行多个epoch，模型可以多次观察和学习数据的不同方面，并逐渐改善其性能。每个epoch之间的样本顺序通常会被随机化，以避免模型对样本顺序的依赖。

### 向前步骤

**向前步骤（Forward Propagation）**：

- 在向前步骤中，输入样本通过模型的前向计算过程，从输入层经过一系列的神经网络层传递，最终得到模型的输出预测结果。
    
- 在每一层中，通过对输入数据进行线性变换（加权和）和非线性变换（激活函数），将信息从前一层传递到后一层，直到到达输出层。
    
- 向前步骤的目的是计算出模型的预测结果，以便与真实标签进行比较，并计算出损失函数的值。
    

### 反向传播步骤

**反向传播步骤（Backpropagation）**：

- 在反向传播步骤中，根据向前步骤中计算得到的损失函数值，通过链式法则计算每个参数对损失的贡献度，并更新模型的参数。
    
- 反向传播通过对损失函数关于模型参数的偏导数进行计算，从输出层向输入层逐层传递，以确定每个参数的梯度。
    
- 梯度表示了损失函数对每个参数的变化率，利用梯度可以确定参数的更新方向和大小，以最小化损失函数

### 梯度的惩罚程度
梯度的惩罚程度是指在计算梯度时对其进行的限制或惩罚的程度。在深度学习中，常用的梯度惩罚方法是通过添加正则化项或其他惩罚项来约束模型的参数更新。

#### 目的
防止参数更新过大或过快，以避免模型的过拟合或不稳定情况。通过对梯度进行惩罚，可以限制参数更新的幅度，使其保持在合理的范围内。

梯度惩罚的程度可以通过调整惩罚项的权重或超参数来控制。增加梯度惩罚的程度意味着更强烈地限制梯度的大小或方向，从而减缓参数更新的速度。相反，减少梯度惩罚的程度会使梯度对参数更新的限制更加宽松。

惩罚程度的选择通常需要根据具体问题和数据集的特点进行调整。过强的梯度惩罚可能导致模型无法学习到有效的特征，而过弱的梯度惩罚可能导致模型过拟合或训练不稳定。因此，需要根据实际情况进行实验和调优，选择合适的梯度惩罚程度以获得最佳的模型性能。


### 自动编码器

自动编码器（Autoencoder，AE）是一种无监督学习模型，用于学习数据的特征表示和压缩。它由编码器（Encoder）和解码器（Decoder）组成。

1. 欠完备自动编码器（Undercomplete Autoencoder）：  
    欠完备自动编码器是指编码器的维度低于输入数据的维度。这种设置迫使模型学习数据的主要特征，因为编码器无法完全捕获原始数据的所有信息。通过限制编码器的容量，欠完备自动编码器可以捕捉数据中最显著的特征，从而实现特征选择和降维。
    
2. 正则化自动编码器（Regularized Autoencoder）：  
    正则化自动编码器通过在损失函数中引入额外的正则化项来约束模型的学习过程，以防止过拟合。常见的正则化方法包括L1正则化和L2正则化。L1正则化通过增加编码器的稀疏性，鼓励模型只使用输入数据的少数关键特征。L2正则化通过限制权重的大小，使模型对输入数据的小变化具有鲁棒性。
    
3. 变分自动编码器（Variational Autoencoder，VAE）：  
    变分自动编码器是一种生成性模型，与判别性模型（欠完备自动编码器和正则化自动编码器）不同，它可以生成新的数据样本。VAE通过在潜在空间中引入随机性，使得模型能够在潜在空间中进行随机采样，并通过解码器生成新的样本。在训练过程中，VAE通过最大化“证据下界”（evidence lower bound，ELBO）来优化模型参数，从而实现对潜在空间的建模。这使得VAE能够学习到数据的潜在分布，并通过从该分布中采样生成新的数据样本。 

### CIFAR-10 和 CIFAR-100

CIFAR[数据集](https://so.csdn.net/so/search?q=%E6%95%B0%E6%8D%AE%E9%9B%86&spm=1001.2101.3001.7020)是 [Visual Dictionary (Teaching computers to recognize objects)](http://groups.csail.mit.edu/vision/TinyImages/) 的子集，由三个教授收集，主要来自google和各类搜索引擎的图片。

[cifar10和cifar100(简介&可视化)_cifar10和cifar100区别-CSDN博客](https://blog.csdn.net/disanda/article/details/90744243)

### MNIST数据集

[https://blog.csdn.net/Iron802/article/details/121826385](https://blog.csdn.net/Iron802/article/details/121826385)

MNIST数据集是NIST（National Institute of Standards and Technology，美国国家标准与技术研究所）数据集的一个子集，MNIST 数据集可在 [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/) 获取，主要包括四个文件：