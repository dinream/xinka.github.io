https://www.bilibili.com/video/BV11G411X7nZ/?p=2&vd_source=31f1c950b5b95af0c48f188f0bc047c7

# 基础概念
## NLP
分类：
1. 中间任务（序列标注）：中文分词，词性标注，NER，句法分析，指代消解，语义 Parser 等，
2. 最终任务：文本分类，文本相似性计算，机器翻译，文本摘要。
	1. 自然语言理解类任务：本质上是分类任务：分类任务、句子关系判断
	2. 自然语言生成类任务

## NLP 三大特征提取器（CNN|RNN|TF）
https://zhuanlan.zhihu.com/p/54743941
1. **一个特征抽取器是否适配问题领域的特点，有时候决定了它的成败，而很多模型改进的方向，其实就是改造得使得它更匹配领域问题的特性**
2. **解决 NLP 任务 最重要的就是 模型的特征提取能力**

### 现状
1. RNN 基本完成它的历史使命。
2. CNN 如果改造得当，有希望有自己在NLP领域的一席之地。
3. Transformer 最主流的特征提取器

### RNN 现状
#### RNN 在 NLP 中的演进
1. RNN 采取线性序列结构不断从前往后输入信息，这种线性结构在反向传播时存在优化困难问题，因为反向传播路径太长，容易导致严重的梯度消失或者梯度爆炸问题。
2. 引入 LSTM 和 GRU 模型，通过增加中间状态信息直接向后转播，以此缓解梯度消失问题，获得很好的效果。
3. 不断优化，从图像领域引入 Attention 机制，叠加网络使得层更深。
4. 引入 Encoder-Decoder 框架。
#### RNN 的优势
1. RNN 的结构天然适配解决 NLP 问题，NLP 问题的输入往往是一个不定长的线性序列句子。
2. 而 RNN 本身结构 就是一个可以接纳不定长输入的由前向后进行信息线性传导的网络结构
3. LSTM 引入三个门， 对于捕获长距离特征也是非常有效的
#### RNN 的问题
1. 老模型先天不如新来的 CNN Transformer 。
2. RNN 本身的序列依赖结构对于大规模并行计算来说相当不友好。而 CNN 和 Transformer 不存在这种问题。
本质：
	时间步有前后依赖
#### RNN 并行改造
1. 保留连续时间步的隐层连接
	1. 在隐层单元之间并行计算，
2. 部分打断连续时间步
	1. （这样改进之后有点像简化的 CNN）失去原本样貌

### CNN 现状
#### 怀旧版 CNN 
1. 输入层
2. 卷积层：特征提取层，卷积核（filter）
3. pooling 层：对 Filter 的特征进行降维操作（从一个卷积核获得的特征向量里只选中并保留最强的那一个特征）
4. 输出层
#### 怀旧版 CNN 的问题
1. 卷积层如何捕获远距离特征
	> CNN 卷积层捕获的实际上是单词 的 k-gram 片段信息，k 的大小决定了能捕获多远距离的特征
	1. 不是覆盖连续区域，在同样的滑动窗口大小的前提下，覆盖不连续区域
	2. 增加卷积层层数。
2. Pooling 层
	> Pooling的操作逻辑是：从一个卷积核获得的特征向量里只选中并保留最强的那一个特征，所以到了Pooling层，位置信息就被扔掉了，这在NLP里其实是有信息损失的。

	所以在 NLP 领域里，目前 CNN 的一个发展趋势是抛弃 Pooling 层，靠全卷积层来叠加网络深度
#### 怀旧版 CNN 的优势
并行计算能力：单层卷积层，首先对于某个卷积核来说，每个滑动窗口位置之间没有依赖关系，所以完全可以并行计算；另外，不同的卷积核之间也没什么相互影响，所以也可以并行计算。

### Transformer 登场
[[Transformer]]
![[Pasted image 20240302135631.png]]
Transformer 模型由编码器（Encoder）和解码器（Decoder）两部分组成，这两部分都采用了多层的自注意力（Self-Attention）和前馈神经网络（Feed-Forward Neural Network）。

**编码器（Encoder）**：编码器的主要任务是理解输入的信息，并将其转化为一种内部表示形式。在 Transformer 中，编码器接收一系列输入（比如一个句子中的每个词），并通过自注意力机制和前馈神经网络，将每个输入转化为一个向量。这个向量包含了输入的信息，以及它与其他输入的关系。编码器由多个这样的层堆叠在一起，每一层都会进一步提炼这些向量。

**解码器（Decoder）**：解码器的主要任务是根据编码器的输出生成最终的输出。在 Transformer 中，解码器也是由多个自注意力机制和前馈神经网络的层组成。但解码器有两个自注意力层，一个是对自身的输入进行自注意力计算，另一个是对编码器的输出进行自注意力计算。这使得解码器在生成每个输出时，都能考虑到所有的输入和已经生成的输出。

**区别和联系**：编码器和解码器的主要区别在于，编码器只需要理解输入，而解码器需要理解输入并生成输出。因此，解码器比编码器多了一个自注意力层，用于理解已经生成的输出。编码器和解码器的联系在于，它们都使用了自注意力机制和前馈神经网络，而且解码器在生成输出时，会使用编码器的输出。

在 GPT 中，只使用了 Transformer 的解码器部分，因为 GPT 的任务是生成文本，不需要理解输入。而在 BERT 中，只使用了 Transformer 的编码器部分，因为 BERT 的任务是理解文本，不需要生成输出。

> 目前 Transformer 不仅统一了 NLP 诸多领域，也逐步替换图像处理各种任务被广泛使用的 CNN 等其他模型的进程之中；
> 类似的，多模态模型也目前 基本都采用了 Transformer 模型

#### Transformer 问题
因为输入的第一层网络是Muli-head self attention层，我们知道，Self attention会让当前输入单词和句子中任意单词发生关系，然后集成到一个embedding向量里，但是当所有信息到了embedding后，位置信息并没有被编码进去。
#### Transformer 如何解决问题
1. 如何解决不定长问题：
	类似 CNN 假定输入的最大长度，不够用 padding 补充，
2. 如何解决位置编码问题：
	必须要有一个位置编码。
	1. Transformer 用位置函数来进行编码；
	2. Bert 模型则给每一个单词一个 Position embedding，和 单词 embedding 加起来形成单词的输入；
3. 如何解决长距离依赖问题。
	elf attention天然就能解决这个问题，因为在集成信息的时候，当前单词和句子中任意单词都发生了联系，所以一步到位就把这个事情做掉了。不像RNN需要通过隐层节点序列往后传，也不像CNN需要通过增加网络深度来捕获远距离特征，Transformer在这点上明显方案是相对简单直观的。

### 三者比较

1. 从语义特征提取能力来说，目前实验支持如下结论：Transformer在这方面的能力非常显著地超过RNN和CNN（在考察语义类能力的任务WSD中，Transformer超过RNN和CNN大约4-8个绝对百分点），RNN和CNN两者能力差不太多。
2. 在长距离特征捕获能力方面，目前在特定的长距离特征捕获能力测试任务中（主语-谓语一致性检测，比如we……..are…），实验支持如下结论：原生CNN特征抽取器在这方面极为显著地弱于RNN和Transformer，Transformer微弱优于RNN模型(尤其在主语谓语距离小于13时)，能力由强到弱排序为Transformer>RNN>>CNN; 但在比较远的距离上（主语谓语距离大于13），RNN微弱优于Transformer，所以综合看，可以认为Transformer和RNN在这方面能力差不太多，而CNN则显著弱于前两者。
3. 从综合特征抽取能力角度衡量，Transformer显著强于RNN和CNN，而RNN和CNN的表现差不太
4. RNN在并行计算方面有严重缺陷，这是它本身的序列依赖特性导致的，所谓成也萧何败也萧何，它的这个线形序列依赖性非常符合解决NLP任务，这也是为何RNN一引入到NLP就很快流行起来的原因，但是也正是这个线形序列依赖特性，导致它在并行计算方面要想获得质的飞跃，看起来困难重重，近乎是不太可能完成的任务。而对于CNN和Transformer来说，因为它们不存在网络中间状态不同时间步输入的依赖关系，所以可以非常方便及自由地做并行计算改造，这个也好理解。并行计算能力由高到低排序如下：Transformer和CNN差不多，都远远远远强于RNN。
# 待完成！！！！https://zhuanlan.zhihu.com/p/54743941
https://zhuanlan.zhihu.com/p/597586623

## Bert|GPT|Transformer 区分
BERT（Bidirectional Encoder Representations from Transformers）、GPT（Generative Pretrained Transformer）和Transformer 是三种在自然语言处理（NLP）领域广泛使用的模型或模型架构。它们之间的关系可以从以下几个方面来理解：

1. **Transformer**：Transformer 是一种模型架构，它在 "Attention is All You Need" 这篇论文中首次被提出。Transformer 模型的主要特点是它完全放弃了传统的 RNN（循环神经网络）或 CNN（卷积神经网络）结构，而是完全依赖于 self-attention 机制来处理序列数据。这种结构使得 Transformer 模型在处理长距离依赖和并行计算方面具有优势。

2. **GPT**：GPT 是 OpenAI 开发的一种基于 Transformer 的模型。GPT 使用了 Transformer 的解码器部分，并且采用了单向（从左到右）的自注意力机制。这使得 GPT 在生成文本（如写作、翻译等任务）方面表现出色。

3. **BERT**：BERT 是 Google 开发的一种基于 Transformer 的模型。与 GPT 不同，BERT 使用了 Transformer 的编码器部分，并且采用了双向的自注意力机制。这使得 BERT 能够理解文本中的上下文信息，因此在理解、分类、问答等任务中表现优秀。

总的来说，Transformer 是一种模型架构，而 GPT 和 BERT 都是基于这种架构的模型，但它们在具体实现和应用上有所不同。

# 时间线前
## 范式转换1.0
1. bert 和 gpt 模型出现以前，NLP 领域流行的技术是深度学习模型
2. NLP 领域的深度学习
	1. 大量改进的 LSTM 模型 | 少量的改进 CNN 模型作为特征抽取器
	2. 以 sequence to sequence（encode-decoder）+ Attention 作为各种具体任务典型。
	3. 目标：如何有效增加模型深层或模型的参数容量。
		即：怎么才能往encoder 和decoder 里不断叠加更深的 LSTM 或 CNN 层，来达成增加层数和模型容量的目标。
	4. 分类：
		1. 中间任务：中文分词，词性标注，NER，句法分析，指代消解，语义 Parser 等，
		2. 最终任务：文本分类，文本相似性计算，机器翻译，文本摘要。
			1. 自然语言理解类任务：本质上是分类任务
			2. 自然语言生成类任务
	5. 评价：总体而言不是很成功，或者说和非深度学习方法相比，带来的优势i不算很大。
	6. 
1. NLP 深度学习不算成功的原因：
	1. 训练数据总量的限制。
	2. LSTM | CNN 特征提取器表达能力不强。
2. Ber|GPT 这两个预训练模型的出现，代表 NLP 领域的飞跃
	1. NLP 研究子领域日渐消亡
		1. 中间任务不应该出现，这是 NLP 技术发展水平不够高的一种体现。很难一步做好有难度的最终任务。
		2. Bert|GPT 出现后没有必要做这些中间任务了，因为 Bert|GPT 已经把这些中间任务作为语言学特征，吸收到了 Transformer 的参数里，
	2. NLP 不同子领域技术方法和技术日渐统一。
		1. NLP 领域特征提取器都逐渐从 LSTM|CNN 统一到 Transformer 上。
		2. 大多数 NLP 子领域的研发模式切换到了两阶段模式，：
			1. 模型预训练阶段
			2. 应用微调
			或者：
			1. Zero Shot
			2. Few Shot