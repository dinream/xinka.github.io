<!DOCTYPE html>
<html lang="en">
    <head>
  <!-- 元数据 -->
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  
  <meta name="author" content="John Doe" />
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="robots" content="index,follow" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <meta name="format-detection" content="telphone=no, email=no" />
  
    <meta name="keywords" content="" />
  
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/7/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  <!-- 站点验证相关 -->
  
    
    
    
  
  <!-- 样式表文件 -->
  <link rel="stylesheet" id="kratos-css" href="/css/kratosr.min.css" media="all"></script>
  
    <link rel="stylesheet" id="darkmode-css" href="/css/kr-color-dark.min.css" media="(prefers-color-scheme: dark)"></script>
    <script src="/js/kr-dark.min.js"></script>
  
  
    <link rel="stylesheet" id="highlight-css" href="/css/highlight/night-eighties.min.css" media="all"></script>
  
  <link rel="stylesheet" id="fontawe-css" href="/vendors/font-awesome@4.7.0/css/font-awesome.min.css" media="all"></script>
  <link rel="stylesheet" id="nprogress-css" href="/vendors/nprogress@0.2.0/nprogress.css" media="all"></script>
  
  
    <link rel="stylesheet" href="/vendors/aplayer@1.10.1/dist/APlayer.min.css"></script>
  
  
    <link rel="stylesheet" href="/vendors/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"></script>
  
  <!-- 不得不预先加载的一些JS文件 -->
  <script src="/vendors/jquery@3.6.0/dist/jquery.min.js"></script>
  
    <script src="/vendors/qrcode_js@1.0.0/qrcode.min.js"></script>
  
  
  <style>
    
    
  </style>
  
<meta name="generator" content="Hexo 7.2.0"></head>


    <body class="custom-background">
        <div id="kratos-wrapper">
    <div id="kratos-page">
        <div id="kratos-header">
            <header id="kratos-desktop-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="nav-header">
                        <nav id="kratos-menu-wrap">
                            <ul id="kratos-primary-menu" class="sf-menu">
                                
                                    
                                    
                                
                            </ul>
                        </nav>
                    </div>
                </div>
            </header>
            <header id="kratos-mobile-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="color-logo"><a href="/">Hexo</a></div>
                    <div class="nav-toggle">
                        <a class="kratos-nav-toggle js-kratos-nav-toggle">
                            <i></i>
                        </a>
                    </div>
                </div>
            </header>
        </div>
        <div class="kratos-start kratos-hero-2">
            <!-- <div class="kratos-overlay"></div> -->
            <div class="kratos-cover kratos-cover-2 text-center">
                <div class="desc desc2 animate-box">
                    <a href="/">
                        <h2>Hexo</h2> <br />
                        <span></span>
                    </a>
                </div>
            </div>
        </div>

        <div id="kratos-blog-post">
            <div class="container">
                <div id="main" class="row">
                    

        

            <section class="col-md-8">

        

            <!-- Breadcrumb for tag & category page -->




    
    
        <article class="kratos-hentry kratos-entry-border-new clearfix" itemscope itemtype="https://schema.org/Article">
            <div class="kratos-status">
                
                    <i class="fa fa-refresh"></i>
                
                <div class="kratos-status-inner">
                    <div class="kratos-status-content" itemprop="articleBody">
                        
                            
                        
                    </div>
                </div>
            </div>
            
                <footer class="kratos-post-meta-new">
                    <span class="pull-left">
                        <time datetime="2024-04-22T13:14:48.920Z" itemprop="datePublished">
                            <a><i class="fa fa-calendar"></i> 2024-04-22</a>
                        </time>
                        
                        
                            <!-- 当前仅支持valine/waline自带的统计功能 -->
                            
                        
                        
                            <!-- 当前仅支持waline自带的统计功能 -->
                            
                        
                    </span>
                    
                    
                        <span class="pull-right">
                            <a class="read-more" href="/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%9C%AA%E5%91%BD%E5%90%8D/" title="留言">留言<i class="fa fa-chevron-circle-right"></i></a>
                        </span>
                    

                </footer>
            
        </article>
    

    
    
        <article class="kratos-hentry kratos-entry-border-new clearfix" itemscope itemtype="https://schema.org/Article">
            <div class="kratos-status">
                
                    <i class="fa fa-refresh"></i>
                
                <div class="kratos-status-inner">
                    <div class="kratos-status-content" itemprop="articleBody">
                        
                            <p>在本文中，我们对学习密码中的语言模式进行了细致入微的研究，并使用最新的评估方法更新了语义 PCFG 的性能，以提供更长的猜测会话的结果 [7]。,我们通过部署在高性能计算基础设施上的大参数扫描实验来实现这一点。,在以下部分中，我们报告了比较 PCFG 性能的实验结果<br>    (a) 使用和不使用语义符号（WordNet 含义和专有名称）进行训练；<br>    (b) 经过不同级别的语义概括训练；<br>    (c) 使用不同大小的密码列表进行训练；<br>    (d) 使用最大似然估计（一种将零概率分配给未见字符串的方法）和概率平滑（将概率质量分配给未见字符串）进行训练 [6]。<br>重要的是，我们将三代 PCFG（Weir 等人[23]、Veras 等人[21] 和 Komanduri [12]）与最新的神经网络模型之一（Melicher 等人[16]）进行了比较。</p>
<p>我们的实验设置包括使用 RockYou 列表训练的语法，以及对从 LinkedIn 和 000webhost 泄露的密码进行测试的语法，以及使用相同数据的独立交叉验证设置。,此外，我们使用语言语法来定性研究最近三个密码泄露的模式：000webhost、Comcast 和 Mate1。,我们提出了这些泄漏的高级图形模型，讨论了语法规则的相似性，并进行了交叉泄漏破解实验。<br>总之，本文研究了多个参数对 PCFG 和神经网络模型猜测性能的影响，并对最近的密码泄漏进行了基于语法的定性分析。</p>
<h1 id="语义-PCFG"><a href="#语义-PCFG" class="headerlink" title="语义 PCFG"></a>语义 PCFG</h1><h2 id="文本处理流"><a href="#文本处理流" class="headerlink" title="文本处理流"></a>文本处理流</h2><p>将每个密码分解为令牌（即终端符号）并为其分配语言属性，从而产生形式的元组（令牌、POS、意义）。</p>
<ol>
<li>分词： Norvig 的统计算法 [18]，该算法选择具有最高联合概率的分词，其中每个标记的概率使用二元模型计算（来自 Google Web Trillion）词语料库）。</li>
<li>词性标注是通过退避标注器完成的，该标注器由在布朗语料库和 WordNet 以及其他已知与密码相关的利基命名实体语料库（城市，给定）上训练的一般统计模型（三元组、二元组和一元组）组成。名和姓氏）。请注意，拼写错误（例如，passwrd）和替换（例如，passw0rd）不按词性语法和语义语法进行分类。将来可以添加将拼写错误的单词分类为语义类别的模块</li>
<li>语义，见下</li>
</ol>
<h2 id="语义模型"><a href="#语义模型" class="headerlink" title="语义模型"></a>语义模型</h2><p>语义模型主要关注单词的意义和语义关系，它利用WordNet等语义资源对单词进行语义标记，以便捕捉密码中的语义规律。<br>能够提供更详细的密码结构分析，有助于更好地理解密码样本中的模式和提供更可解释的密码强度建议。<br>语义模型在小样本训练和需要解释性较强的情况下显示出更好的猜测性能。</p>
<h3 id="语义"><a href="#语义" class="headerlink" title="语义"></a>语义</h3><p>该词的意义取自 WordNet 语料库 [17]，这是一种语言树结构，其中概念通过代表 IS-A 关系的边连接起来，就像狗 IS-A 动物 IS-A 哺乳动物 IS-A 生物一样。每个被词性标注器标注为名词或动词的字母字符串都会收到一个 WordNet 意义键形式的语义标签。单词可能有多种含义，因此我们选择 WordNet 频率最高的含义。<br>为了使词义具有泛化能力，需要建立映射，以便将低级词义分组为更广泛的类别。这可以通过选择由一组抽象类（内部树节点）组成的树切割来完成，每个抽象类代表其所有后代。因此，如果“运动”类是树木砍伐成员，则任何出现“棒球”的情况。都将在训练的语法中被标记为“sport”。这种映射具有扩大语法推理范围的效果，因为通过抽象类间接提高了未见过单词的概率。<br>在实践中，包含数据中频率相似的词义的 WordNet 子树往往会比包含异常值的子树映射到更抽象的含义。请注意，高频词更有可能映射到特定类别，通常意味着根本没有抽象（例如，love → love.n.01）。</p>
<h2 id="词性模型"><a href="#词性模型" class="headerlink" title="词性模型"></a>词性模型</h2><p>关注单词的语法属性，比如名词、动词、形容词等，它利用词性标记器对单词进行词性标记，以便捕捉密码中的语法规律。<br>词性模型则更适合在短时间内进行密码猜测，因为它能够更灵活地捕捉密码中的语法规律，尤其在初始猜测阶段表现更好。</p>
<h2 id="终端平滑"><a href="#终端平滑" class="headerlink" title="终端平滑"></a>终端平滑</h2><p> 在 Weir 等人的 PCFG 和语义 PCF G 中，终端概率是通过最大似然（ML）来估计的。<br>在机器学习中，选择模型的参数是为了最大化给定模型的数据的概率；因此，不会将概率质量分配给看不见的字符串。从理论上讲，这会影响小样本的学习，<br>语义泛化的有效性取决于终端概率平滑。终端平滑需要两个决定：将哪些词汇添加到语法中以及如何为其分配概率。</p>
<ol>
<li>Wordnet 涵盖了大量单词。我们称之为先验词汇，由以各种方式变形的每个 Wordnet 引理组成：名词以单数和复数形式出现，动词以所有词形变化形式出现。根据可用资源，可以通过分配给语义类别的附加单词列表来丰富先验词汇表（类似于我们处理名称的方式）。</li>
<li>除了先验终端之外，后验词汇表还包括数据中观察到的每个终端。给定非终结符号的终结串为 θˆ i &#x3D; xi + α N + αd ，<ol>
<li> (1) 其中 xi 是观测频率；</li>
<li>N 是非终结符号下观测频率之和；</li>
<li>α 称为伪计数，可以被解释为假定先验观察字符串的次数（当 α &#x3D; 0 时，默认为 ML）；</li>
<li>d 是给定非终结符的词汇大小。<br> 该估计器 ( ˆθi ) 称为加性平滑或拉普拉斯平滑，当假设先验一致时，它相当于贝叶斯估计量 [15]。当词汇量非常大时，加法平滑可能不准确，就像三元组一样，在这种情况下，更复杂的估计量更合适，比如 Good -Komanduri 使用图灵估计器来训练 PCFG [12]。由于我们的词汇量约为数十万，拉普拉斯估计器就足够了，并且实现更简单。</li>
</ol>
</li>
</ol>
<h5 id="蒙特卡罗强度估计器"><a href="#蒙特卡罗强度估计器" class="headerlink" title="蒙特卡罗强度估计器"></a>蒙特卡罗强度估计器</h5><p>是一种用于测量密码破解成功率的方法。它通过计算测试集中每个语法的密码的概率，然后估计猜测次数。具体来说，猜测次数指的是在尝试所有语法中概率最高的密码之前需要尝试的次数。蒙特卡罗估计器允许我们在无限长度的会话中测量成功率，而无需进行大量的猜测枚举，这在计算上是非常昂贵的。因此，蒙特卡罗强度估计器是一种用于评估密码破解成功率的有效方法。</p>
<h2 id="基于语义信息，各个数据集之间的关系？"><a href="#基于语义信息，各个数据集之间的关系？" class="headerlink" title="基于语义信息，各个数据集之间的关系？"></a>基于语义信息，各个数据集之间的关系？</h2><p>基于语义信息，RockYou、Comcast、Mate1和000webhost泄露的密码列表之间存在一些相似之处。研究发现，RockYou和Mate1泄露的密码具有很大的语义和结构重叠，表明这两个泄露数据具有相似的语义模式。而在000webhost密码列表中，密码具有明显的统一结构，但在语义上却缺乏一致性，至少在英语语言语法中如此。此外，Comcast密码列表中出现了许多包含类别”worker”和”hacker”的模式，还有包含类别”defender”和”windows”的模式，以及服务名称本身的模式。这些发现揭示了密码泄露的语义偏好与服务的人口统计和主题之间的关联，以及密码政策对所选密码结构的影响。</p>
<p>（a）LinkedIn 密码缺乏强英语语义依赖性； (b) LinkedIn 具有较强的英语语义依赖性，但与 RockYou 中的不同； (c) 英文密码一般缺乏强语义依赖性。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这篇文章主要研究了语义密码模型和密码中的语言模式，通过对不同语言模型的大规模分析，比如PCFG和神经网络模型，以及对RockYou，LinkedIn，Mate1，Comcast和000webhost等密码泄露数据进行定性分析和比较。研究发现，PCFG在猜测非随机密码方面表现出色，尤其是在小样本训练集上。此外，语义模型在特定情况下表现出更好的猜测性能。在对不同密码列表的语言模式进行比较时，研究发现了一些有趣的区别，比如Mate1和RockYou泄露数据中共享的语义模式。总的来说，这项研究提供了关于密码模型和语言模式的深入见解，特别是在不同类型的密码列表中的比较分析。</p>
<ol>
<li>重点是语义密码模型的参数，这是一个用词性和语义信息训练的 PCFG。</li>
<li>未经概率平滑训练的语法往往会过度拟合，平滑可以为用小样本训练的 PCFG 提供强大的能力：仅用 1,000 个 RockYou 密码训练的语法就能够猜测近 500 万个 000webhost 密码，训练样本大小增加到超过 1000 万个密码时，收益递减。</li>
<li>分离了有语义和没有语义的 POS 的影响，发现向语法中添加语义信息的好处与添加 POS 信息带来的收益相比很小，并且取决于训练数据的大小。（维拉斯等人。证明具有语义和词性信息的语法在很大程度上优于 Weir 等人的语法。）</li>
<li>发现 Komanduri PCFG 比其他测试的 PCFG 具有更好的猜测能力，并且存在 Melicher 等人的神经模型的情况。比我们测试的 PCFG 的猜测能力更差。</li>
<li>利用语义模型的解释能力来定性检查最近的密码泄露事件<ol>
<li>RockYou 和 Mate1 泄露事件在语义和结构上有很大的重叠，</li>
<li>而 000webhost 密码具有非常统一的结构，但语义统一性很小，至少从英语语法来看是这样。</li>
</ol>
</li>
</ol>

                        
                    </div>
                </div>
            </div>
            
                <footer class="kratos-post-meta-new">
                    <span class="pull-left">
                        <time datetime="2024-04-22T13:14:48.920Z" itemprop="datePublished">
                            <a><i class="fa fa-calendar"></i> 2024-04-22</a>
                        </time>
                        
                        
                            <!-- 当前仅支持valine/waline自带的统计功能 -->
                            
                        
                        
                            <!-- 当前仅支持waline自带的统计功能 -->
                            
                        
                    </span>
                    
                    
                        <span class="pull-right">
                            <a class="read-more" href="/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E8%AF%AD%E4%B9%89%E5%AF%86%E7%A0%81%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%AF%86%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%BC%8F%E5%88%86%E6%9E%90/" title="留言">留言<i class="fa fa-chevron-circle-right"></i></a>
                        </span>
                    

                </footer>
            
        </article>
    

    
    
        <article class="kratos-hentry kratos-entry-border-new clearfix" itemscope itemtype="https://schema.org/Article">
            <div class="kratos-status">
                
                    <i class="fa fa-refresh"></i>
                
                <div class="kratos-status-inner">
                    <div class="kratos-status-content" itemprop="articleBody">
                        
                            
                        
                    </div>
                </div>
            </div>
            
                <footer class="kratos-post-meta-new">
                    <span class="pull-left">
                        <time datetime="2024-04-22T13:14:48.905Z" itemprop="datePublished">
                            <a><i class="fa fa-calendar"></i> 2024-04-22</a>
                        </time>
                        
                        
                            <!-- 当前仅支持valine/waline自带的统计功能 -->
                            
                        
                        
                            <!-- 当前仅支持waline自带的统计功能 -->
                            
                        
                    </span>
                    
                    
                        <span class="pull-right">
                            <a class="read-more" href="/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8F%A3%E4%BB%A4%E5%AE%89%E5%85%A8%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6/" title="留言">留言<i class="fa fa-chevron-circle-right"></i></a>
                        </span>
                    

                </footer>
            
        </article>
    

    
    
        <article class="kratos-hentry kratos-entry-border-new clearfix" itemscope itemtype="https://schema.org/Article">
            <div class="kratos-status">
                
                    <i class="fa fa-refresh"></i>
                
                <div class="kratos-status-inner">
                    <div class="kratos-status-content" itemprop="articleBody">
                        
                            <h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>口令常用，口令脆弱</p>
<p>口令破解主要关注点在于利用尽可能小的生成子点集合覆盖尽可能多的真实口令</p>
<ol>
<li>Markov</li>
<li>PCFG</li>
<li>NN<br>反向传播算法是神经网络的核心算法。该算法的思想是，通过对预估结果的评价的反馈判断模型参数中导致预测结果变坏的量，对其加以纠正，使得模型逐渐趋向最优。</li>
</ol>

                        
                    </div>
                </div>
            </div>
            
                <footer class="kratos-post-meta-new">
                    <span class="pull-left">
                        <time datetime="2024-04-22T13:14:48.905Z" itemprop="datePublished">
                            <a><i class="fa fa-calendar"></i> 2024-04-22</a>
                        </time>
                        
                        
                            <!-- 当前仅支持valine/waline自带的统计功能 -->
                            
                        
                        
                            <!-- 当前仅支持waline自带的统计功能 -->
                            
                        
                    </span>
                    
                    
                        <span class="pull-right">
                            <a class="read-more" href="/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E6%98%8E%E6%96%87%E5%8F%A3%E4%BB%A4%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/" title="留言">留言<i class="fa fa-chevron-circle-right"></i></a>
                        </span>
                    

                </footer>
            
        </article>
    

    
    
        <article class="kratos-hentry kratos-entry-border-new clearfix" itemscope itemtype="https://schema.org/Article">
            <div class="kratos-status">
                
                    <i class="fa fa-refresh"></i>
                
                <div class="kratos-status-inner">
                    <div class="kratos-status-content" itemprop="articleBody">
                        
                            <h1 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h1><p>数据集：<br><a target="_blank" rel="noopener" href="https://wiki.skullsecurity.org/index.php/Passwords">RockYou password leak</a></p>
<p>使用论文预训练模型：</p>
<ol>
<li>最多 10 个字符密码训练的 PassGPT <a target="_blank" rel="noopener" href="https://huggingface.co/javirandor/passgpt-10characters/">此处</a>。 </li>
<li>最多 16 个字符的密码训练的版本需要我们团队的研究批准，可以在<a target="_blank" rel="noopener" href="https://huggingface.co/javirandor/passgpt-16characters/">此处</a>找到。</li>
</ol>
<p>自己训练模型：</p>
<ol>
<li>密码 tokenizer ：字符级（防止像 NLP 那样将 字符 串为分词），在模型下保留有意义的概率分布。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">python src/create_tokenizer.py --train_path &#123;PATH_TO_TRAINING_DATA&#125; --output_path &#123;PATH_TO_TOKENIZERS_FOLDER&#125;</span><br><span class="line"><span class="comment"># Customization</span></span><br><span class="line">python src/create_tokenizer.py --train_path data/rockyou.txt --output_path output</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>自定义一个配置文件： <code>CONFIG_PATH</code> ，可以使用默认的文件 <a target="_blank" rel="noopener" href="https://github.com/javirandor/passbert/blob/main/configs/passgpt-16chars.yaml">yaml 文件</a>.</li>
<li>训练模型：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python src/train_passgpt.py --config_path &#123;CONFIG_PATH&#125;</span><br><span class="line"><span class="comment"># Customization</span></span><br><span class="line">python src/train_passgpt.py --config_path configs/passgpt-16chars.yaml</span><br></pre></td></tr></table></figure></li>
<li>模型使用<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python src/generate_passwords.py --model_path &#123;MODEL_PATH&#125; --out_path &#123;PASSWORD_OUTPUT_FOLDER&#125; --num_generate &#123;NUM_PASSWORDS&#125; --train_data_path &#123;PATH_TO_TRAINING_DATA&#125; --eval_data_path &#123;PATH_TO_EVAL_DATA&#125;</span><br><span class="line"><span class="comment"># Customizeation</span></span><br><span class="line"> python src/generate_passwords.py --model_path train_output/last/ --out_path ./train_output/last/ --num_generate <span class="number">100000000</span> --train_data_path data/rockyou.txt --eval_data_path data/rockyou.txt --tokenizer_path output/byte_bpe_tokenizer_99/</span><br></pre></td></tr></table></figure>
 进一步的生成参数，例如“–temper”、“–top_p”或“–top_k”</li>
</ol>
<h1 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h1><p>conda + python3.11<br>requirements.txt</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">accelerate==0.28.0</span><br><span class="line">aiohttp==3.9.3</span><br><span class="line">aiosignal==1.3.1</span><br><span class="line">appdirs==1.4.4</span><br><span class="line">asttokens==2.4.1</span><br><span class="line">attrs==23.2.0</span><br><span class="line">azure-core==1.30.1</span><br><span class="line">azure-storage-blob==12.19.1</span><br><span class="line">certifi==2024.2.2</span><br><span class="line">cffi==1.16.0</span><br><span class="line">charset-normalizer==3.3.2</span><br><span class="line">click==8.1.7</span><br><span class="line">comm==0.2.1</span><br><span class="line">cryptography==42.0.5</span><br><span class="line">datasets==2.18.0</span><br><span class="line">debugpy==1.6.7</span><br><span class="line">decorator==5.1.1</span><br><span class="line">dill==0.3.8</span><br><span class="line">docker-pycreds==0.4.0</span><br><span class="line">exceptiongroup==1.2.0</span><br><span class="line">executing==2.0.1</span><br><span class="line">filelock==3.13.1</span><br><span class="line">frozenlist==1.4.1</span><br><span class="line">fsspec==2024.2.0</span><br><span class="line">gitdb==4.0.11</span><br><span class="line">GitPython==3.1.42</span><br><span class="line">huggingface-hub==0.21.4</span><br><span class="line">idna==3.6</span><br><span class="line">importlib_metadata==7.0.2</span><br><span class="line">ipykernel==6.29.3</span><br><span class="line">ipython==8.22.2</span><br><span class="line">isodate==0.6.1</span><br><span class="line">jedi==0.19.1</span><br><span class="line">Jinja2==3.1.3</span><br><span class="line">joblib==1.3.2</span><br><span class="line">jsonlines==4.0.0</span><br><span class="line">jupyter_client==8.6.0</span><br><span class="line">jupyter_core==5.7.1</span><br><span class="line">lamini==2.1.2</span><br><span class="line">lamini-configuration==0.8.3</span><br><span class="line">MarkupSafe==2.1.5</span><br><span class="line">matplotlib-inline==0.1.6</span><br><span class="line">mpmath==1.3.0</span><br><span class="line">multidict==6.0.5</span><br><span class="line">multiprocess==0.70.16</span><br><span class="line">nest_asyncio==1.6.0</span><br><span class="line">networkx==3.2.1</span><br><span class="line">numpy==1.26.4</span><br><span class="line">packaging==24.0</span><br><span class="line">pandas==2.2.1</span><br><span class="line">parso==0.8.3</span><br><span class="line">pexpect==4.9.0</span><br><span class="line">pickleshare==0.7.5</span><br><span class="line">pip==23.3.1</span><br><span class="line">platformdirs==4.2.0</span><br><span class="line">prompt-toolkit==3.0.42</span><br><span class="line">protobuf==4.25.3</span><br><span class="line">psutil==5.9.0</span><br><span class="line">ptyprocess==0.7.0</span><br><span class="line">pure-eval==0.2.2</span><br><span class="line">pyarrow==15.0.1</span><br><span class="line">pyarrow-hotfix==0.6</span><br><span class="line">pycparser==2.21</span><br><span class="line">Pygments==2.17.2</span><br><span class="line">python-dateutil==2.9.0</span><br><span class="line">pytz==2024.1</span><br><span class="line">PyYAML==6.0.1</span><br><span class="line">pyzmq==25.1.2</span><br><span class="line">regex==2023.12.25</span><br><span class="line">requests==2.31.0</span><br><span class="line">safetensors==0.4.2</span><br><span class="line">scikit-learn==1.4.1.post1</span><br><span class="line">scipy==1.12.0</span><br><span class="line">sentry-sdk==1.43.0</span><br><span class="line">setproctitle==1.3.3</span><br><span class="line">setuptools==68.2.2</span><br><span class="line">six==1.16.0</span><br><span class="line">smmap==5.0.1</span><br><span class="line">stack-data==0.6.2</span><br><span class="line">sympy==1.12</span><br><span class="line">threadpoolctl==3.3.0</span><br><span class="line">tokenizers==0.15.2</span><br><span class="line">torch==2.2.1</span><br><span class="line">tornado==6.3.3</span><br><span class="line">tqdm==4.66.2</span><br><span class="line">traitlets==5.14.1</span><br><span class="line">transformers==4.38.2</span><br><span class="line">triton==2.2.0</span><br><span class="line">typing_extensions==4.10.0</span><br><span class="line">tzdata==2024.1</span><br><span class="line">urllib3==2.2.1</span><br><span class="line">wandb==0.16.4</span><br><span class="line">wcwidth==0.2.13</span><br><span class="line">wheel==0.41.2</span><br><span class="line">xxhash==3.4.1</span><br><span class="line">yarl==1.9.4</span><br><span class="line">zipp==3.17.0</span><br></pre></td></tr></table></figure>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h2 id="问题：-‘utf-8’-解析出错"><a href="#问题：-‘utf-8’-解析出错" class="headerlink" title="问题： ‘utf-8’ 解析出错"></a>问题： ‘utf-8’ 解析出错</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(args.train_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># with open(args.train_path, &quot;r&quot;, encoding=&quot;utf-8&quot;, errors=&#x27;replace&#x27;) as f:</span></span><br><span class="line"></span><br><span class="line">        lines = f.read().splitlines()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>UnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xf1 in position 5079963: invalid continuation byte</p>
</blockquote>
<p>解决1：使用默认字符替换，默认是 “?”。<br><code>with open(args.train_path, &quot;r&quot;, encoding=&quot;utf-8&quot;, errors=&#39;replace&#39;) as f</code></p>
<p>解决2：定位到没有解析的字符行，重新输入</p>
<h2 id="问题：-包缺失"><a href="#问题：-包缺失" class="headerlink" title="问题： 包缺失"></a>问题： 包缺失</h2><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  File &quot;/opt/conda/envs/py3_11/lib/python3.11/site-packages/transformers/training_args.py&quot;, line 1905, in _setup_devices</span><br><span class="line">    raise ImportError(</span><br><span class="line">ImportError: Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/root/PSG/passgpt-main/src/train_passgpt.py&quot;, line 120, in &lt;module&gt;</span><br><span class="line">    trainer = Trainer(</span><br><span class="line">              ^^^^^^^^</span><br><span class="line">  File &quot;/opt/conda/envs/py3_11/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 533, in __init__</span><br><span class="line">    self.callback_handler = CallbackHandler(</span><br><span class="line">                            ^^^^^^^^^^^^^^^^</span><br><span class="line">  File &quot;/opt/conda/envs/py3_11/lib/python3.11/site-packages/transformers/trainer_callback.py&quot;, line 313, in __init__</span><br><span class="line">    self.add_callback(cb)</span><br><span class="line">  File &quot;/opt/conda/envs/py3_11/lib/python3.11/site-packages/transformers/trainer_callback.py&quot;, line 330, in add_callback</span><br><span class="line">    cb = callback() if isinstance(callback, type) else callback</span><br><span class="line">         ^^^^^^^^^^</span><br><span class="line">  File &quot;/opt/conda/envs/py3_11/lib/python3.11/site-packages/transformers/integrations/integration_utils.py&quot;, line 673, in __init__</span><br><span class="line">    raise RuntimeError(&quot;WandbCallback requires wandb to be installed. Run `pip install wandb`.&quot;)</span><br></pre></td></tr></table></figure>

<h2 id="问题：显存不够"><a href="#问题：显存不够" class="headerlink" title="问题：显存不够"></a>问题：显存不够</h2><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 44.62 MiB is free. Process 1338858 has 10.69 GiB memory in use.</span><br></pre></td></tr></table></figure>
<ol>
<li><p>换服务器，当前服务器显卡资源不足：<br>![[Pasted image 20240322093550.png]]</p>
</li>
<li><p>新服务器<br>![[Pasted image 20240322093300.png]]</p>
</li>
</ol>
<h2 id="问题：-显示-Driver-Version-过低"><a href="#问题：-显示-Driver-Version-过低" class="headerlink" title="问题： 显示 Driver Version 过低"></a>问题： 显示 Driver Version 过低</h2><p>（导致训练时不能使用 GPU）<br>    此时不一定是 Driver Version 版本的问题，在实际中只要 显卡 显卡驱动兼容，那么就可以使用显卡跑程序。<br>    通过 降低 pytorch 的版本到 2.0.0 ，成功运行。</p>
<p>![[Pasted image 20240322011742.png]]</p>
<p>![[Pasted image 20240322091910.png]]</p>
<h1 id="当前的进度"><a href="#当前的进度" class="headerlink" title="当前的进度"></a>当前的进度</h1><p>目前的进度： PassGPT 训练过程结束（最大字符 长度 设置为 10）</p>
<p>![[Pasted image 20240322102651.png]]</p>
<p>当前实验存在的问题：<br>运行模型 需要 的配置较高。<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/673125345">模型显存需求估计</a></p>
<h1 id="下一步计划"><a href="#下一步计划" class="headerlink" title="下一步计划"></a>下一步计划</h1><p>之前是跑了模型的训练部分，下一步就是使用模型生成密码，并且分析数据。</p>
<p>计划的实验目标是：<br>    1. 论文中没有提到只是说生成一定的数量的密码之后对测试数据集的覆盖率达到了多少，但是并没有提到那些 类型数据 是模型难以拟合的，我主要想看一下哪些没有匹配到的密码长什么样子。<br>    2. 对比以下 开源的预训练 10 字符 模型 和 此次训练的模型的效果，验证一下论文的效果。</p>
<p>目前的一个小 idea，参考 [[MoEs]] ，但了解还比较粗浅。</p>
<p>最后感觉论文读的还是太少了，需要接着调研论文 </p>
<p>模型运行结果：<a target="_blank" rel="noopener" href="https://wandb.ai/xinka/huggingface/runs/f4ta95w4?nw=nwuserfengdreamin">https://wandb.ai/xinka/huggingface/runs/f4ta95w4?nw=nwuserfengdreamin</a></p>

                        
                    </div>
                </div>
            </div>
            
                <footer class="kratos-post-meta-new">
                    <span class="pull-left">
                        <time datetime="2024-04-22T13:14:48.905Z" itemprop="datePublished">
                            <a><i class="fa fa-calendar"></i> 2024-04-22</a>
                        </time>
                        
                        
                            <!-- 当前仅支持valine/waline自带的统计功能 -->
                            
                        
                        
                            <!-- 当前仅支持waline自带的统计功能 -->
                            
                        
                    </span>
                    
                    
                        <span class="pull-right">
                            <a class="read-more" href="/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%A4%8D%E7%8E%B0/%E5%A4%8D%E7%8E%B0%EF%BC%9APassGPT/" title="留言">留言<i class="fa fa-chevron-circle-right"></i></a>
                        </span>
                    

                </footer>
            
        </article>
    

    
    
        <article class="kratos-hentry kratos-entry-border-new clearfix" itemscope itemtype="https://schema.org/Article">
            <div class="kratos-status">
                
                    <i class="fa fa-refresh"></i>
                
                <div class="kratos-status-inner">
                    <div class="kratos-status-content" itemprop="articleBody">
                        
                            <h1 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h1><ol>
<li>完成 GuessFuse 阅读。</li>
<li>阅读 GuessFuse 提到的多种模型结合方法。（也可能是密码强度估计器。）<ol>
<li>有一种[33]介绍了一种称为 hyPassGu 的混合猜测框架。 hyPassGu 通过限制每个模型生成目标类型的密码并分别确定猜测次数，利用 PCFG 和马尔可夫模型的优势。 尽管声称 hyPassGu 可以应用于 PCFG 和马尔可夫之外的其他模型， 和MOE 的思路有点像</li>
</ol>
</li>
<li>MOE 相关的论文和博客阅读。</li>
</ol>
<h1 id="密码生成思路"><a href="#密码生成思路" class="headerlink" title="密码生成思路"></a>密码生成思路</h1><p>参考 [[GuessFuse]] 感觉结构可以</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="道德考虑。"><a href="#道德考虑。" class="headerlink" title="道德考虑。"></a>道德考虑。</h3><p>尽管我们使用的数据集是公开的，并广泛用于密码猜测研究 [8]、[27]、[32]–[34]，但这些数据集包含敏感的个人信息。 因此，我们仅分析密码数据的分布特征，并报告汇总的统计信息。 我们不会将任何数据用于学术研究以外的目的，因此不会增加受影响个人的风险。</p>

                        
                    </div>
                </div>
            </div>
            
                <footer class="kratos-post-meta-new">
                    <span class="pull-left">
                        <time datetime="2024-04-22T13:14:48.845Z" itemprop="datePublished">
                            <a><i class="fa fa-calendar"></i> 2024-04-22</a>
                        </time>
                        
                        
                            <!-- 当前仅支持valine/waline自带的统计功能 -->
                            
                        
                        
                            <!-- 当前仅支持waline自带的统计功能 -->
                            
                        
                    </span>
                    
                    
                        <span class="pull-right">
                            <a class="read-more" href="/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/TEMP/" title="留言">留言<i class="fa fa-chevron-circle-right"></i></a>
                        </span>
                    

                </footer>
            
        </article>
    

    
    
        <article class="kratos-hentry kratos-entry-border-new clearfix" itemscope itemtype="https://schema.org/Article">
            <div class="kratos-status">
                
                    <i class="fa fa-refresh"></i>
                
                <div class="kratos-status-inner">
                    <div class="kratos-status-content" itemprop="articleBody">
                        
                            <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><ol>
<li>传统基于规则的方法依赖于专家知识</li>
<li>最新的基于深度学习方法，passGAN rpassGAN 在某些方面不如 PCFG</li>
<li>不同的深度学习方法有不同的候选密码空间，且无法重叠。</li>
<li>本文提出 REDPACK ，解决了 GAN 方法的不足，以有效的方法集合了多个候选密码生成器模型。<ol>
<li>使用 rPassGAN 的候选密码鉴别器作为密码选择器。</li>
<li>有选择的搜集密码，我们实现了更真实的密码字典。</li>
</ol>
</li>
<li>在符号，数字，大小写字母组成的密码字典上测试我们的系统。<ol>
<li>优于 基于规则，GAN，PCFG，</li>
<li>生成候选密码减少 65% 时，破解性能仅损失 20%。</li>
</ol>
</li>
</ol>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><ol>
<li>基于规则破解。基于专家的知识扩展性差。</li>
<li>先进的密码破解很有用<ol>
<li>防御角度：作为密码强度估计器</li>
<li>攻击角度：通过多种方式利用先进的密码破解方法。<ol>
<li>个人角度：破解自己忘记了的密码。</li>
<li>社会层面：执法机构需要破解密码来收集犯罪证据。</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="旧的：Recurrent-PassGAN"><a href="#旧的：Recurrent-PassGAN" class="headerlink" title="旧的：Recurrent PassGAN"></a>旧的：Recurrent PassGAN</h2><p>对 PassGAN 的改进，弥补基于规则的方法和数据驱动的方法，例如马尔科夫模型等。</p>
<ol>
<li>模型足够表示用户使用密码的模式和上下文结构。</li>
<li>深度学习不需要密码属性和结构的先验知识，只从训练数据中学习特征。</li>
</ol>
<p>普通 PassGAN：使用了Wasserstein GAN梯度惩罚（WGAN-GP）[11]模型。 WGAN-GP 的基础深度神经网络是基于卷积神经网络 (CNN) 的残差网络。</p>
<p>rPassGAN 通过修改PassGAN的基础神经网络类型和结构来提高密码破解性能。</p>
<p>rPassD2CGAN（rPassGAN 的双鉴别器版本）的性能优于 rPassGAN。然而，在rPassD2CGAN的训练过程中，它有时会变得不稳定。因此，我们通过使用另一个模型 rPassD2SGAN 来克服这个弱点。我们通过多个实验证明了 rPassGAN 生成的候选密码对于增强密码强度估计器的有效性。</p>
<p>与概率上下文无关语法（PCFG）和其他马尔可夫模型（例如 OMEN [1]）相比，rPassGAN 有时破解的密码较少。</p>
<h2 id="新的：REDPACK"><a href="#新的：REDPACK" class="headerlink" title="新的：REDPACK"></a>新的：REDPACK</h2><p>核心：通过少量猜测来最大化密码破解性能。<br>策略：</p>
<ol>
<li>有选择地收集更真实的候选密码，以提高密码破解的效率。如果可以使用各种模型的候选密码字典。</li>
<li>相对论平均标准 GAN [13] 的判别器作为估计器运行，它评估来自各种预先生成的候选者的输入密码的真实性。</li>
<li>强调：通常只使用 GAN 生成器，我们还使用了 GAN 鉴别器。<br>贡献：</li>
<li>GAN 判别器用于帮助生成密码。</li>
<li>REDPACK 能用多个预生成的候选密码制作更有效的候选字典。</li>
<li>为 REDPACK 构建了一个自定义规则集。</li>
</ol>
<h2 id="文章结构"><a href="#文章结构" class="headerlink" title="文章结构"></a>文章结构</h2><ol>
<li>第 2 节：相关密码猜测模型</li>
<li>第 3 节：GANs 和 relativistic average GANs</li>
<li>第 4 节：REDPACK 的概念和架构</li>
<li>第 5 节：训练超参数的配置过程。实验结果</li>
<li>第 6 节：结论</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><ol>
<li>基于规则的方法：<ol>
<li>hashcat ，JTR 开源软件多种攻击方式。</li>
<li>马尔科夫模型。</li>
<li>缺点：有明确的密钥空间。</li>
</ol>
</li>
<li>马尔可夫和 PCFG。<ol>
<li>使用符号：数字，字母，符号，单词。组合包含在特定概率分布中</li>
<li>使用马尔可夫过滤器消除滴概率密码</li>
<li>使用彩虹标加速。</li>
<li>图灵机处理特殊符号密码。</li>
<li>一些改进</li>
<li>PCFG 使用学习语法结构，密码组合。</li>
<li>一些优化。</li>
</ol>
</li>
<li>深度学习方法<ol>
<li>PassGAN ：WGAN-GP 代价函数 + CNN</li>
<li>rPassGAN：RNN<br> 效果：能破解 PCFG 等其他模型无法破解的密码。</li>
</ol>
</li>
</ol>
<h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><ol>
<li>[[GAN]] : GAN, WGAN, IWGAN 。其中 IWGAN 实验激发了 Hitaj 等人的兴趣。 [9]将IWGAN应用于密码猜测问题。 他们将创建的模型称为 PassGAN。</li>
</ol>
<h2 id="Relativistic-average-GAN"><a href="#Relativistic-average-GAN" class="headerlink" title="Relativistic average GAN"></a>Relativistic average GAN</h2><ol>
<li>GAN 可分为 IPM 和 non-IPM，<ol>
<li>non-IPM 在学习阶段不稳定，模型优化困难。</li>
<li>IPM： Jolicoeur [13] 分析了 SGAN 的损失函数，以分析非基于 IPM 的 GAN 的局限性，并提出了一种相对论 GAN 来解决这个问题。</li>
</ol>
</li>
<li>来源：<ol>
<li>问题：古德费洛等人。 [10]证明，当判别器以 0.5 的概率对真实数据进行分类时，GAN 训练达到全局最优。 然而，在许多情况下，判别器将真实数据和假数据都分类为真实数据，这对于训练好的生成器来说是不利的。 </li>
<li>这是因为判别器没有意识到学习过程中一半的数据是假的； </li>
<li>基于 IPM 的 GAN 在学习过程中相对稳定，因为它隐含地解释了这一事实。 </li>
<li>从散度最小化的角度来看，训练判别器以增加 D(xf)，而 D(xr) 不会相应减少，其中 xr 和 xf 分别表示真实数据和假数据。 为了解决这个问题，Jolicoeur [13] 设计了判别器的输出，使其依赖于真实数据和虚假数据。</li>
</ol>
</li>
<li>relativistic GAN<br> ![[Pasted image 20240416105310.png]]<br> 其中 C(x) 是假定的批评者 (D(x) &#x3D; σ(C(x)))。 方程（2）可以解释为判别器对给定真实数据比假数据更真实的概率的估计（D(xf, xr)可以以相反的方式解释）。 如果根据等式（2）设置判别器，则与 SGAN 的生成器仅依赖于假数据不同，相对论性 GAN 的生成器将同时依赖于真实数据和假数据。 然而，它在计算损失时具有 O(m2) 复杂度（m 表示小批量的大小），因为它计算小批量中真数据和假数据之间的批评家的成对差异。</li>
<li>RaGAN<br>  为了解决这个问题，Jolicoeur [13] 提出了一种相对论平均 GAN（RaGAN），它对某些给定数据取相反类型数据的期望。 RaGAN 使用以下损失函数来学习判别器和生成器：<br> ![[Pasted image 20240416105456.png]]<br> 其中 LD 和 LG 分别表示学习判别器和生成器的损失。 方程(3)的复杂度为O(m)。 RaGAN 的判别器平均估计某些给定数据比相反类型的数据更真实的概率。 Jolicoeur [13] 使用不同的数据集表明，与其他 GAN 模型相比，训练 RaGAN 更快、更可靠，并且 RaGAN 的生成器生成了高质量的假数据。</li>
</ol>
<h1 id="方法介绍：REDPACK"><a href="#方法介绍：REDPACK" class="headerlink" title="方法介绍：REDPACK"></a>方法介绍：REDPACK</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ol>
<li>背景：之前 GAN 训练完成之后使用 GAN 生成器实现目的，作者在之前的论文中提到了基于 RNN 的 GAN 方法 rPassGAN ，本文使用的就是 rPassGAN 的 判别器</li>
<li>REDPACK：GAN 的判别器是在模型训练结束之后使用的。<br> ![[Pasted image 20240416111123.png]]<br> 训练阶段：<ol>
<li>生成器生成假密码，鉴别器区分，对抗训练，直到收敛。<br> 选择阶段：</li>
<li>使用多个密码生成器产生候选密码。作为训练阶段的结果。（本文使用了 三个或者四个）</li>
<li>鉴别器计算每个候选密码真实程度的概率。</li>
<li>将高概率的候选密码提供给 Hashcat等密码破解工具。</li>
</ol>
</li>
</ol>
<h2 id="鉴别器训练结构"><a href="#鉴别器训练结构" class="headerlink" title="鉴别器训练结构"></a>鉴别器训练结构</h2><p>架构：<br>![[Pasted image 20240416143214.png]]<br>优化基于 RNN 的 GAN （判别器优化：使用了 RaGAN 和 IWGAN 的概念来实现了更强大的判别器）的流程：</p>
<ol>
<li>G 根据给定的任意噪声分布生成假密码，图中绿色路径；</li>
<li>根据等式 (3) ，当且仅当假密码的批评值大于图中蓝色存储所描述的真实数据时，D 才确定假密码为真实密码。</li>
<li>蓝色路径上的真实密码在相反情况下被视为真实的。(这里的相反可能是指的比较对象和)</li>
<li>D 给出梯度作为惩罚，鼓励 G 生成更真实的密码，用红色路径表示。<br>这样的流程使得 D 会生成比标准 GAN 更强大的判断。训练过程，算法描述如下：<blockquote>
<pre><code>梯度惩罚系数 λ、每个生成器的判别器迭代次数 ncritic 、每个判别器的生成器迭代次数 ngen 、批量大小 m 和 Adam 超参数 α、β1 和 β2。 Critic C 的参数 w 和生成器 G 的参数 θ。 真实密码 xr 和假密码 xf 之间直线上的随机样本 ˆx。
</code></pre>
<p> ![[Pasted image 20240416155902.png]]</p>
</blockquote>
</li>
<li>主要源自 IWGAN，但使用相对论平均 GAN 的损失函数。 大多数符号遵循方程 (3)。 主要区别在于第 9 行和第 15 行中的损失函数。</li>
<li>这些损失函数取决于相对论判别器，估计一种类型密码的批评者相对于相反类型密码的平均批评者。 这种直接比较使 G 能够快速收敛到最佳点并产生高质量的假密码。</li>
<li>此外，由于我们基于 RNN 的 GAN 采用​​ IWGAN，因此我们将梯度惩罚添加到判别器的损失函数中。 此惩罚迫使 ^x 的梯度 2-范数小于 1，其中 ^x 是点对 (xr, xf) 之间直线上的随机样本。 这为 GAN 的训练提供了极大的稳定性。 </li>
<li>另一个重要因素是从第 13 行到第 16 行优化 G 的迭代。尽管大多数 GAN 都有一个在给定 G 上优化 D 的循环，但这不足以最大化 GAN 的性能。 因此，我们添加了训练 G 的循环来稳定和增强我们的 GAN。 根据我们在第 5 节中描述的实验，该因素对 REDPACK 的破解性能具有关键影响。 一般情况下，一旦模型被优化，生成器就被用作生成模型。 相反，我们利用鉴别器作为 REDPACK 的真实密码估计器。</li>
</ol>
<h2 id="密码生成器选择"><a href="#密码生成器选择" class="headerlink" title="密码生成器选择"></a>密码生成器选择</h2><p>流程图：<br>![[Pasted image 20240416171606.png]]</p>
<ol>
<li>多个候选密码生成器，比如：Hashcat、PCFG、rPassGAN 与 WGAN-GP 以及 rPassGAN 与 RaGAN-GP。</li>
<li>注意：不同损失函数的 rPassGAN 可以作为不同的 生成器使用，因为它们有不同的密码破解结果。</li>
<li>注意：单鉴别器 rPassGAN 和双鉴别器 rPassGAN 也是。</li>
<li>每个候选密码生成器生成的 10 亿个候选密码从字符串转换为张量，传递给鉴别器（D），估计密码真实性。</li>
<li>由最大概率选择器选择最高的密码，转换为 字符串形式。</li>
<li>这些候选者被传输给 Hashcat 破解。</li>
</ol>
<h1 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h1><h2 id="实验数据准备"><a href="#实验数据准备" class="headerlink" title="实验数据准备"></a>实验数据准备</h2><ol>
<li>大多数之前的研究使用的是 Rockyou 和 Linkedln。</li>
<li>但是本文在性能测试中使用到了包含长密码和 4 级密码的额外密码字典。所以使用 Melicher 等人的密码分类。 [27]。总共有七个训练和破解数据集：<ol>
<li>Rockyou 和 LinkedIn 包含一些 4 级密码。 </li>
<li>我们还使用了来自 Hashes.org 的四个破解密码字典，其中提供了多个破解和泄露的明文密码。<br> ![[Pasted image 20240416194910.png]]</li>
</ol>
</li>
</ol>
<h2 id="超参数配置"><a href="#超参数配置" class="headerlink" title="超参数配置"></a>超参数配置</h2><ol>
<li>G:D：RaSGAN 通常使用 1:1 或者 1:10 </li>
<li>批次大小： 通常使用 128。使用 64 的批量大小可以来应对训练不稳定。</li>
<li>epoch<br>具体如下图：<br>![[Pasted image 20240416201514.png]]</li>
</ol>
<h2 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h2><p>Tensorflow-gpu 1.10.1 和 Python 版本 3.5.4 进行 GPU 计算。 所有实验均在韩国大学NMLab的OpenHPC系统上进行。 发达。 OpenHPC的每个节点运行在具有32GB内存的CentOS 7服务器上； 这些节点使用 Intel Xeon E5 2.20GHz CPU(x2) 和 Nvidia TitanXP 12GB GPU(x4)。</p>
<h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><ol>
<li>根据我们之前的研究[8,12]推断，决定破解性能的主要因素是epoch、G&#x2F;D比和循环神经网络（RNN）细胞类型。</li>
<li>如果 Epoch 过大，会导致过拟合。如图 4a 所示。</li>
<li>提高破解性能，需要在模型中应用较大范围的 G&#x2F;D 比，但是比较费时。从而退一步：<br> 因此，我们对RaSGAN-GP成本函数使用1:1和1:10的G&#x2F;D比进行了实验，如图4b所示。 虽然两者都可能不是最优值，但它们足以说明模型的有效性。 </li>
<li>最后要考虑的因素是神经单元类型。对于基于 RNN 的相对论判别器，有必要确定长短期记忆 (LSTM) [28] 和门控循环单元 (GRU) [33] 之间哪个更好。<br>![[Pasted image 20240416202041.png]]<br>结论：整个实验中，200k 训练周期、1:10 G&#x2F;D 比率和 GRU 单元类型被确定为密码破解的最佳设置并确保通用性，如图 5 所示。<br>![[Pasted image 20240416202513.png]]</li>
</ol>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>所有实验，我们的结果都优于其他模型。</p>
<ol>
<li>REDPACK 在数据集 2 到数据集 7 上的破解性能比任何单一密码猜测模型高出 5-20%。 在数据集1（短长密码）的情况下，PCFG在早期破解阶段表现出了很强的性能。 下半程的破解，REDPACK最终反超了PCFG。 然而，随着密码长度变长（从 dataset3 到 dataset7），REDPACK 的性能明显优于 PCFG 和其他密码候选生成器。 也就是说，对于复杂且长的密码，REDPACK的有效性得到了明显的体现。</li>
<li>对于数据集 6 和 7，由于训练数据量较小，我们的团队无法创建 10 亿个 Hashcat 候选者。</li>
<li> 在表3中，最大概率选择器从图3中选择的每个模型的候选数量与单个模型的密码破解性能成正比。 这个结果表明REDPACK不是随机选择候选者而是选择性地选择候选者。 此外，这意味着 REDPACK 的鉴别器可以正确评估生成真实密码的概率。</li>
</ol>
<h2 id="REDPACK-的限制"><a href="#REDPACK-的限制" class="headerlink" title="REDPACK 的限制"></a>REDPACK 的限制</h2><ol>
<li>选择了更现实的密码候选者。 然而，更现实的密码候选选择并不总是能保证有效的密码破解。将候选密码的数量压缩了，但它也遗漏了一些对密码破解可能很重要的候选密码。</li>
<li>将 OMEN 作为密码候选生成器组件时EDPACK 的候选字典的破解性能恶化。<br> 原因： OMEN 和 PCFG 都有相似的特性。 它们都以高阶概率生成候选密码（本身就筛选了一些密码）<br> 对 OMEN 和 PCFG 中的密码候选集应用随机洗牌。 这种简单的方法不能完全消除破解性能的损失。</li>
<li>基于三种不同方法（Hashcat：基于规则、PCFG：基于概率、rPassGAN：基于深度学习）的模型用作生成器时，REDPACK 在我们的实验中效率最高。</li>
</ol>
<h2 id="后续工作"><a href="#后续工作" class="headerlink" title="后续工作"></a>后续工作</h2><p>自定义了一套规则集。</p>
<ol>
<li>base64 是 hashcat 最高效的规则集。</li>
<li>搜集：将7条Hashcat规则（best64、dive、specific、generate、InsidePro-PasswordPro、Incisive-leetspeak、T0X1Cv1）组合成一个巨大的规则文件。</li>
<li>测试：每条规则记录有助于密码破解的机会数量。</li>
<li>选择：对于REDPACKU4的自定义Hashcat规则集，选择了100条规则（与best64相同的数量）<br>试验证明有效的。</li>
</ol>

                        
                    </div>
                </div>
            </div>
            
                <footer class="kratos-post-meta-new">
                    <span class="pull-left">
                        <time datetime="2024-04-22T13:14:48.844Z" itemprop="datePublished">
                            <a><i class="fa fa-calendar"></i> 2024-04-22</a>
                        </time>
                        
                        
                            <!-- 当前仅支持valine/waline自带的统计功能 -->
                            
                        
                        
                            <!-- 当前仅支持waline自带的统计功能 -->
                            
                        
                    </span>
                    
                    
                        <span class="pull-right">
                            <a class="read-more" href="/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/REDPACK/" title="留言">留言<i class="fa fa-chevron-circle-right"></i></a>
                        </span>
                    

                </footer>
            
        </article>
    

    
    
        <article class="kratos-hentry kratos-entry-border-new clearfix" itemscope itemtype="https://schema.org/Article">
            <div class="kratos-status">
                
                    <i class="fa fa-refresh"></i>
                
                <div class="kratos-status-inner">
                    <div class="kratos-status-content" itemprop="articleBody">
                        
                            <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><ol>
<li>密码攻击频繁</li>
<li>利用密码的相似性和可预测性之间的相关性。使用基于规则的方法，但将规则推导、分类和预测委托给循环神经网络 (RNN)。</li>
<li>试验结果：尝试的猜测次数限定为10次，结果 5 次就达到了 83% 准确率。其他模型的两倍</li>
<li>模型能有效进行有针对性的<strong>在线密码</strong>猜测，而不会恢复或者锁定。</li>
</ol>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><ol>
<li>密码使用频繁，密码有规律性。</li>
<li>密码攻击频繁，使用之前网站泄漏的密码攻击同一用户的新密码。</li>
<li>密码攻击方式多样，字典，规则，模型。</li>
<li>基于旧的密码，规则能在很少的次数下，对新的网站达到很高的破解率。</li>
<li>本文使用基于规则的方法，使用神经网络自动执行猜测过程。<ol>
<li>导出修改模式，</li>
<li>建立分类，神经网络方法</li>
<li>生成密码猜测，BiLSTM模型</li>
</ol>
</li>
<li>本文建立了一个实验模型，可以在不知道猜测模式的情况下，进行预测。<ol>
<li>使用基于字符 LSTM 编码器解码器模型</li>
</ol>
</li>
<li>本文将 RNN 和 预训练Transformer 用于（研究较少的）密码短语猜测领域，<ol>
<li>构建了一个具有注意力机制的双向 LSTM 模型，预测短语的模式（用少量短语生成完整的密码），</li>
<li>比传统方法所需要的猜测次数明显减少。</li>
</ol>
</li>
</ol>
<h1 id="密码猜测——针对性在线猜测"><a href="#密码猜测——针对性在线猜测" class="headerlink" title="密码猜测——针对性在线猜测"></a>密码猜测——针对性在线猜测</h1><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><ol>
<li>来源：2018 ，6000，0000 个密码</li>
<li>处理：<ol>
<li>数据集中每个用户至少两个密码，只选择两个构成密码对。</li>
<li>消除重复密码对，和多次出现的密码，剩余 1700，0000 唯一密码对。</li>
</ol>
</li>
<li>两个指标来评价数据分布：<ol>
<li>编辑距离是将一个字符串转换为另一个字符串所需的编辑（例如，替换、插入或删除）次数。数据集中，密码之间的编辑距离范围为 0 到 17，如图 1 所示。大多数密码对的编辑距离范围为 1 到 11。编辑距离可以帮助设置密码重用规则易于用户理解（例如，确保后续密码与原始密码相差三个字符）。</li>
<li>表现出最高相似度的密码将具有最小的 Levenshtein 距离和最高的 JaroWinkler 距离，并且将是跨站点猜测攻击的最佳候选者。</li>
</ol>
</li>
<li>处理：<ol>
<li>已知规则：在之前的研究中，确定了几种最常见的修改模式，包括子字符串、公共子字符串、大写、Leet 和顺序键（Wang 等人，2018 年；Walia 等人，2020 年）。我们根据这五种模式标记每个密码对，以创建一个标记数据集。最后，不符合任何这些规则的对将被丢弃。带标签的数据集包含 3,006,871 个唯一密码对。</li>
<li>工作集：唯一密码对中 长度在 5 - 17 的字符。</li>
</ol>
</li>
</ol>
<h2 id="模型预测过程"><a href="#模型预测过程" class="headerlink" title="模型预测过程"></a>模型预测过程</h2><ol>
<li>目标：预测流程：对上面处理好的每一个数据：识别标签，分类，生成预测。<ol>
<li>第一步，定义常见的修改模式，分析每个密码对并标记相应的类别。</li>
<li>在第二步中，我们使用神经网络模型将每个原始密码分配到单个修改类别中。这个过程称为单标签预测问题。</li>
<li>在第三步中，我们构建第二个模型来了解每个类别中可能的修改。该模型对测试数据的准确率达到 90%（），可以理解并生成每个类别的所有可能的修改。</li>
<li>然后组合两个模型，并在最后一步组装生成的管道。我们的方法可以仅将一个原始密码作为输入，将其分类为修改类别，并生成密码。</li>
</ol>
</li>
<li>标记：同上面的处理过程。</li>
<li>分类：使用 Keras Python 库构建了一个 4 层分类器，用于自动密码修改类别预测。<ol>
<li>输入层采用与数据集中最长密码相同长度的单个字符序列，即 17 个字符长。 </li>
<li>One-Hot 编码器将每个密码处理为字符序列，并将这些序列转换为 One-Hot 数值数组。</li>
<li>编码被传递到 LSTM 单元。我们使用字符级双向 LSTM (BiLSTM) 层，它是传统 LSTM 的扩展，可以提高模型在序列分类问题上的性能。 BiLSTM 层在两个方向上运行输入，一个从过去到未来，另一个从未来到过去。与单向 LSTM 不同，BiLSTM 使用两个隐藏状态，可以保留过去和未来任意时间点的信息。由于这些特性，BiLSTM 可以更好地理解序列中每个字符周围的上下文（Xu 等人，2019）。 BiLSTM 单元的输出被馈送到密集的激活层。</li>
<li>激活层包含一个激活函数，它定义如何将输入的加权和转换为输出。为了确保模型学习特征并且不会过早收敛，我们使用具有较小学习率的 Adam 优化器（Kingma &amp; Ba，2014）。该优化器用于在每次训练迭代期间更新网络权重。</li>
</ol>
</li>
<li>生成：训练字符级 BiLSTM 模型来生成每个修改类别内的密码。<br> 输入层 1 将先前使用的字符序列形式的密码作为输入，而输入层 2 将密码修改类别作为输入。第一个流包括输入层 1、One-Hot 编码器和 BiLSTM 层。这三层的作用与第 3.2.2 节中描述的类似。第二个输入流包括输入层 #2 和重复向量层，这两个层都用于在单个类别内生成每个预测时向模型添加修改模式列表。最后，串联层组合两个流的输出，并将组合的输出馈送到激活层。激活层的作用与第 3.2.2 节中描述的相同。生成的模型可以为每个修改类别生成高度准确的密码猜测。<br>我们最终采用了这种架构，因为它在超参数调整过程中产生了最佳性能。然而，添加更多 LSTM 层可能会导致模型过度拟合。</li>
</ol>
<p>此外，单独实现了一个直接密码预测，在我们的模型中，它将确定输出修改后的密码。由于模型可以生成具有不同置信度的多个预测，因此我们需要一种算法来选择前十个最可能的输出。我们使用 Beam 搜索算法，这是序列到序列机器翻译问题中最广泛使用的算法之一（Yoo 等人，2020），来帮助我们识别最可能的预测。直接密码预测是最有前途的方法，因为它消除了不断的规则推导和分布分析的需要，并简化了数据集预处理。由此产生的模型，即直接预测机制 (DPM)，是独立于规则的，并提供高预测率。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>在 Google Colab Pro（一个基于云的 Jupyter 笔记本环境）上运行了该项目的大部分内容。</p>
<p>托管运行时环境使用 Tesla P100-PCIE-16GB GPU、Intel(R) Xeon(R) CPU @ 2.20GHz 处理器、25GB RAM 和 109GB 磁盘空间。实验中对硬件要求最高的部分是数据集预处理、模型训练和直接密码预测。直接密码预测的计算成本最高。根据 Google Colab 的测量，在 40 万条记录上训练模型需要 13GB RAM，在 50 万条记录上训练模型需要 20GB。</p>
<h1 id="短语猜测——针对性离线攻击"><a href="#短语猜测——针对性离线攻击" class="headerlink" title="短语猜测——针对性离线攻击"></a>短语猜测——针对性离线攻击</h1><p>第一种方法采用基于单词级注意力的 LSTM 模型，这与我们在第 3 节中开发的没有注意力机制的字符级 LSTM 模型不同。<br>第二种方法利用 OpenAI 提供的生成式预训练 Transformer (GPT-2)，并且可供公众使用 Radfordetal。 （2019）。</p>
<h2 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h2><p>单词集语料库中的短语集（单词数量2 3 4）+ 频率<br>生成的数据集包含 574,531 个短语，其中 99,953 个是不同的。</p>
<h2 id="攻击向量"><a href="#攻击向量" class="headerlink" title="攻击向量"></a>攻击向量</h2><ol>
<li>输入是一个短语，用于预测后续短语;</li>
<li>通过使用马尔可夫链模型建立基线来开始我们的实验。该模型将二元语法或三元语法的第一个单词作为输入，并使用马尔可夫算法生成短语的其余部分。</li>
<li>然后，该模型根据两个单词同时出现的统计概率构建一个转移矩阵。</li>
<li>结果，模型生成预测，我们将其与目标短语进行比较，看看它们是否匹配。</li>
</ol>
<h2 id="第一种——LSTM"><a href="#第一种——LSTM" class="headerlink" title="第一种——LSTM"></a>第一种——LSTM</h2><p>![[Pasted image 20240409191656.png]]</p>
<ol>
<li>由编码器和解码器组成<ol>
<li>编码器：获取输入序列并且将信息汇总为上下文向量。<ol>
<li>输入层：第一个单词作为预测的起点。</li>
<li>嵌入层：使用 GloVe 模型（2014）进行词嵌入；</li>
<li>BiLSTM层：压缩输入，降低向量维度。</li>
</ol>
</li>
<li>上下文向量：所有编码器单元的输出作为输入来计算每个单字解码器想要生成的概率分布。它帮助解码器捕获有关输入的整体信息。</li>
<li>解码器：生成预测<ol>
<li>输入层：上一步的向量；</li>
<li>注意力机制：多个注意力层<ol>
<li>使用函数而不是使用单个隐藏状态来编码和解码</li>
<li>上下文向量将所有编码器单元的输出作为输入来计算每个单字解码器想要生成的概率分布</li>
</ol>
</li>
<li>基于LSTM层。</li>
</ol>
</li>
</ol>
</li>
<li>使用 Adam 优化器和稀疏分类交叉熵作为损失函数。使用稀疏分类交叉熵的一个优点是更好的内存和计算资源利用率，因为它为每个类使用单个整数而不是整个向量。</li>
</ol>
<h2 id="第二种——GPT-2"><a href="#第二种——GPT-2" class="headerlink" title="第二种——GPT-2"></a>第二种——GPT-2</h2><p>Transformer 长期以来一直用于短语自动完成（Vaswani 等人，2017）。我们使用 GPT-2，一种开源语言模型（Radford 等人，2019）。<br>该模型的架构与纯解码器变压器非常相似。</p>
<ol>
<li>由堆叠在一起的多个解码器块组成。</li>
<li>每个解码器块都有一个前馈神经网络和一个屏蔽自注意力层。</li>
<li>屏蔽自注意力层基于模型对相关和关联单词的理解以及处理该单词之前所需的上下文。该层分配定义片段中每个单词的相关性的分数，并将它们的向量表示相加（Vaswani 等人，2017）。</li>
<li>矢量表示被传递到完全连接的神经网络层进行处理。<br>实验过程：</li>
<li>使用具有 124M 参数的 GPT-2 Small 模型版本。</li>
<li>使用的 4 元数据集样本中提取的单词列表上重新训练它。</li>
<li>还利用模型作者提供的一些超参数<ol>
<li>前缀参数用于为模型提供短语的开头。</li>
<li>温度参数是控制随机性的浮点数。较低的温度使预测更加重复。</li>
<li>长度控制生成的短语将包含多少个单词。</li>
<li>参数 top_p 有助于缩小预测范围以选择最佳候选者。</li>
<li>我们生成的短语数量由 N_samples 参数控制，设置为 50。</li>
</ol>
</li>
</ol>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>不幸的是，大多数模型需要进行大量尝试才能正确猜测密码。请注意，现有工作的大多数作者没有透露他们实验的每个细节和参数。此外，在现有模型和我们的模型之间进行公平的一对一比较非常困难，因为它们经常使用并不总是提供的不同数据集。</p>
<p>LSTM 模型随时间变化的预测率。该模型在大约 1000 次尝试中破解了 24% 的密码。经过大约 5000 次尝试，它最终达到了 40% 的最大预测率。然后预测率开始趋于平缓。</p>
<p>除了我们开发的 LSTM 和 GPT-2 模型之外，我们还实现了基于马尔可夫的模型作为基线 和  我们开发和配置的模型之间的详细比较。我们对基于马尔可夫的模型的实现并未针对有效的 GPU 和资源利用进行优化，并且在大约 20 次尝试后停止收敛。那时，我们无法记录预测率。如果我们将尝试次数设置为高于 20，模型就会开始超时。</p>
<ol>
<li>对于 LSTM，我们发现尝试次数越多，资源就会成比例增加；然而，总体利用率仍然较低。 </li>
<li>GPT-2 模型在预测较长短语方面表现最佳，但需要更多输入才能更快地提供准确的猜测。</li>
<li>即使对于不太频繁出现的短语（通常更难破解），LSTM 也表现出了出色的预测率。</li>
</ol>
<p>该实验断言，对于基于马尔可夫的模型和 LSTM，较长的短语和具有三个或更多单词的短语更难预测。我们所有的模型都显着增加了正确预测的数量，同时减少了尝试的次数。它表明我们的模型在破解密码方面非常有效。</p>
<h1 id="未来展望"><a href="#未来展望" class="headerlink" title="未来展望"></a>未来展望</h1><h2 id="密码的见解"><a href="#密码的见解" class="headerlink" title="密码的见解"></a>密码的见解</h2><p>原始密码进行轻微修改在跨站点攻击中几乎没有提供额外的安全性。姐妹密码之间的相似度越低，用户数据就越安全。<br>    因此构建一个主动密码检查器<br>    防止用户选择易于猜测的后续密码，特别是当它与使用的密码相似时。<br>服务提供商应考虑单独使用密码，转而使用双因素身份验证、生物识别、行为身份验证和其他替代手段</p>
<h2 id="密码短语的见解"><a href="#密码短语的见解" class="headerlink" title="密码短语的见解"></a>密码短语的见解</h2><ol>
<li>易于在企业级别进行实施和维护，对用户程序友好。</li>
<li>更长，更容易记住。</li>
<li>本质上是不安全的。构成密码的短语通常来自于主流文化和</li>
<li>容易被人工智能方式攻击。</li>
</ol>
<h1 id="未来展望-1"><a href="#未来展望-1" class="headerlink" title="未来展望"></a>未来展望</h1><ol>
<li>更多密码泄漏</li>
<li>原始密码生成后续密码，可以用新数据训练旧模型。</li>
</ol>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在本研究中，我们首先研究了有针对性的在线密码猜测问题。,我们使用 RNN 构建了一个密码预测管道来自动进行密码分类和生成。,预测结果优于传统的分类和猜测算法。,当我们将猜测尝试限制为五次时，性能提升尤其显着。,我们结合了对基于规则的预测算法的理解和 LSTM 神经网络的强大功能，解决了同一用户创建的密码的跨站点预测问题。,这是一种相对较新的方法，也许是使用 RNN 来完成此特定任务的首次尝试之一。,我们可以量化后续密码的相似性、修改模式和可预测性之间的相关性。,此外，我们还展示了最常见的修改策略的易于预测性和高精度，例如在原始密码或大写中添加头或尾符号。,我们展示了由于 RNN 模型的低复杂性和浅层性质，可以通过负担得起的硬件或在线计算资源（例如 Google Colab）来促进这种预测过程。,此外，预测效率使其可以在帐户被锁定之前允许尝试五次或更少的平台上运行。,我们还讨论了在线服务应采取的具体步骤，以提高身份验证过程的安全性。,然后我们探讨了使用 RNN 进行有针对性的离线密码猜测的问题。,我们构建了一个基于注意力的 LSTM 模型和一个微调的 GPT-2 模型来预测常用密码。,我们分析了结果，并将其与最常用的方法进行了比较，例如基于字典、基于规则和基于马尔可夫链的预测算法。,我们实现了明显更好的预测率，特别是考虑到使用的尝试次数、部署时间、资源利用率和操作简便性。,尽管这些方法并不是全新的，但它们在密码短语预测领域是新颖的，并且产生有竞争力的预测率。,资金 没有收到任何资金来协助准备本手稿。</p>

                        
                    </div>
                </div>
            </div>
            
                <footer class="kratos-post-meta-new">
                    <span class="pull-left">
                        <time datetime="2024-04-22T13:14:48.842Z" itemprop="datePublished">
                            <a><i class="fa fa-calendar"></i> 2024-04-22</a>
                        </time>
                        
                        
                            <!-- 当前仅支持valine/waline自带的统计功能 -->
                            
                        
                        
                            <!-- 当前仅支持waline自带的统计功能 -->
                            
                        
                    </span>
                    
                    
                        <span class="pull-right">
                            <a class="read-more" href="/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Pass_RNN/" title="留言">留言<i class="fa fa-chevron-circle-right"></i></a>
                        </span>
                    

                </footer>
            
        </article>
    

    
    
        <article class="kratos-hentry kratos-entry-border-new clearfix" itemscope itemtype="https://schema.org/Article">
            <div class="kratos-status">
                
                    <i class="fa fa-refresh"></i>
                
                <div class="kratos-status-inner">
                    <div class="kratos-status-content" itemprop="articleBody">
                        
                            <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><ol>
<li>LLM 无需显式监督： PassGPT</li>
<li>引导密码生成：利用 PassGPT 来生成匹配任意约束的密码。</li>
</ol>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>贡献：</p>
<ol>
<li>密码猜测</li>
<li>密码强度估计<br>PassGPT 多猜测 20% 的未见过的密码，并且对新的泄露展现了良好的泛化能力。<br>此外，我们通过矢量量化增强了 PassGPT [55]。由此产生的架构是 PassVQT，它可以增加生成密码的复杂性。</li>
</ol>
<p>与之前整体生成密码的深度生成模型不同，PassGPT 对每个字符进行顺序采样，从而引入了引导密码生成的新颖任务。此方法确保对搜索空间进行更细粒度（字符级）的引导探索，其中根据任意约束对生成的密码进行采样。</p>
<p>最后，与 GAN 相比，PassGPT 提供了密码概率分布的明确表示。我们证明密码概率与最先进的密码强度估计器一致：PassGPT 为更强的密码分配较低的概率。</p>
<p>总结如下：<br>– 我们引入了 PassGPT，一种自回归转换器，它在密码生成和对未见过的数据集的泛化方面获得了最先进的结果。<br>– 我们展示 PassGPT 如何在任意约束下启用一种新颖的密码生成方法：引导密码生成。<br>– 我们检查 PassGPT 下的密码概率以及它们如何与强度保持一致。我们讨论如何使用该指标来改进当前强度估计器。<br> – 我们提出了 PassVQT，这是一种通过矢量量化增强的类似架构，以增加生成的复杂性。</p>
<h1 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h1><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>![[Pasted image 20240319110425.png]]</p>
<ol>
<li>最主要的是  RockYou 和 LinkedIn</li>
<li>对于 RockYou，我们分别获取最多 10 个和 16 个字符的所有密码列表。<ol>
<li>我们将此列表中的 80% 作为训练数据。</li>
<li>在剩下的 20% 中，我们将训练分割中未包含的所有密码保留为测试数据，仅保留频率较低的密码。<ol>
<li>测试集中最常用的密码仅出现了7 次</li>
<li>平均频率为 1.03</li>
<li>有助于测试生成低概率密码的能力。</li>
</ol>
</li>
</ol>
</li>
<li>对于 LinkedIn <ol>
<li>LinkedIn 泄露事件没有提供有关密码频率的信息，因此我们将 80% 作为训练数据，其余 20% 用于评估。同样保证没有相同的数据 同时 出现在两组中</li>
<li>评估集中删除 RockYou 训练密码来定义交叉评估测试集，反之亦然，以评估对未见分布的泛化。</li>
</ol>
</li>
<li>最后，我们还将 MySpace、phpBB 和 Hotmail [52] 泄漏视为评估集。我们执行类似的交叉评估程序，从所有数据中删除 RockYou 和 LinkedIn 训练数据。 OST 10 OST。</li>
</ol>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>自回归生成模型。PassGPT 和PassVQT 对密码中某个字符的出现概率进行建模，给定前面的值，对密码中某个字符的概率进行建模，给定前面的值，从分布中顺序采样可以生成可能的密码。</p>
<ol>
<li>模型在词汇表 Σ 上运行，包含 256 个 UTF-8 字符。</li>
<li>tokenizer 定义为将词汇表中的每个字符 σ 映射为整数的函数，分词器：Σ 7→ [0, |Σ| − 1]。</li>
<li>然后，在 tokenizer 函数下使用其图像的 one-hot 编码为每个 token σ 创建向量表示。这会产生一个维度向量 |Σ|，在标记器 (σ) 位置处所有条目均等于 0，单个条目等于 1。</li>
</ol>
<h3 id="PassGPT"><a href="#PassGPT" class="headerlink" title="PassGPT"></a>PassGPT</h3><p>![[Pasted image 20240319150308.png]]<br>GPT 模型利用 Transformer 的解码器组件，并经过训练以自回归方式预测序列中的下一个标记。</p>
<ol>
<li>为了预测密码中的特定字符 xi，转换器解码器仅考虑之前的字符 x0,…。 。 。 ，xi−1 作为输入，</li>
<li>并输出一个维度为 d 的潜在向量（在我们的工作中 d &#x3D; 768）。</li>
<li>然后这个潜在向量被映射到维度 |Σ| 的实数向量通过线性层并使用 softmax 函数进一步转换为词汇表上的概率分布。</li>
<li>词汇表上的输出分布表示 p(xi|x&lt;i; θ)。使用相对于在该位置找到的真实字符的独热编码表示的交叉熵损失来优化该分布。<br>一旦网络经过训练，它就会为我们提供基于先前标记的词汇表的参数化分布，即 p(xi|x&lt;i; θ)。</li>
<li>我们可以从密码开始标记 &lt;\s&gt;  开始，并找到 p(x1|x0 &#x3D; &lt;\s&gt;)。这为我们词汇表中的每个字符分配了成为密码中第一个标记的概率。如果我们从这个分布中采样，我们可以固定第一个字符并通过计算 p(x2|x0, x1) 重复该过程来找到第二个字符。当在任何给定步骤从分布中采样到密码结束标记 &lt;/s&gt; 时，密码采样过程即告完成。<ol>
<li>与训练不同，此过程是连续的。我们的 PassGPT 实现使用 HuggingFace 库 [53]，并具有以下规范：12 个注意力头、8 个解码器层和 GeLU 激活 [24]。此外，我们使用 AdamW 优化器对所有模型进行 1 轮训练，起始学习率为 5e-5，训练期间线性衰减。</li>
</ol>
</li>
</ol>
<h1 id="PassVQT"><a href="#PassVQT" class="headerlink" title="PassVQT"></a>PassVQT</h1><p>使用潜在空间的矢量量化增强了转换器架构。在计算机视觉领域，这已被证明可以提高样本质量[55]。 PassVQT 遵循 Yu 等人设计的架构。  [55]。 </p>
<ol>
<li>在对与 PassGPT 相同的条件分布进行建模时，我们的目标是评估量化是否可以提供任何额外的好处。</li>
<li>在图 2 所示的架构中，变压器编码器将每个输入标记映射到维度为 768 的潜在表示。</li>
<li>然后使用线性层将该潜在表示映射到 10 维，并使用 k 均值和具有 N 的码本进行量化条目。量化的 10 维向量通过线性层映射回 768 维，并作为变压器自回归解码器的输入。</li>
<li>该解码器经过训练，可以仅使用先前标记的量化表示来逐个字符地重建输入密码。</li>
<li>我们通过最小化 RockYou 泄漏训练分割的重建损失来进行超参数搜索。</li>
<li>我们的研究结果表明，更深的编码器和解码器结构可以提供更好的结果，码本大小为 300 可以提供最佳性能。</li>
<li>PassVQT 采用变压器编码器和 GPT-2 解码器，分别具有 12 个注意力头和 8 个层。它是使用 HuggingFace 库 [53] 实现的，并使用 AdamW 优化器进行端到端训练，起始学习率为 5e-5，具有线性衰减。一旦编码器-解码器网络收敛，该模型就可以从压缩的量化潜在表示重建输入密码。如果我们对潜在代码的分布进行建模，我们可以从中采样以生成可能的代码序列，然后解码器可以将其转换为可能的密码。为此，我们在训练数据集的量化表示上训练自回归代码模型。在推理过程中，我们通过从代码模型中采样代码序列并使用原始解码器将它们转换为密码来创建新密码。不再需要编码器。</li>
</ol>
<p>4.3 引导式生成<br>我们提出了一种新颖的密码生成方法：引导式密码生成。<br>与之前生成整体密码的深度生成方法不同，PassGPT 分别对每个令牌进行建模，从而授予对每个字符的完全控制权。<br>这使得生成过程能够满足特定的约束。<br>这些限制的一些示例包括：密码长度、固定字符（例如，第一个位置的“a”）和模板（例如，四个小写字母和两个数字）。<br>这可以通过限制采样分布 p(xi|x0, · · ·, xi−1) 以仅考虑分配给感兴趣子集 Σ′ ⊂ Σ 的概率质量来实现；例如，限制 Σ′。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>将 PassGPT 和 PassVQT 和最新的深度生成模型，并演示了它们对不同数据集的泛化，而无需进一步训练。<br>检查了 PassGPT 下密码的概率和熵，以深入了解其功能和建模分布。</p>
<p>训练集的两种变体：（1）唯一密码和（2）所有出现的密码。</p>
<ol>
<li>在使用唯一密码进行训练时，PassGPT 表现出卓越的泛化能力，</li>
<li>相反，在使用唯一条目进行训练时，PassVQT 会遇到生成分发内密码的困难，但在合并其绝对出现次数后会显着改进。</li>
<li>从 PassGPT（针对唯一密码进行训练）和 PassVQT（针对所有密码进行训练）中对越来越大的密码猜测池进行采样，对越来越大的密码猜测池进行采样，并计算它们恢复的 RockYou 测试分割的百分比。</li>
<li>结果表明 PassGPT 优于所有其他模型。它在 109 个猜测中恢复了测试集的 41.9%，而最先进的 GAN 模型的匹配率为 23.33%。</li>
</ol>
<p> </p>
<p>密码生成评估的另一个重要因素是生成新颖且独特样本的能力。<br>    在 109 次猜测中，PassGPT 保留唯一密码的比例最高 (60%)，而 PassVQT 则下降至 20%。由于 PassVQT 是针对所有出现的密码进行训练的，因此在其发行版下</p>
<p>测试集的恢复比例<br>![[Pasted image 20240327105738.png]]</p>
<p>长密码效率：<br>PassGPT 在对独特样本进行训练时表现最佳，PassVQT 现在在使用唯一密码进行训练时获得了更好的性能。在对这个新分布进行训练后，模型保持了相似的准确性。从 108 个猜测中，PassGPT 和 PassVQT 分别恢复了测试集的 15.5% 和 8.57%，而在 10 个字符设置中则为 19.37% 和 10.30%。</p>
<p>模型对为见过密码分布的泛化能力：。分别用 RockYou 和 LinkedIn 的数据集进行训练，然后都用  LinkedIn 的测试集测试。<br>![[Pasted image 20240327142047.png]]</p>
<p>用 Rockyou 训练的模型 泛化效果更好<br>![[Pasted image 20240327111524.png]]</p>

                        
                    </div>
                </div>
            </div>
            
                <footer class="kratos-post-meta-new">
                    <span class="pull-left">
                        <time datetime="2024-04-22T13:14:48.841Z" itemprop="datePublished">
                            <a><i class="fa fa-calendar"></i> 2024-04-22</a>
                        </time>
                        
                        
                            <!-- 当前仅支持valine/waline自带的统计功能 -->
                            
                        
                        
                            <!-- 当前仅支持waline自带的统计功能 -->
                            
                        
                    </span>
                    
                    
                        <span class="pull-right">
                            <a class="read-more" href="/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/PassGPT/" title="留言">留言<i class="fa fa-chevron-circle-right"></i></a>
                        </span>
                    

                </footer>
            
        </article>
    

    
    
        <article class="kratos-hentry kratos-entry-border-new clearfix" itemscope itemtype="https://schema.org/Article">
            <div class="kratos-status">
                
                    <i class="fa fa-refresh"></i>
                
                <div class="kratos-status-inner">
                    <div class="kratos-status-content" itemprop="articleBody">
                        
                            <h1 id="关键点"><a href="#关键点" class="headerlink" title="关键点"></a>关键点</h1><ol>
<li>第一个使用 GANs 生成密码</li>
</ol>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>已有工作 HashCat 和 John the Ripper 很成熟，但是还是需要扩展来进一步建模密码；<br>PassGAN 使用的是 GAN；<br>PassGAN 可以提取相当数量的密码属性，而其他工具没有；</p>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>密码常见；用户使用的密码简单；导致密码猜测很有效；<br>密码猜测的效率由匹配大量可能密码和特定密码哈希的速度有关；（可能的密码不是穷尽所有组合，而是有一定的候选密码）<br>最先进的工具在密码猜测时使用一些启发式算法可以构造许多新的候选密码。<br>但是，这些启发式的变换规则基于直觉，而不是基于大型密码数据集的分析。</p>
<p>因此，生成的候选密码覆盖范围小并且这些启发式的变换规则的开发和测试是很耗时的事情；<br>总结：当前方法的伸缩较差。</p>
<h1 id="我们的方法"><a href="#我们的方法" class="headerlink" title="我们的方法"></a>我们的方法</h1><p><strong>核心：</strong>训练一个模型自动决定密码的特征和结构，并利用这些知识生成新的候选密码；同时神经网络的训练必须要任何先验知识和属性的假定。<br><strong>比较：</strong></p>
<ol>
<li>马尔可夫模型：隐含地假设所有相关的密码特征都可以用n元语法定义；</li>
<li>基于规则：只能猜测与可用规则匹配的密码；<br><strong>结果：</strong></li>
<li>神经网络生成的密码覆盖范围很广<br>GANs：在高维空间中进行密度估计<br> 通过训练一个深度神经网络架构来执行隐式生成建模，该架构会输入一个简单的随机分布（如高斯分布或均匀分布），并生成遵循可用数据分布的样本。<br> 在某种程度上，他们用深度神经网络隐含地模拟了累积分布的逆模型，即 x &#x3D; F-1 θ (s)，其中 s 是一个均匀分布的随机变量。<br> 训练过程：GAN 使用了一种猫捉老鼠的游戏<ol>
<li>其中深度生成网络（G）试图模仿样本的底层分布，</li>
<li>而判别型深度神经网络（D）则试图区分原始训练样本（即 “真样本”）和由 G 生成的样本（即 “假样本”）。</li>
<li>这种对抗性程序迫使 D 泄露训练数据的相关信息，这些信息有助于 G 充分再现原始数据的分布</li>
</ol>
</li>
</ol>
<h1 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h1><ol>
<li>预测率高</li>
<li>输出质量高于规则</li>
<li>PassGAN 可以输出几乎没有限制的密码猜测数</li>
<li>实验中 PassGAN猜出来的密码最多，但是生成的候选密码也最多</li>
<li>与基于深度神经网络的密码猜测算法相比有竞争力。</li>
<li>PassGAN可以用于有效增强密码生成规则</li>
<li>缺点：<br> 分析：有缺点<ol>
<li>优势(表现力、通用性和从样本中自主学习的能力) 和 输出大小方面的成本 之间存在权衡。</li>
<li>基于规则的密码猜测工具可以在极少的尝试次数内生成大量的匹配结果，而 PassGAN 则必须输出更多的密码才能达到同样的效果。<br> 分析：不是问题</li>
<li>密码猜测工具可以很容易地组合在一起，先 hashcat 不行再 PassGAN</li>
<li>存储不是问题，可以离线生成。<br> 结论：用匹配个数作为主要指标是有意义的，而不是匹配生成的速度</li>
</ol>
</li>
</ol>
<h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><ol>
<li>GAN</li>
<li>当前工具，hashcat | JTR <ol>
<li>基于字典</li>
<li>基于规则</li>
<li>基于马尔可夫模型： <a target="_blank" rel="noopener" href="https://www.trustwave.com/en-us/resources/blogs/spiderlabs-blog/hashcat-per-position-markov-chains/">HC解释</a><a target="_blank" rel="noopener" href="https://openwall.info/wiki/john/markov">JTR解释</a></li>
<li>神经网络的使用（注意点在密码强度估计）</li>
</ol>
</li>
</ol>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><ol>
<li>模型：使用改进型 Wasserstein GANs（IWGAN）训练。</li>
<li>优化器：ADAM 优化器</li>
<li>参数：<ol>
<li>批次大小：</li>
<li>迭代次数：在每次迭代中，GAN 会运行一次生成器迭代和一次或多次判别器迭代。</li>
<li>每次生成器迭代的判别器迭代次数：表示判别器在每次 GAN 迭代中执行的迭代次数。每次生成器迭代的判别器迭代次数设为 10，这是 IWGAN 使用的默认值。</li>
<li>模型维度：表示每个卷积层的维数。我们尝试在生成器和鉴别器中使用 5 个残差层，两个深度神经网络中的每个层都有 128 个维度。</li>
<li>梯度惩罚系数 (λ)：它规定了应用于判别器相对于输入的梯度准则的惩罚。增加该参数可使 GAN 的训练更加稳定。在实验中，我们将梯度惩罚值设为 10。</li>
<li>输出序列长度，表示生成器 (G) 生成的字符串的最大长度。我们将 GAN 生成的序列长度从 32 个字符（IWGAN 的默认长度）修改为 10 个字符，以符合训练过程中使用的密码最大长度。</li>
<li>输入噪声矢量（种子）的大小：它决定了正态分布中的随机数有多少个作为 G 的输入，以生成样本。我们将这一大小设置为 128 个浮点数。</li>
<li>最大示例数：表示要加载的训练项（PassGAN 中为密码）的最大数量。GAN 加载的最大示例数设置为整个训练数据集的大小。</li>
<li>ADAM 优化器的超参数：</li>
</ol>
<ul>
<li>学习率，即调整模型权重的速度 </li>
<li>系数 β1，表示梯度运行平均值的衰减率。</li>
<li>系数 β2，表示梯度平方运行平均值的衰减速度。</li>
<li>ADAM 优化器的系数 β1 和 β2 分别设置为 0.5 和 0.9，学习率为$$<br> 10^{-4}<br>$$这些参数是 Gulrajani 等人使用的默认值。</li>
</ul>
</li>
<li>整体结构：<ol>
<li>残差块结构<br> ![[Pasted image 20240222172953.png]]</li>
<li>生成器和判别器各有五个残差块<br> ![[Pasted image 20240222173025.png]]</li>
</ol>
</li>
<li>运行环境<ol>
<li>软件<ol>
<li>IWGAN 的 TensorFlow 版本【代码】</li>
<li>TensorFlow 1.2.1</li>
<li>Python 2.7.12</li>
<li>Ubuntu 16.04.2 LTS</li>
</ol>
</li>
<li>硬件：<ol>
<li> 64GB 内存</li>
<li>12 核 2.0 GHz 英特尔至强 CPU </li>
<li>英伟达™（NVIDIA®）GeForce GTX 1080 Ti GPU（11GB 全局内存）。</li>
</ol>
</li>
</ol>
</li>
<li>IWGAN<ol>
<li>寻常在训练深度神经网络时，最初的训练误差会随着层数的增加而减小。然而，在达到一定层数后，训练误差又开始增加。</li>
<li>但是 ResNet 包含层与层之间的 “快捷连接”。这可以看作是对这些层的封装，并以标识函数（图 1 中表示为残差块）的形式实现。通过使用多个连续的残差块，ResNet 可以随着层数的增加不断减少训练误差。</li>
<li>PassGAN 中的残差区块由两个一维卷积层组成，并通过整流线性单元（ReLU）激活函数相互连接，如图 1 所示。区块的输入是标识函数，并与 0.3 个卷积层的输出相加，产生区块的输出。</li>
</ol>
</li>
<li>训练数据<ol>
<li>RockYou 数据集：选择长度为10，并且选择 80% 作为训练集，20% 作为测试集</li>
<li>LinkedIn 数据集：数据是散列形式的，基于规则的系统具有潜在的优势。</li>
</ol>
</li>
<li>测试方向<ol>
<li>相同的密码分布上训练和测试时，PassGAN 的预测效果如果</li>
<li>PassGAN 在不同数据集的预测效果</li>
</ol>
</li>
<li>比较<ol>
<li>用 PassGAN 的训练集作为HashCat Best64、HashCat gen2、JTR Spiderlab 规则、马尔可夫模型、PCFG 和 FLA 的输入数据集<ol>
<li>按频率降序排序的密码实例化了 HashCat 和 JTR 的规则<ol>
<li>HashCat Best64 生成了 754,315,842 个密码，其中 361,728,683 个密码是唯一的，长度不超过 10 个字符。请注意，这是 Best64 规则集在给定输入集（即 RockYou 训练集）上生成的最大样本数。</li>
<li>对于 HashCat gen2 和 JTR SpiderLab，我们从它们的输出中统一抽取了一个大小为 10的9 的随机子集。该子集由长度不超过 10 个字符的密码组成。</li>
</ol>
</li>
<li>对于 FLA，我们根据中提供的说明设置了 [44] 中的代码。我们训练了一个包含 2 个隐藏层和 1 个大小为 512 的密集层的模型（全部参数列表见附录 A 表 6）。为了与其他工具保持一致，我们没有对训练集进行任何转换（例如删除符号或将所有字符转换为小写）。训练完成后，FLA 会枚举其输出空间的一个子集，该子集由概率阈值 p 定义：当且仅当一个密码的估计概率至少为 p 时，该密码才属于 FLA 的输出。这样，长度为 10 个字符或以下的密码总数达到了 747,542,984 个。在评估中使用这些密码之前，我们按照概率从大到小进行了排序。</li>
<li>我们使用 3-gram Markov 模型生成了 494,369,794 个长度不超过 10 的唯一密码。我们使用该模型的标准配置运行了该模型[18]。</li>
<li>我们使用韦尔等人[91]的 PCFG 实现生成了 109 个长度在 10 或以下的唯一密码</li>
</ol>
</li>
</ol>
</li>
<li>评估<ol>
<li>首先评估了 PassGAN 的输出所生成的匹配数</li>
<li>然后将其与 FLA、马尔可夫模型的一种流行的 3-gram 实现[18]、PCFGs [91]以及 JTR 的密码生成规则（SpiderLab 混淆规则[82]）和 HashCat 的密码生成规则（Best64 和 gen2 规则[29]）进行了比较，（这些工具都在本文的训练数据集上做了多年的优化）。</li>
<li>结合 HashCat Best64 对 PassGAN 进行评估的实验结果。最后，我们从概率密度和密码分布的角度对 PassGAN 和 FLA 进行了比较。</li>
<li>如何避免过拟合：<ol>
<li>比较不同迭代次数时的生成匹配个数，个数开始下降说明即将过拟合</li>
</ol>
</li>
<li>如何 PassGAN 生成密码的质量<ol>
<li>唯一个数</li>
<li>生成的密码和其他数据集的匹配个数</li>
<li>结果：<ol>
<li>可以生成不少于其他工具的匹配个数的密码</li>
<li>且总的密码和其他工具相差在一个数量级之内</li>
<li>猜测不同于训练数据集的密码时，比基于规则的密码生成更有优势。</li>
</ol>
</li>
</ol>
</li>
<li>结合 PassGAN 和 HashCat<ol>
<li>首先删除训练集中所有匹配HashCat Best64 的密码 </li>
<li>实验结果表明结合后可以匹配更多密码，</li>
<li>Hashcat 的下新版本的  “slow candidates” 支持了这重结合</li>
</ol>
</li>
<li>比较 PassGAN 和 FLA <ol>
<li>FLA 是一个概率估计模型，本质是一个参数估计模型，会输出密码以及其概率估计</li>
<li>FLA 受马尔可夫过程限制，变异性与 n-gram 范围有关，而 PassGAN 的变异性更高</li>
<li>次数较少时，FLA 的概率模拟不准确，次数较高，两者差不多</li>
</ol>
</li>
</ol>
</li>
</ol>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><ol>
<li>PassGAN 很有效，能从一个数据集中猜测另一个数据集的密码</li>
<li>基于规则的限制很大。</li>
<li>FLA  和 PassGAN 也差不多</li>
</ol>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol>
<li>方法是革命性的，无需用户干预就可以生成密码，也无需用户分析密码库</li>
<li>PassGAN 可以输出相同的匹配数目，但是需要输出更多密码，这个成本可以忽略。</li>
<li>将 PassGAN 的生成模型换位条件 GAN 可以生成特定字符字段的密码</li>
</ol>
<h1 id="专业名词"><a href="#专业名词" class="headerlink" title="专业名词"></a>专业名词</h1><table>
<thead>
<tr>
<th>名词</th>
<th>含义</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>leet speak</td>
<td>黑客文</td>
<td></td>
</tr>
</tbody></table>

                        
                    </div>
                </div>
            </div>
            
                <footer class="kratos-post-meta-new">
                    <span class="pull-left">
                        <time datetime="2024-04-22T13:14:48.840Z" itemprop="datePublished">
                            <a><i class="fa fa-calendar"></i> 2024-04-22</a>
                        </time>
                        
                        
                            <!-- 当前仅支持valine/waline自带的统计功能 -->
                            
                        
                        
                            <!-- 当前仅支持waline自带的统计功能 -->
                            
                        
                    </span>
                    
                    
                        <span class="pull-right">
                            <a class="read-more" href="/2024/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/PassGAN/" title="留言">留言<i class="fa fa-chevron-circle-right"></i></a>
                        </span>
                    

                </footer>
            
        </article>
    



    <div class='text-center pagination'>
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right"></i></a>
    </div>



    <div class="hidden">
        <!-- 加载文章阅读对应的统计功能，评论自带的那种 -->
        
    </div>



        

            </section>

        

                
            

<section id="kratos-widget-area" class="col-md-4 hidden-xs hidden-sm">
    <!-- 文章和页面根据splitter来分割，没有的话就从头开始设置为sticky -->
    
        <div class="sticky-area">
    
    
                <aside id="krw-about" class="widget widget-kratos-about clearfix">
    <div class="photo-background"></div>
    <div class="photo-wrapper clearfix">
        <div class="photo-wrapper-tip text-center">
            <img class="about-photo" src="/images/avatar.webp" loading="lazy" decoding="auto" />
        </div>
    </div>
    <div class="textwidget">
        <p class="text-center"></p>
    </div>
    <div class="site-meta">
        <a class="meta-item" href="/archives/">
            <span class="title">
                文章
            </span>
            <span class="count">
                97
            </span>
        </a>
        <a class="meta-item" href="/categories/">
            <span class="title">
                分类
            </span>
            <span class="count">
                0
            </span>
        </a>
        <a class="meta-item" href="/tags/">
            <span class="title">
                标签
            </span>
            <span class="count">
                0
            </span>
        </a>
    </div>
</aside>
            
                

            
                
            
                
  <aside id="krw-posts" class="widget widget-kratos-posts">
  <h4 class="widget-title"><i class="fa fa-file"></i>最新文章</h4>
  <div class="tab-content">
      <ul class="list-group">
        
        
          
          
        
          
          
        
          
          
        
          
          
            <a class="list-group-item" href="/2024/04/22/TEMP%EF%BC%9A%E4%B8%AD%E9%93%81/"><i class="fa  fa-book"></i> Hello Worldaaaaaaa</a>
            
          
        
          
          
        
          
          
            <a class="list-group-item" href="/2024/04/22/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/%E8%8B%B1%E8%AF%AD%E5%8D%95%E8%AF%8D/"><i class="fa  fa-book"></i> Englislearning</a>
            
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
            <a class="list-group-item" href="/2024/04/22/hello-world/"><i class="fa  fa-book"></i> Hello Worldaaaaaaa</a>
            
          
        
      </ul>
  </div>
  </aside>

            
    </div>
</section>
        
        </div>
    </div>
</div>
<footer>
    <div id="footer"  class="ap-lrc"  >
        <div class="container">
            <div class="row">
                <div class="col-md-6 col-md-offset-3 footer-list text-center">
                    <ul class="kratos-social-icons">
                        <!-- Keep for compatibility -->
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <!-- New links -->
                        
                    </ul>
                    <ul class="kratos-copyright">
                        <div>
                            <li>&copy; 2024 Hexo 版权所有.</li>
                            <li>本站已运行<span id="span_dt">Loading...</span></li>
                        </div>
                        <div>
                            <li>Theme <a href="https://github.com/Candinya/Kratos-Rebirth" target="_blank">Kratos:Rebirth</a></li>
                            <li>Site built with&nbsp;<i class="fa fa-heart throb" style="color:#d43f57"></i>&nbsp;by John Doe.</li>
                        </div>
                        <div>
                            <li>Powered by <a href="https://hexo.io" target="_blank" rel="nofollow">Hexo</a></li>
                            <li>Hosted on <a href="https://github.io" target="_blank">Github Pages</a></li>
                        </div>
                        <div>
                            
                            
                        </div>
                    </ul>
                </div>
            </div>
        </div>
        <div class="kr-tool text-center">
            <div class="tool">
                
                    <div class="box search-box">
                        <a href="/search/">
                            <span class="fa fa-search"></span>
                        </a>
                    </div>
                
                
                    <div class="box theme-box" id="darkmode-switch">
                        <span class="fa fa-adjust"></span>
                    </div>
                
                
                
            </div>
            <div class="box gotop-box">
                <span class="fa fa-chevron-up"></span>
            </div>
        </div>
    </div>
</footer>
</div>
</div>

        <script defer src="/vendors/bootstrap@3.3.4/dist/js/bootstrap.min.js"></script>
<script defer src="/vendors/nprogress@0.2.0/nprogress.js"></script>
<script>
    if (!window.kr) {
        window.kr = {};
    }
    window.kr.notMobile = (!(navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i)));
    window.kr.siteRoot = "/";
</script>


    <script async src="/js/candy.min.js"></script>



    <script defer src="/vendors/aplayer@1.10.1/dist/APlayer.min.js"></script>
    
    <script defer src="/vendors/meting@2.0.1/dist/Meting.min.js"></script>
    <meting-js
        server="netease"
        type="playlist"
        id="3204190542"
        order="random"
        fixed="true"
    >
    </meting-js>



    <script defer src="/vendors/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

<script defer src="/js/kratosr.min.js"></script>
<script defer src="/js/pjax.min.js"></script>



<!-- Extra support for third-party plguins  -->


    </body>
</html>